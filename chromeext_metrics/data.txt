
# Corrective Changes
Purpose: Fixing defects or bugs in the system.

INTENT: Bug Fix
IMPACT: Correctness, reliability, and aligns with regression prevention and test-driven development (TDD).

INTENT: Security Patch
IMPACT: Addresses confidentiality, integrity, and availability based on security standards such as OWASP.

INTENT: Crash Fix
IMPACT: Enhances system resilience, ensures fail-safe design, and improves robustness.

INTENT: Logic Error Correction
IMPACT: Ensures functional correctness and logical validity using predicate logic.

INTENT: Regression Fix
IMPACT: Prevents the reintroduction of old bugs through regression testing and disciplined version control.

INTENT: UI/UX Bug Fix
IMPACT: Improves interface consistency and accessibility based on usability heuristics and design standards.

INTENT: Performance Bug Fix
IMPACT: Optimizes efficiency, reduces latency, and enhances resource utilization.

# Perfective Changes
Purpose: Improving performance, readability, or usability without altering core functionality.

INTENT: Code Refactoring
IMPACT: Follows clean code practices and software design principles like SRP, reducing coupling and improving cohesion.

INTENT: Code Smell Resolution
IMPACT: Eliminates bad design patterns, improves maintainability, and reduces technical debt.

INTENT: Internal Quality Improvement
IMPACT: Enhances software modularity, reduces cyclomatic complexity, and increases abstraction clarity.

INTENT: External Quality Improvement
IMPACT: Improves responsiveness, user satisfaction, and interface behavior without changing internal logic.

INTENT: Documentation Update
IMPACT: Enhances knowledge transfer, supports maintainability, and provides better developer onboarding.

INTENT: Logging Improvements
IMPACT: Increases observability, improves system debuggability, and aids in post-mortem analysis.

INTENT: Improving Test Coverage
IMPACT: Boosts defect detection rate and confidence in system behavior through line and branch coverage.

INTENT: Optimization (Speed, Memory, etc.)
IMPACT: Enhances algorithmic efficiency using profiling tools and performance tuning.

# Adaptive Changes
Purpose: Modifying software to support new or evolving environments.

INTENT: Platform Migration
IMPACT: Improves portability and supports abstraction across heterogeneous systems.

INTENT: Library/Dependency Update
IMPACT: Ensures up-to-date security and semantic version compatibility.

INTENT: Framework Upgrade
IMPACT: Increases maintainability and unlocks new features through backward-compatible evolution.

INTENT: OS Compatibility Update
IMPACT: Aligns software with platform-specific runtime behavior and optimizations.

INTENT: Hardware Adaptation
IMPACT: Supports embedded constraints and optimizes for device-specific performance.

INTENT: Cloud/Container Adaptation
IMPACT: Implements 12-factor app principles and improves scalability via infrastructure as code.

INTENT: Feature Addition
IMPACT: Expands functional completeness and meets new business requirements.

INTENT: Feature Update/Improvement
IMPACT: Enhances existing functionality and incorporates user feedback or usability data.

INTENT: New Module Integration
IMPACT: Improves modularity, enables plugin architectures, and supports better separation of concerns.

INTENT: Enhancement of Existing Functionality
IMPACT: Refines usability and performance while preserving backward compatibility.

INTENT: API Addition/Modification
IMPACT: Defines clean contracts using OpenAPI or equivalent standards and improves extensibility.

INTENT: UI Feature Expansion
IMPACT: Supports advanced UX and component reuse through design system integration.

# Preventive Changes
Purpose: Enhancing maintainability and reducing long-term risks.

INTENT: Removing Dead Code
IMPACT: Simplifies the codebase, reduces risk, and improves readability through cleaner logic.

INTENT: Automated Test Addition
IMPACT: Strengthens the feedback loop, ensures coverage, and supports CI/CD pipelines.

INTENT: Static Analysis Fixes
IMPACT: Enforces linting rules and contract-based verification for early error detection.

INTENT: Upgrade Build Scripts / CI/CD
IMPACT: Streamlines release pipelines and ensures consistent builds through DevOps automation.

INTENT: Security Hardening
IMPACT: Mitigates vulnerabilities and improves system integrity using secure defaults and access controls.

INTENT: Tech Debt Cleanup
IMPACT: Reduces long-term maintenance costs and aligns code with architectural standards.

INTENT: Architecture Refactoring
IMPACT: Supports scalability and maintainability by re-aligning system structure with layered or service-based architecture.

# Refactoring-Guru Based Enhancements

INTENT: Extract Method
IMPACT: Increases cohesion and modularity by isolating sub-procedures into independent functions.

INTENT: Inline Method
IMPACT: Simplifies structure by removing unnecessary delegation and improving flow readability.

INTENT: Replace Temp with Query
IMPACT: Eliminates temporary state and reduces duplication to conform with DRY principle.

INTENT: Move Method/Field
IMPACT: Improves cohesion and reduces inappropriate intimacy between classes.

INTENT: Extract Class
IMPACT: Resolves Large Class smell and aligns with SRP by redistributing responsibilities.

INTENT: Inline Class
IMPACT: Simplifies object hierarchies by eliminating unnecessary abstraction layers.

INTENT: Replace Conditional with Polymorphism
IMPACT: Reduces branching complexity and promotes extensibility through polymorphic dispatch.

INTENT: Introduce Null Object
IMPACT: Improves robustness by avoiding null reference checks and supporting predictable defaults.

INTENT: Encapsulate Collection
IMPACT: Ensures controlled access and integrity for mutable list-based fields.

INTENT: Encapsulate Field
IMPACT: Promotes encapsulation by mediating access to class internals.

INTENT: Rename Method/Parameter
IMPACT: Improves readability, reduces ambiguity, and aligns naming with domain semantics.

INTENT: Replace Type Code with Subclasses
IMPACT: Promotes OO design by encapsulating behavior within specialized types.


-------------------------------------------------------------------------------------
I. I NTRODUCTION
The success of a software system depends on its ability to
retain high-quality design in the face of continuous change.
However, managing the growth of the software while continuously developing its functionalities is challenging and can
account for up to 75% of the total development [1], [2].
One key practice to cope with this challenge is refactoring.
Refactoring is the art of remodeling the software design
without altering its functionalities [3]. It was popularized by
Fowler [3], who identiﬁed 72 refactoring types and provided
examples of how to apply them in his catalog.
Refactoring is a critical software maintenance activity that
is performed by developers for an amalgamation of reasons
[4]–[6]. Refactoring activities in the source code can be
automatically detected [4], [7] providing a unique opportunity
to practitioners and researchers to analyze how developers
maintain their code during different phases of the development
life-cycle and over large periods of time. Such valuable knowledge is vital for understanding more about the maintenance
phase; the most costly phase in software development [1], [8].
To detect refactorings, the state-of-the-art techniques [4], [7]
typically search at the level of commits. As a result, these
techniques are also able to group commit messages with their
corresponding refactorings.
III. RESEARCH GOALS
To cope with the above-mentioned challenges, throughout
this project, we aim to achieve the following research goals:
• Goal #1: Exploring how developers document refactoring
activities. We aim to extract how developers express
their nonfunctional activities, namely improving software design, renaming semantically ambiguous identiﬁers, removing code redundancies etc. Multiple studies
have been detecting the performed refactoring operations,
e.g., rename class, move method etc. within committed
changes to better understand how developers cope with
bad design decisions, also known as design antipatterns,
and to extract their removal strategy through the selection
of the appropriate set of refactoring operations [9].
• Goal #2: Understanding developer perception of refactoring. We aim to augment our understanding of the
development contexts that trigger refactoring activities
and enable future research to take development contexts
into account more effectively when studying refactorings.
Thus, the advantages of analyzing the textual description of the code change that was intended to describe
refactoring activities are three-fold: 1) it improves our
ability to study and commit message content and relate
this content to refactorings; a challenging task which
posed a signiﬁcant hurdle in recent work on contextualizing rename refactorings, 2) it gives us a stronger
understanding of commit message practices and could
help us improve commit message generation by making
it clear how developers prefer to express their refactoring activities, 3) it provides us with a way of relating
common words and phrases used to describe refactorings
with one another. We plan to progress on refactoring
motivation direction by identifying, among the various
quality models presented in the literature, the ones that
are more inline with the developer’s vision of quality,
when they explicitly state that they are refactoring the
code to improve it.
• Goal #3: Studying developers refactoring perspective in
practice. We aim to survey professional developers and
conduct a case study in the industry to gain practical insights from refactoring in practice by studying refactoring
motivation, documentation practices, and challenges. This
direction of study can help us design future studies in
refactoring that are empirically relevant to practitioners’
obstacles, challenges, and needs, and create the next generation of industry-relevant automated refactoring tools.

discussing a refactoring at all, and it is hard to determine
how a commit message is discussing the refactoring.
As the accuracy of refactoring detectors has reached
a relatively high rate, the mined commits represent a
rich space to understand how developers describe, in
natural language, and their refactoring activities. Yet, such
information retrieval can be challenging since there are
no common standards on how developers should be formally documenting their refactorings, besides inheriting
all the challenges related to natural language processing.
However, using the developer’s inline documentation has
added another dimension to better understanding software
quality, as mining developers comments, for instance, has
unveiled how developers knowingly commit code that is
either incomplete, temporary, error-prone.
• Documenting refactoring, similarly to any code change
documentation, is helpful to decipher the the rationale
behind any applied change, and it can help future developers in various engineering tasks, such as program comprehension, design reverse-engineering, and debugging.
The detection of such refactoring documentation was
hardly manual and limited. There is a need to automate
the detection of such documentation activities with an
acceptable level of accuracy. Indeed, the automated detection of refactoring documentation may support various
applications and provide actionable insights to software
practitioners and researchers, including empirical studies
around the developer’s perception of refactoring. This can
question whether developers care about structural metrics
and code smells when refactoring their code, or if other
factors may inﬂuence such non-functional changes.
• Despite the growing effort in recommending refactorings through structural metrics, optimization and code
smells removal, there is very little evidence on whether
developers follow that intention when refactoring their
code. Thus, there is a need to distinguish among all the
structural metrics, typically used in refactoring literature,
the particular ones that are a better representation of the
developers’ perception of software quality improvement.
• Refactoring, just like any code change, has to be reviewed
before being merged into the code base. However, little
is known about how developers perceive and practice
refactoring during the code review process, especially
since refactoring, by deﬁnition, is not intended to alter
the system’s behavior, but to improve its structure, so its
review may differ from other code changes. Yet, there is
not much research investigating how developers to review
code refactoring. The refactoring research has focused on
its automation by identifying refactoring opportunities in
the source code and recommending adequate refactoring
operations to perform. Moreover, the research on code
reviews has been focused on automating them by recommending the most appropriate reviewer for a given code
change. However, despite the critical role of refactoring
and code review, their innate relationship is still largely
unexplored in practice.

IV. C ONTRIBUTIONS
In accomplishing the goals, the solutions are organized into
3 main contributions as it is shown in Figure 1.
• In our research (IWoR2019 [10], JSS2020 [11], KMDDIS2020 [12], ASEj2021 [13], MSR2022 [14]), we extracted how developers express their nonfunctional activities, namely improving software design, renaming
semantically ambiguous identiﬁers, removing code redundancies etc.

1

HOW?

Refactoring Documentation

1.1

Exploring how developers document refactoring activities (IWoR 2019 & MSR 2022)

1.2

Automating the detection of refactoring documentation (JSS 2020)

1.3

Mining and managing big data refactoring for design improvement (KMDDIS 2020)

1.4

Improving the quality of refactoring documentation (ASEj 2020)

2.1

Understanding developer perception of refactoring (ESWA 2020)

2.2

Understanding the relationship between developer
improvement of internal quality attributes (ESEM 2019)

perception

and

the

2.3

Understanding the relationship between developer perception
improvement of external quality attribute (ICSR 2020 & ISSE 2021)

and

the

2.4

Understanding the relationship between developer experience and refactoring
(IWoR 2020 & JSEP 2021)

3.1

Studying the challenges of reviewing refactoring changes (ICSE 2021 & MSR 2022)

3.2

Improving the usability of automated refactoring tools (FSE 2021)

3.3

Understanding behavior preservation approaches in software refactoring
(IST 2021)

3.4

Understanding relationship between technical debt and refactoring
(SCP 2022)

Lack of refactoring
culture

2

Preference of manual
refactoring

No formal refactoring
documentation

refactoring tools are
underused

Understand developer
perception
of refactoring

WHY?

Refactoring Motivation

Misperception of quality
metrics

3

WHAT?

Refactoring in Practice

Problem Investigation

Research Objective

Research Plan

Research Method

the existing effort in accurately detecting refactorings, by
augmenting with any description that was intended to describe
the refactoring activity. As per our ﬁndings, developers tend
to add a high-level description of their refactoring activity,
and occasionally mention their intention behind refactoring
(remove duplicate code, improve readability), along with mentioning the refactoring operations they apply (type migration,
inline methods, etc.). Our model, combined with detecting
refactoring operations, serves as a solid background for various
empirical investigations. For instance, previous studies have
analyzed the impact of refactoring operations on structural
metrics [31]. One of the main limitations of these studies
is the absence of any context related to the application of
refactorings, i.e., it is not clear whether developers did apply
these refactoring with the intention of improving design metrics. Therefore, using our model will allow the consideration
of commits whose commit messages speciﬁcally express the
refactoring to optimize structural metrics, such as coupling and
complexity. So, many empirical studies can be revisited with
an adequate dataset.

In our research (ESEM2019 [15], ICSR2020 [16],
ESWA2020 [17], IWoR2020 [18], JSEP2021 [19],
ISSE2021 [20]), we augmented our understanding of the
development contexts that trigger refactoring activities
and enable future research to take development contexts
into account more effectively when studying refactorings.
• In our research (ICSE2021 [21], MSR2022 [22],
FSE2021 [23], IST2021 [24], SCP2022 [25]), we surveyed professional developers and conduct a case study
in the industry to gain practical insights from refactoring
in practice by studying refactoring motivation, documentation practices, and challenges.
Research repositories: The comprehensive experiments package are available online in [26].
•

V. R ESEARCH I MPLICATION
 Augmenting refactoring automation with documentation. Recent studies have been focusing on automatically
identifying any execution of a refactoring operation in the
source code [9], [27]. The main purpose of the automatic
detection of refactoring is to understand better how developers
cope with their software decay by extracting any refactoring
strategies that can be associated with removing code smells
[28], [29], or improving the structural design measurements
[30]. However, these techniques only analyze the changes at
the source code level, and provide the operations performed,
without associating it with any textual description, which may
infer the rationale behind the refactoring application. Our
proposed model intends to bridge this gap by complementing

Furthermore, our study provides software practitioners with
a catalog of common refactoring documentation patterns
which would represent concrete examples of common ways
to document refactoring activities in commit messages. This
catalog of SAR patterns can encourage developers follow
best documentation patterns and also to further extend these
patterns to improve refactoring changes documentation in
particular and code changes in general. Indeed, reliable and
accurate documentation is crucial in any software project.

explicitly stating the motivation of the refactoring. This will
provide the context of the proposed changes, for the reviewers,
so they can quickly identify how they can comprehend it.
According to our initial investigations, examples of refactoring
intents include enforcing best practices, removing legacy code,
improving readability, optimizing for performance, code clean
up, and splitting logic.

The presence of documentation for low-level changes such
as refactoring operations and commit changes helps to keep
track of all aspects of software development, and it improves
the quality of the end product. Its main focuses are learning
and knowledge transfer to other developers.
 Understanding developer’s motivation behind refactoring. One of the main ﬁndings shows that developers are not
only driven by design improvement and code smell removal
when taking decisions about refactoring. According to our
ﬁndings, ﬁxing bugs, and feature implementation play a major
role in triggering various refactoring activities. Traditional
refactoring tools are still leading their refactoring effort based
on how it is needed to cope with design antipatterns, which
is acceptable to the extent where it is indeed the developer’s
intention, otherwise, they have not been designed or tested
in different circumstances. So, an interesting future direction
is to study how we can augment existing refactoring tools to
better frame the developer’s perception of refactoring, and then
their corresponding objectives to achieve (reducing coupling,
improving code readability, renaming to remove ambiguity
etc.). This will automatically induce the search for more
adequate refactoring operations, to achieve each objective.
The categories of refactoring motivation provide software
practitioners with a catalog of common documentation patterns that represent concrete examples of common ways to
document refactoring activities in commit messages. Having
these higher-level categories helps developers ﬁnd the speciﬁc
refactoring patterns they are looking for faster. Generally, in
industry, there is no guideline on structuring commit messages.
This catalog of SAR patterns can encourage developers to follow the best documentation patterns and further extend these
patterns to improve refactoring documentation in particular
and code changes in general. This work will also help developers to improve the quality of the refactoring documentation and
trigger the need to explore the motivation behind refactoring.
Further, these categories tell the opinion of developers, so
it is essential for managers to learn developers’ opinions
and feelings, especially for distributed software development
practices. If developers do not document, managers will not
know their intention. Since software engineering is a humancentric process, it is essential for managers to understand the
intention of people working on the team.
 Understanding code review practice for refactoring
changes. It is heartening for us to realize that developers
refactor their code and perform reviews for the refactored code.
Our main observation, from developers’ responses, is how the
review process for refactoring is being hindered by the lack
of documentation. Therefore, as part of our survey report to
the company, we designed a procedure for documenting any
refactoring ReR, respecting three dimensions that we refer to
as the three Is, namely, Intent, Instruction, and Impact. We
detail each one of these dimensions as follows:
Intent. According to our survey results, it is intuitive that
reviewers need to understand the purpose of the intended
refactoring as part of evaluating its relevance. Therefore, when
preparing the request for review, developers need to start with

Instruction. Our results show how rarely developers report
refactoring operations as part of their documentation. Developers need to clearly report all the refactoring operations they
have performed, in order to allow their reproducibility by
the reviewers. Each instruction needs to state the type of the
refactoring (move, extract, rename, etc.) along with the code
element being refactored (i.e., package, class, method, etc.),
and the results of the refactoring (the new location of a method,
the newly extracted class, the new name of an identiﬁer, etc.).
If developers have applied batch or composite refactorings,
they need to be broken down for the reviewers. Also, in case
of multiple refactorings applied, they need to be reported in
their execution chronological order.
Impact. We observe that practitioners care about understanding the impact of applied refactoring. Thus, the third
dimension of the documentation is the need to describe how
developers ensure that they have correctly implemented their
refactoring and how they veriﬁed the achievement of their
intent. For instance, if this refactoring was part of a bug ﬁx,
developers must reference the patch. If developers have added
or updated the selected unit tests, they need to attach them
as part of the review request. Also, it is critical to self-assess
the proposed changes using Quality Gate, to report all the
variations in the structural measurements and metrics (e.g.,
coupling, complexity, cohesion, etc.), and provide a necessary
explanation in case the proposed changes do not optimize the
quality deﬁcit index.
 Developing next-generation refactoring-related code
review tools. Finding that reviewing refactoring changes takes
longer than non-refactoring changes reafﬁrms the necessity
of developing accurate and efﬁcient tools and techniques that
can assist developers in the review process in the presence of
refactorings. The refactoring toolset should be treated like the
CI/CD toolset and integrated into the tool chain. Researchers
could use our ﬁndings with other empirical investigations of
refactoring to deﬁne, validate, and develop a scheme to build
automated assistance for reviewing refactoring considering the
refactoring review criteria as review code become an easier
process if the code review dashboard augmented with the
factors to offer suggestions to document the review better.
 Teaching Documentation Best Practices. Prospective
software engineers are mainly taught how to model, develop
and maintain software. With the growth of software communities and their organizational and socio-technical issues, it
is important also to teach the next generation of software
engineers the best practices of refactoring documentation. So
far, these skills can only be acquired by experience or training.









Automatic Motivation Detection for Extract Method Refactoring Operations


Mohammad Sadegh Aalizadeh



A Thesis in
The Department of
Computer Science and Software Engineering



Presented in Partial Fulfillment of the Requirements For the Degree of
Master of Applied Science (Computer Science) at Concordia University
Montréal, Québec, Canada



August 2021

(c) Mohammad Sadegh Aalizadeh, 2021

CONCORDIA UNIVERSITY
School of Graduate Studies

This is to certify that the thesis prepared

By:	Mohammad Sadegh Aalizadeh
Entitled:	Automatic Motivation Detection for Extract Method Refactoring Operations
and submitted in partial fulfillment of the requirements for the degree of

Master of Applied Science (Computer Science)

complies with the regulations of this University and meets the accepted standards with re- spect to originality and quality.

Signed by the final examining committee:


	Chair
Dr. Tse-Hsun Chen
	External

	Examiner
Dr. Weiyi Shang

	Examiner
Dr. Tse-Hsun Chen
	Thesis Supervisor
Dr. Nikolaos Tsantalis




Approved by		
Dr. Leila Kosseim, Graduate Program Director


August 19, 2021		
Dr. Mourad Debbabi, Dean
Gina Cody School of Engineering and Computer Science


Abstract

Automatic Motivation Detection for Extract Method Refactoring Operations
Mohammad Sadegh Aalizadeh


Refactoring is a common maintenance practice that enables developers to improve the in- ternal structure of a software system without altering its external behaviour. In this study we propose a novel method to automatically detect 11 motivations driving the applica- tion of EXTRACT METHOD refactoring operations. We conduct a large-scale study on 325 open-source Java repositories to automatically detect the motivations of 346K EX- TRACT METHOD refactoring instances. Previous studies have been merely based on sur- veys, manual analysis of pull requests or commit-messages to detect the motivations of developers. In this study we develop motivation detection rules to automatically extract the developer motivations based on the context of a refactoring operation in the commit. We find that the top four motivations for EXTRACT METHOD refactoring is to introduce reusable methods, remove duplication, facilitate the implementation of new features and bug fixes, and decompose long methods to improve their readability. There is an asso- ciation between the removal of duplication and the introduction of reusable methods in the refactoring instances with multiple motivations. The findings of this study provide es- sential feedback and insight for the research community, refactoring recommendation tool builders, and project managers, to better understand why and how developers perform EX- TRACT METHOD refactorings and help them build refactoring tools tailored to the needs and practices of developers.







Acknowledgments

I would like to express my best gratitude and thanks to my supervisor, Prof. Nikolaos Tsan- talis. This thesis could not be possible without his compassionate guidance and invaluable motivational insights that opened new horizons of knowledge towards me.

   I would also like to express my appreciation to my family, my wife, Faezeh and my daughter, Fatemeh for all the beautiful moments and the serenity and aspiration they pro- vided for me to concentrate and proceed in all the hard times.

   Finally, I would like to thank my colleagues, Hassan Mansour, Mehran Jodavi ,Mosab- bir Khan Shiblu and Ameya Ketkar that shared their best experiences and were amazing in teamwork and helped me to learn a lot in my journey at Concordia.



Thank you.
Mohammad Sadegh Aalizadeh







Contents


List of Figures	viii
List of Tables	ix
1 Introduction	1
1.1 Motivation	1
1.2 Problem Statement	2
1.3 Objectives and Contributions	5
1.4 Outline	6
2 Literature Review	7
2.1 Refactoring mining tools	8
2.2 Refactoring Motivation	9
3 Research Methodology	13
3.1 Automatic Refactoring Motivation Detection	13
3.2 Step 1: Building Motivation Detection Rules	14
3.2.1 Generic Motivation Detection Rules	15
3.2.2 Apply the Rules on the Training Dataset	15
3.2.3 Handle Exceptional Cases	16
3.2.4 Filtering Motivations by Applying Precedence Rules	17
3.2.5 Optimize the Detection Rules	17
3.3 Extract Operation Motivation Detection	18
3.3.1 Reusable Method	19
3.3.2 Introduce Alternative Method Signature	22
3.3.3 Decompose Method to Improve Readability	24
3.3.4 Facilitate Extension	28
3.3.5 Remove Duplication	31
3.3.6 Replace method Preserving Backward Compatibility	32
3.3.7 Improve Testability	35
3.3.8 Enable Overriding	36
3.3.9 Enable Recursion	38
3.3.10 Introduce Factory Method	40
3.3.11 Introduce Async Operation	42
3.4 Step 2: Applying the Detection Rules in Large Scale	43
4 Experiment Results	45
4.1 RQ1: Accuracy of Automatic Motivation Extractor	45
4.1.1 Accuracy on the Training Dataset	45
4.1.2 Accuracy on the Test Dataset	46
4.2 RQ2: Most Prevalent Motivations for Extract Method Refactoring Operations 52
4.3 RQ3: What are the characteristics of the EXTRACT METHOD refactorings having Facilitate Extension as motivation	60
4.4 RQ4: Multiple concurrent EXTRACT METHOD Motivations	63
5 Threats to Validity	66
5.1 Internal Validity	66
5.2 Construct Validity	67
5.3 External Validity	67
6 Conclusion and future work	69
Bibliography	71



List of Figures

1 Refactoring Motivation Detection Process in Large Scale	14
2 Reusable Method Decision Tree	22
3 Introduce Alternative Method Signature Decision Tree	24
4 Decompose Method To Improve Readability Decision Tree	27
5 Facilitate Extension Decision Tree	30
6 Remove Duplication Decision Tree	32
7 Replace Method Preserving Backwards Compatibility Decision Tree	34
8 Improve Testability Decision Tree	36
9 Enable Overriding Decision Tree	37
10 Enable Recursion Decision Tree	39
11 Introduce Factory Method Decision Tree	41
12 Introduce Async Operation Decision Tree	42
13 Comparison of our automatically extracted motivation ranking with Silva
et al. (2016) survey ranking	52
14 Reusable Extracted Methods Visibility Changes	54
15 Multiple Method vs. Single Method Duplication Removal	56
16 Multiple and Single Method Decomposition to Improve Readability	59
17 Top SAR patterns in message of commits that include EXTRACT METHOD
with Facilitate Extension as motivation	62
18 Extract Motivation Motivation Detection Rate	64







List of Tables
1 Extract Method Motivation Themes	18
2 General Notations used in EXTRACT METHOD Motivation Detection Rules	19
3 Reusable Method Detection Rule	20
4 Introduce Alternative Method Signature Detection Rule	23
5 Decompose to Improve Readability Detection Rule	25
6 Facilitate Extension Detection Rule	28
7 Remove Duplication Detection Rule	31
8 Replace Method Preserving Backwards Compatibility Detection Rule	33
9 Improve Testability Detection Rule	35
10 Enable Overriding Rule	37
11 Enable Recursion Detection Rule	38
12 Introduce Factory Detection Rule	40
13 Enable Async Detection Rule	42
14 Extract Method Refactoring Motivation Flags	44
15 Extract Method Motivations Detection Precision and Recall	46
16 Pull Request Motivation Mapping to Extract Method Motivations	48
17 Pull Request Motivations	50
18 Precision and Recall of Extract Method Motivations in Pull Requests	51
19 Self-affirmed refactoring patterns	61
20 Association Rules for EXTRACT METHOD concurrent motivations	65



Chapter 1 Introduction
1.1 Motivation

Software systems continuously evolve and adapt to new requirements implemented through maintenance tasks, such as feature additions or bug fixes (Chen et al., 2016). In such environments, refactoring has been adopted by developers as a useful practice to improve the internal structure of software without altering its functional behaviour (Kaya et al., 2018).
   Many research studies have been conducted to build refactoring recommendation and prediction tools based on product and process metrics (Pantiuchina et al., 2020; Nyamawe et al., 2018). Next generation tools and techniques, such as user-aware intelligent refac- toring bots, automatic advisors and smart search-based refactoring methods, have been developed to optimize the refactoring recommendation process according to the developer decisions and feedback (Ivers et al., 2020; Stefano et al., 2020).
   However, there is a lack of large-scale empirical research that signifies the main in- terest points and reasons behind developer refactoring activities. Therefore, in this study we propose a novel context-aware approach to automatically find the developer motiva- tions behind applied refactoring operation. We focus our study on the EXTRACT METHOD

refactoring that is known as the Swiss army knife of refactoring due to its applicability in many different scenarios (Silva et al., 2016; Hora and Robbes, 2020).
   Previous research mainly focused on surveys or interviews and investigations of pull request reviews and commit messages to generate taxonomies and categorize refactoring motivations (Bavota et al., 2015; Silva et al., 2016; Pantiuchina et al., 2020). These research works discovered 11 major motivations behind the application of EXTRACT METHOD refactoring operations. These motivations are detected based on source code changes in commits. Therefore, we formulate detection rules and use them at the core of our auto- matic motivation detection tool to analyze the context of EXTRACT METHOD refactoring instances and determine the motivation(s) of the developer. We have not analyzed higher- level design artifacts, such as UML models or architecture documents to detect motivations, which can be studied in the scope of architectural refactoring.

1.2 Problem Statement

In this study we will investigate EXTRACT METHOD refactoring motivations from different aspects. We initially evaluate the accuracy of our motivation detection tool in RQ1 accord- ing to a dataset of actual developer responses about the reasons they applied EXTRACT METHOD refactorings in their projects. The remaining research questions will shed light to the most prevalent EXTRACT METHOD refactoring motivations (RQ2), investigate what self-affirmed refactoring-related keywords AlOmar et al. (2019a) appear in the messages of commits including EXTRACT METHOD refactorings (RQ3) and investigate the associations between different motivations in the cases of refactoring activities with multiple detected motivations (RQ4).
   RQ1: How accurate is our EXTRACT METHOD refactoring motivation detection tool? The automatic motivation detection tool utilizes an enriched source code representation model, which provides the required contextual information about the EXTRACT METHOD

refactoring operations. In chapter 3, we define detection rules for each motivation by an- alyzing the context in which an EXTRACT METHOD refactoring instance is applied. The accuracy and efficiency of the refactoring detection process can affect the motivation de- tection results. Therefore, we relied on the current state-of-the-art refactoring mining tool, RefactoringMiner (Tsantalis et al., 2020; Tsantalis et al., 2018), which has an overall pre- cision of 99.6% and a recall of 94%. Furthermore, it is essential to validate the results of the automatic motivation detection tool by examining the responses from developers about the reason behind the application of specific refactoring instances. Therefore, we reconstructed a training dataset of developer responses about the reasons they applied cer- tain EXTRACT METHOD refactoring operations in specific commits of their projects (Silva et al., 2016). The motivation detection algorithm is optimized on this training dataset to reach an acceptable accuracy level for a large scale study. We further validate the accuracy of the motivation detection algorithm on a testing dataset of manually analyzed and tagged refactoring motivations at pull-request level (Pantiuchina et al., 2020). The high accuracy obtained at this stage will have a direct effect on the analysis of the results to answer the research questions of the study.
RQ2: What are the most prevalent motivations behind the application of EXTRACT
METHOD refactoring operations?
   Developers commonly apply refactoring techniques to maintain and improve the de- sign quality of software systems during the development period. Automated tools have been developed to detect and recommend refactoring opportunities to assist developers in maintenance tasks, but they are often underused. To tailor these tools to the actual needs and practices of the developers, it is essential to find the most common reasons that moti- vate developers to refactor code. In this research question we will answer what specific rea- sons have the highest prevalence when a developer decides to perform EXTRACT METHOD refactoring operations.

   There have been a few studies (Silva et al., 2016; Pantiuchina et al., 2020) that cate- gorized possible motivations and reasons for refactoring through manual investigation and surveys to build motivation taxonomies. However, these studies due to their manual analy- sis nature focused on a small number of project commits and refactoring instances. We au- tomated the detection process of 11 motivations that were recognized in these taxonomies for the EXTRACT METHOD refactoring operations found in a large number of analyzed commits (346K commits from 325 open-source Java repositories). The results for this re- search question will give us the big picture about the reasons that motivate developers to extract methods.
   RQ3: What are the characteristics of the EXTRACT METHOD refactoring instances having Facilitate Extension as motivation?
   Refactoring is not always done for the sole purpose of design quality improvements and at times can be interleaved with other common development tasks, such as bug fixing or adding new features.
   In cases where an EXTRACT METHOD refactoring is tangled with such tasks, we can categorize motivations based on the self-affirmed commit messages (AlOmar et al., 2019a; AlOmar et al., 2021). For instance, bug-removal related keywords, such as fix, bug, issue, remove, cleanup can be used to detect the code extension motivation at a more fine-grained level (i.e., the refactoring operation facilitates a bug fix). On the other hand, collateral refactoring tasks are done without any clear discussions or messages, and therefore it is more complicated to find the exact intent of the developer without manual analysis.
   We have formulated a thorough detection rule for the Facilitate Extension motivation (Section 3.3.4), which captures EXTRACT METHOD refactoring instances with additional statements contributing to a bug fix or a feature addition. The rule is optimized to find statements that are directly contributing to functionality extension. At the same time, it filters out the statements inside the extracted method that have a minimal effect on adding

a new feature or fixing a bug.
   We will discuss the results of the Facilitate Extension motivation detection and the features that are considered in the detection rule. Furthermore, we will discuss what self- affirmed refactoring keywords are commonly used in the messages of commits that include EXTRACT METHOD refactorings with a Facilitate Extension motivation.
RQ4: Are there concurrent motivations for extracting methods?
   The motivation detection process is designed to analyze the EXTRACT METHOD refac- toring context to find the evidence related to each defined motivation. Therefore at times it is possible that multiple motivations are reported for a single EXTRACT METHOD refactor- ing instance. In such circumstances, various post-processing rules are applied to eliminate the conflicting motivations that have less priority.
   We will study the combinations of refactoring motivations that appear concurrently and analyze their co-occurrence to understand the possible relation between different refactor- ing motivations.

1.3 Objectives and Contributions

In this thesis, we formulate the logical rules to detect the motivations of developers that per- form EXTRACT METHOD refactoring operations. The proposed approach and developed tool for the automatic refactoring motivation detection can be utilized in refactoring-related studies that require more detailed information about the intention of the developers when performing refactoring activities. It can be also used for building smart refactoring advisors and recommendation tools to address more effectively the developer needs and practices.
Our most significant and important contributions are listed as follows:

• We build a general motivation detection approach for refactoring operations.

• We formulate EXTRACT METHOD motivation detection rules based on the context of

a refactoring instance for 11 major motivations based on two prominent studies that provided refactoring motivation taxonomies through developer surveys and manual analysis.
• We develop an open-source automated motivation detection tool that can detect EX- TRACT METHOD motivations, named Motivation Extractor, which is based on Refac- toringMiner 2.0 (Aalizadeh, 2021; Tsantalis et al., 2020).
• We manually analyze the Pull Requests (PRs) studied by Pantiuchina et al. (2020), which contain EXTRACT METHOD refactoring instances in order to validate the high precision and recall of our automated tool that was achieved on a training dataset from real developer responses Silva et al. (2016).
• We present and discuss empirical results found by analyzing 346K commits from 325 open-source Java repositories.

1.4 Outline

The rest of the thesis is organized as follows. In chapter 2 we discuss the various related works that have investigated the developer motivations to perform refactoring operations during maintenance tasks. In chapter 3 we present the details of the motivation detection model for EXTRACT METHOD refactoring and the detection rules for 11 different moti- vations. These rules are formulated based on the context of a refactoring instance. We will demonstrate the rules using decision trees that clarify how different detection rule con- ditions are matched to determine the developer motivations. In chapter 4 we present the results of our large scale study and the answers to our research questions. Threats to the validity are discussed further in chapter 5. Finally, we conclude the thesis in chapter 6.







Chapter 2 Literature Review
There has been a tremendous body of research in the field of software refactoring in the past 30 years. The term refactoring was first coined by Opdyke (1990) to describe source code transformations that can be automated and used to improve understandability and reusabil- ity. Since then there have been numerous papers about the different aspects of refactoring activity, such as the refactoring process, goals and incentives, automation techniques, and recommendation systems.
   Refactoring and its applications have been studied on various software artifacts from source code to models and design patterns (Derezin´ska, 2017). Tanhaei (2020) propose a method to automate architectural refactoring based on stakeholder quality attributes.
   Researchers have used different methods to study refactoring activities. For instance, search-based techniques are used to find an optimized refactoring sequence for automated design quality optimization (Mohan and Greer, 2017). Data mining techniques are uti- lized to predict the effect of refactoring on code quality during software evolution and also analyze the impact of refactoring on design quality metrics (AlOmar et al., 2019b). Some studies use fuzzy logic for automatic code smell detection and refactoring, as well as formal verification methods to ensure refactoring has not caused behavioural changes (Nasagh et al., 2021; LUO et al., 2011). Refactoring actions are performed for multiple

objectives. Refactoring can improve the internal quality attributes of the software like co- hesion, coupling, complexity, inheritance depth, and size (Chávez et al., 2017). It can also improve external software quality attributes that are indirectly measured through code metrics like reusability, flexibility, understandability, functionality, extensibility, and effec- tiveness (Vashisht et al., 2018). Performance-related measures can also be improved by refactoring at various scales. Arcelli et al. (2018) use performance anti-patterns to perform non-functional driven refactoring on UML models. Refactoring is further used for security purposes. Abid et al. (2020) study the impact of refactoring on security metrics related to data access, when improving quality attributes from the QMOOD model.

2.1 Refactoring mining tools

Refactoring actions can be performed manually by editing the code or through the automa- tion support provided in IDEs. Various refactoring detection tools have been developed and researchers used methods to detect the application of a wide range of refactoring types in software systems.
   Dig et al. (2006) developed Refactoring Crawler that uses a combination of syntactic and semantic analysis for detecting and refining of refactorings with an accuracy above 85%. Prete et al. (2010) used a template-based refactoring reconstruction approach that uses logic rules to describe code elements and their dependencies (adopted from their pre- vious work on LSdiff (Kim and Notkin, 2009)) and a logic programming engine to infer concrete refactorings. Their tool, Ref-Finder, can identify complex refactoring instances that consist of multiple atomic refactoring types between program versions, and achieves a precision of 0.79 and recall of 0.95.
   Silva and Valente (2017) propose RefDiff, an automated tool that employs heuristics based on static analysis and code similarity to detect refactoring operations between two successive revisions of a Git repository. This tool finds 13 well-known refactoring types

and has a precision of 100% and a recall of 88%. It is extended in RefDiff 2.0 to support multiple programming languages using the Code Structure Tree (CST) to abstract the spec- ifications of programming languages. In addition to Java, RefDiff 2.0 supports JavaScript and C programming languages (Silva et al., 2020).
   Tsantalis et al. (2018) designed, implemented and evaluated RefactoringMiner, which detects refactoring operations with a precision of 98% and a recall of 87%. Refactoring- Miner uses in its core an AST statement matching algorithm to detect refactorings in project revisions without requiring user-defined thresholds. In a more recent work Tsantalis et al. (2020), RefactoringMiner has been extended to support low-level refactorings that are per- formed within the body of methods. The accuracy of the tool was further improved reaching a precision of 99.6% and a recall of 94%.

2.2 Refactoring Motivation

In the human behaviour domain of refactoring, the developer's motivation is recognized as a key factor that has theoretical and practical implications on software development and maintenance.
   Liu and Liu (2016) conduct an interview with 25 developers to find the motivations of developers when they perform EXTRACT METHOD refactorings. Among all motiva- tions reuse, decomposition of long methods, clone resolution are known to be the major motivations. They analyze 7 open source repositories to validate the results. They find that EXTRACT METHOD opportunities should not merely be affected by the inner struc- ture of the methods (e.g., complexity or size). Due to the high prevalence of immediate reuse, they suggest refactoring recommendation tools can utilize recognition or prediction of reuse to improve chances of developers applying the recommended EXTRACT METHOD refactorings.

   Wang (2009) conducts a semi-controlled interview and presents an empirical model of refactoring that describes the relationship between motivators, contextual factors, and ac- tions. This research only considers psychological and personal values along with cultural, economic and other aspects of refactoring in organizations to determine motivations.
   Silva et al. (2016) applied thematic analysis on the responses received from 222 devel- opers contributing in 124 open source projects about why they perform specific refactoring operations. They used RefactoringMiner to detect refactorings and then manually inspected true positives. A catalogue of 44 distinct motivations for 12 types of refactoring operations was compiled. The results show refactorings are mainly performed due to requirement changes rather than code smells. For instance, among all 11 motivations for EXTRACT METHOD refactoring only two of them (decompose method to improve readability and remove duplication) are targeting code smells. Their study also shows that 55% of the studied refactoring instances are performed manually, mainly because of the developers' lack of trust in automated tools. This research has been the main motivation of our research and it also explicitly proposes that future research should refocus from code-smell-oriented to maintenance-task-oriented refactoring solutions. For this purpose, we automate the pro- cess of detecting motivations to facilitate analyzing the motivation categories of refactoring. We believe our empirical findings will help to build smarter recommendation tools that can target the actual developer needs.
   Kim et al. (2014) present a field study at Microsoft to quantitatively assess the benefits and challenges of refactoring from the developer perception. They utilize three comple- mentary research methods: survey, semi-structured interviews with professional software engineers and quantitative analysis of version history data. In the survey, they find that developers perceive substantial costs and risks when they refactor, and that refactoring def- inition is not confined to a rigorous semantic-preserving code transformation. Furthermore,

in the quantitative analysis of Windows 7 version history, they find that the top 5% of pref- erentially refactored modules experience a higher reduction of inter-module dependencies and complexity measures. This study investigates system-wide metrics such as defects, inter-module dependencies, size and locality of code changes, complexity, test coverage, people and organization metrics. However, it has some limitations, as it analyzes only the commits using the "refactor" keyword in their message, and thus misses a substantial undocumented refactoring activity. Moreover, the quantitative analysis focuses on the ver- sion history of a single large system, namely Windows 7, which is a threat to the external validity of the findings.
   Bavota et al. (2015) analyze refactoring operations over 63 releases of three Java open source projects. They utilize Ref-Finder to detect refactoring operations and build logistic regression models to find code smells and quality metrics that are significantly correlated with refactoring types. They highlight WMC metric as the only exception for quality met- rics that has a clear relationship with refactoring. They also find that only some of the analyzed code smells such as Blob, Long Method, Spaghetti Code and Feature Envy in- crease the chance of affected classes being refactored. Among all refactoring operations detected in the release history of the three examined Java projects, 42% are performed on code smells and 7% actually removed the code smells. Manual analysis is performed to improve the detection of code smells. They conclude that the developers' point-of-view of classes in need of refactoring does not always match with quality indicators.
   Pantiuchina et al. (2020) present a large-scale study to quantitatively and qualitatively investigate why developers refactor in order to generalize and complement previous studies that are based on surveys. They mine 287,813 refactoring operations and build a mixed ex- planatory logistic regression model to find the correlation between 42 product and process related metrics and refactoring operations in 150 open-source projects. Among product- related factors code readability is mostly correlated with refactoring. In process-related

metrics, source code change, fault-proneness, and the experience of developers changing a code component, play a significant role in triggering refactoring. They also manually tag a randomly stratified sample of 551 pull requests with 8,108 refactorings to build a taxonomy of 67 motivations in 6 main categories. This study demonstrates the factors and reasons that make a refactoring meaningful from the developers' perspective and can be utilized to make better refactoring recommendation tools to address specific developer needs.
   AlOmar et al. (2021) propose a two-step approach to automatically classify self-affirmed refactoring (SAR) operations that contain developer-related events documented in commit messages. They build a model with a high F-measure that combines N-Gram TF-IDF feature selection and binary and multi-class classifiers that outperforms pattern-based and random classifier approaches in classifying SAR commits into internal/external quality at- tributes and code smells categories. Their method is solely based on the commit messages to find the intent or rationale behind the refactoring.
   Paixão et al. (2020) inspect and classify developer intents behind refactoring during code reviews in 7 distinct categories. They manually analyze code changes in 1,780 code reviews that employ refactoring operations. They study the sequence, composition and evolution of refactoring operations in code reviews. They find that developers most often apply refactoring with other code changes to support feature additions or bug fixes.
   Our work is the first to automate the detection of refactoring motivations by analyzing the refactoring-related edits and the context in which the refactoring operations are applied within a commit. Such fine-grained analysis allows to extract the refactoring motivations with a high accuracy. Moreover, it enables the collection of refactoring motivations in a large-scale, as it has been implemented as a fully automated process.







Chapter 3

Research Methodology

In this chapter, we first present a general method for the automatic refactoring motivation detection. Using this method, we came up with logical rules and decision trees customized to detect 11 major motivations for EXTRACT METHOD refactorings. The motivation de- tection rules are implemented in an open-source tool named Motivation Extractor, which is built on top of RefactoringMiner Tsantalis et al. (2020) and is available on GitHub (Aal- izadeh, 2021).

3.1 Automatic Refactoring Motivation Detection

We have developed a general process for the automatic detection of refactoring motivations. This process can be further extended and applied to detect the motivations of different refactoring types. Figure 1 shows the two main steps of our method. In the first step, we build motivation detection rules in an iterative process and optimize them based on a training set that is constructed from the motivations given by the actual developers who performed specific refactoring operations in open source projects Silva et al. (2016). In the second step, we use the optimized detection rules and perform a large scale motivation detection in the commit history of open-source repositories.


Figure 1: Refactoring Motivation Detection Process in Large Scale

3.2 Step 1: Building Motivation Detection Rules

The first step of the automatic motivation detection process is to build the motivation de- tection rules from the generic descriptions of refactoring motivation themes, which are obtained from the Silva et al. (2016) study. In this study, the authors monitored 185 open- source projects for commits including refactoring operations. When such commit was detected, the commit author was contacted via email to explain the reasons behind the application of the detected refactoring operations. By applying thematic analysis on the responses received from 222 developers, Silva et al. created a catalogue of 44 distinct motivations for 12 well-known refactoring types. Each motivation theme is accompanied with a number of refactoring instances, which were applied for the reason described in the corresponding motivation description.
   We initially build a generic detection rule based on the motivation description and then optimize it by handling exceptional cases and conflicts with other co-detected motivations

in an iterative process as shown in Figure 1.


3.2.1 Generic Motivation Detection Rules

The generic motivation detection rules are initially formed based on the description of the motivations themes in natural language. Table 1 shows the descriptions of the motivation themes for the EXTRACT METHOD refactoring type. For example, the description for Reusable Method is "Extract a piece of reusable code from a single place and call the extracted method in multiple places". As a result, the generic rule should check if the extracted method is called in at least one more method other than the method from which it was extracted.
   These generic rules are very simple at first and are not tuned to address exceptional cases or conflicts with other co-detected motivations. Therefore, we need to use our train- ing dataset to assess the current precision and recall of the generic rules, and guide the optimization of the rules to detect the motivations in various refactoring contexts more accurately.

3.2.2 Apply the Rules on the Training Dataset

It is necessary to ensure the compatibility of the automatically detected motivations with the actual intentions of the developers. Therefore, we apply the rules derived from the previous step on our training dataset, which includes instances of refactoring operations labeled with a motivation theme based on the explanations of the developers who actually performed these refactorings Silva et al. (2016). Since the motivation labels come directly from the developers who performed the refactorings, we consider our training dataset a reliable ground truth (i.e., oracle), based on which we can optimize the detection rules. We can compute the precision and recall of the detection rules based on this oracle, and we can find all the exceptional cases in which the generic rules missed or mislabeled the

actual developer motivation(s). We use these insights to add the missing logic to the generic detection rules and handle specific scenarios.

3.2.3 Handle Exceptional Cases

Guided by the missed and mislabeled refactoring instances from the oracle, we analyze the commits in which these refactoring were detected to find exceptional cases that should be taken into account when enhancing the detection rules. For example, a missed case for the Reusable Method motivation, involved an extracted method that was reused in the same method from which it was extracted from. This case helped us to improve the detection rule to handle additional calls to the extracted method that were added in the origin method after the EXTRACT METHOD refactoring.
   Moreover, by analyzing individually each refactoring in a commit we might get motiva- tions that are in disagreement with the motivations obtained when analyzing refactorings in groups. For example, when a duplicated piece of code is extracted from multiple methods, a separate EXTRACT METHOD refactoring instance is reported for each source method. Analyzing each EXTRACT METHOD instance in isolation from the rest triggers the detec- tion of the Reusable Method motivation, as the corresponding rule finds that the extracted method is also called in methods other than the method from which it was extracted. How- ever, by combining the information from multiple EXTRACT METHOD instances, we can group the instances that have exactly the same extracted method, detect the correct motiva- tion, namely Remove Duplication, and discard the individually reported Reusable Method motivations.
   We will show these exceptional cases while explaining the motivation detection rules in Section 3.3 and demonstrate how different contexts in which a refactoring is applied affect the detected motivation.

3.2.4 Filtering Motivations by Applying Precedence Rules

In some cases there are multiple detected motivations for a single refactoring instance. For example, if we have a method that is extracted in production code, and in the same commit a unit test is added calling the extracted method, two motivations will be detected, namely Reusable Method as the extracted method is called in a method other than the method from which it was extracted, and Improve Testability as a new unit test is added to test the extracted method. In that case, Improve Testability is a more dominant motivation, as it captures more accurately the refactoring intention of the developer, and thus has precedence over Reusable Method.
   We present the precedence rules for each motivation in Section 3.3, and use them to fil- ter out some co-detected motivations with lower priority for the same refactoring instance.

3.2.5 Optimize the Detection Rules

Following this iterative process for each motivation rule, we end up with a set of optimized detection rules, which maximizes the overall precision and recall on our training dataset. The achieved accuracy results on the training dataset are shown in Section 4.1.1. To make sure our detection rules are not over-fitting the training dataset, we applied them also on a testing dataset. The accuracy results on the testing dataset are shown in Section 4.1.2. The high accuracy levels achieved in both training and testing dataset gives us confidence that the automatically detected motivations in our large-scale experiment (Section 3.4) will accurately reflect the intentions of the developers, and thus our experimental results will have a strong validity.

3.3 Extract Operation Motivation Detection

In this section, we describe the motivation detection rules for the EXTRACT METHOD refactoring. We are focusing on the motivations of EXTRACT METHOD refactoring for two reasons. First, according to Negara et al. (2013) and Tsantalis et al. (2020), EXTRACT METHOD is the most commonly applied refactoring on methods. Second, according to Silva et al. (2016), EXTRACT METHOD has the most observed motivations (11 in total) compared to other refactoring types. Therefore, EXTRACT METHOD is a significant refac- toring that is worth investigating the reasons driving its application at a large-scale.
   Table 1 shows 11 motivation themes along with their descriptions, as documented by Silva et al. (2016). We used the descriptions of the motivation themes, along with the accompanied refactoring instances for each motivation theme, as inputs for the process explained in Section 3.2 in order to derive our motivation detection rules.
Table 1: Extract Method Motivation Themes


Reusable Method
Extract a piece of reusable code from a single place and call the extracted method
in multiple places.
Introduce Alternative Method Signature
Introduce an alternative signature for an existing method (e.g., with additional
or different parameters) and make the original method delegate to the extracted one.
Decompose Method to Improve Readability
Extract a piece of code having a distinct functionality into a separate method to
make the original method easier to understand.
Facilitate Extension
Extract a piece of code in a new method to facilitate the implementation of a
feature or bug fix, by adding extra code either in the extracted method, or in the original method.
Remove Duplication
Extract a piece of duplicated code from multiple places and replace the dupli-
cated code instances with calls to the extracted method.
Replace Method Preserving Backward Compatibility
Introduce a new method that replaces an existing one to improve its name or re-
move unused parameters. The original method is preserved for backward com- patibility, it is marked as deprecated, and delegates to the extracted one.
Improve Testability
Extract a piece of code in a separate method to enable its unit testing in isolation
from the rest of the original method.
Enable Overriding
Extract a piece of code in a separate method to enable subclasses override the
extracted behavior with more specialized behavior.
Enable Recursion
Extract a piece of code to make it a recursive method.
Introduce Factory Method
Extract a constructor call (class instance creation) into a separate method.
Enable Async Operation
Extract a piece of code in a separate method to make it execute in a thread.


   The motivation rules are described in two ways. First, in a formal way using logical conditions combined with AND or OR logical operators, and second in a visual way using decision trees. Each of the sub-section that follows includes the definition of one of the 11

motivation detection rules. For the formal descriptions, we use the some common general notations shown in Table 2, while specific notations applicable only for a single motivation detection are shown in the corresponding sub-sections.
Table 2: General Notations used in EXTRACT METHOD Motivation Detection Rules
Notation	Description
EM	Extract Method Refactoring instance
 ma ? mb		A single call from method ma to method mb calls(C, mb)	 A set of calls within class C to the method mb calls(M, mb)		A set of calls from the set of methods M to method mb calls(S, mb)	A set of calls from the set of statements to method mb
ma	Source Operation Before Extraction
ma!	Source Operation After Extraction
mb.n	Signature of the Extracted Method
mb.C	Name of the Class containing the Extracted Method
ma.P	Ordered Parameter list of ma
ma.b	Set of All statements in the ma
ma.b.C	Set of Composite statements in method ma
ma.b.L	Set of leaf statements in method ma
mb.T2	Set of added statements(T2) in the extracted method mb
ma! .T2	Set of added statements(T2) in the source operation after extraction ma!
mb.T2.L	Set of leaf statements added in method mb
codeElementType(ma.leaf  )	Code element type of the leaf statement in method ma
calls(ma.leaf )	Set of all invocations in the leaf statement of method ma
Msi	Set of Methods in the parent commit si
C+	Set of Added Classes in child commit si+1
C~	Set of Modified Classes in child commit si+1
M +	Set of Added Methods in child commit si+1
M ~	Set of Modified Methods in child commit si+1
EM.Mapping	Mapped code in the extracted method
EM.Mapping.Fragment1	Set of mapped code statements before extraction
EM.Mapping.Fragment2	Set of mapped code statements after extraction
EM.Mapping.L	Set of Leaf statements in the Extract Method Refactoring mapped code
EM.Mapping.C	Set of Composite statements in the Extract Method Refactoring mapped code
EM.NotMapped.T1	Set of not-mapped statements that are deleted from ma
   EM.NotMapped.T2		Set of all not-mapped statements that are added to ma! , mb EM.NotMapped.T2.Child	Set of not-mapped statements that are added to mb EM.NotMapped.T2.Parent		Set of not-mapped statements that are added to ma!
Tsi+1	All test methods present in child commit si+1
mb.b.L.Return	Set of return statements in the extracted method mb



3.3.1 Reusable Method

Reusability is a significant external quality attribute that can be improved by refactoring (Bogart et al., 2020). EXTRACT METHOD refactoring is useful to extract smaller pieces of

functionality from longer methods for reusability purposes (Yang et al., 2009).

Generic Motivation Detection Rules

The rule to detect the Reusable Method motivation is shown in Table 3.

• All the changed classes and newly added classes will be examined to find invocations to the Extracted Operation.
• Additional invocations should exist outside of the Extracted Operation and Source Operation After Extraction.
• OR multiple invocations should exist within the Source Operation After Extraction.

Table 3: Reusable Method Detection Rule

? mr ? mb | mr ? {C+ ? C~} ?
mr ? mb ?/ calls(Dsi+1 , mb) ?
(mr ? mb ?/ calls(Tsi+1 , mb) ? ma! ?/ Tsi+1 ) ? (mr ? mb ? calls(Tsi+1 , mb) ? ma! ? Tsi+1 ) ?
¬ mr ? ma replaced with mr ? mb ?
? mr ? mb! | mb! .n = mb.n ? mb! .C /= mb.C ?
mr ? mb ?/ calls(Nsi+1 , mb)
? |calls({ma! }, mb)| > 1
mb! : Another method with the same signature as the extracted method mb
Dsi+1 : All methods from which the body of mb was extracted in child commit si+1
Nsi+1 : All nested extracted methods in child commit si+1


Exceptional cases

1. Duplicate code fragments extracted from a single method or multiple methods. The call site of an extracted method which is involved in duplication removal should not be considered as reuse. (e.g., BuildCraft 1)
2. Test methods call the extracted method, but it belongs to production code. If the extracted method belongs in production code, all calls from test code are ignored
1https://github.com/BuildCraft/BuildCraft/commit/a5cdd8c

when assessing reusability. However, if the extracted method belongs in test code, then calls from test code are considered when assessing reusability.
3. The additional call takes place in a method which was originally calling the source method, but calls the extracted method after the refactoring. This typically happens when the entire body of a method is extracted in a new method for the purpose of deprecating the source method, or introducing an alternative signature for the source method. In such case, the additional call is ignored when assessing the reusability of the extracted method, because it simply changes the delegation from the source to the extracted method. (e.g., Google truth2)
4. The additional call to the extracted method takes place in another class, in which a new method is added having the same signature as the extracted method. In such case, the additional call is ignored when assessing the reusability of the extracted method, since a local method is called and not the extracted one. (e.g., Intellij 3)
5. When we have nested EXTRACT METHOD refactorings (i.e., a method is extracted from the body of another extracted method), the calls to the extracted methods are not placed in the source method originally containing the extracted code, but inside the bodies of the subsequently extracted methods. As a result, although the refactor- ing instance reports that the nested extracted method came from the source method originally containing the extracted code, the call site to the nested extracted method is not placed in the source method. In such case, the call inside the body of a nested extracted method is ignored when assessing the reusability of the extracted method. (e.g., Checkstyle4 , JGroup5)
2https://github.com/google/truth/commit/200f157
3https://github.com/JetBrains/intellij-community/commit/10f769a  4https://github.com/checkstyle/checkstyle/commit/5a9b724
5https://github.com/belaban/JGroups/commit/f153375

   Furthermore, The motivation detection logic for Reusable Method is demonstrated as the decision tree shown in Figure 2.

Figure 2: Reusable Method Decision Tree


Filtering the set of refactoring motivations

Precedence of Backward Compatibility and Alternative Method Signature: An extracted method is not considered as reusable, if the Backward Compatibility or Alternative Method Signature motivations are detected for the same instance.

3.3.2 Introduce Alternative Method Signature

EXTRACT METHOD refactorings are sometimes used to introduce an alternative method signature. The original method will be used as a delegate to the extracted method that may have added parameters or changed parameter types. Understanding the signature- level changes can be utilized to improve refactoring-based tools for API migration (Dig and Johnson, 2006).

Table 4: Introduce Alternative Method Signature Detection Rule

ma! .P /= mb.P ?
|calls({ma! }, mb)| = 1 ?
? leaf ? ma! .b.L | {calls(ma! .leaf ) n calls({ma! }, mb)} = 0 ?
¬codeElementType(ma! .leaf ) = V ARIABLE_DECLARATION _STATEMENT
Generic Motivation Detection Rules

The rule to detect the Alternative Method Signature motivation is shown in Table 4.

• The extracted operation parameter types are not equal with the Source Operation After Extraction (SOAE) parameter types. Therefore, either the number of the pa- rameters or their types changes in the extracted method.
• And Source Operation After Extraction (SOAE) should delegate and hand over the responsibility to the extracted method. Hence, we look for the invocations to the extracted method in the source operation after extraction. The number of the invoca- tions to the extracted method should be one to consider that as delegation.
• And SOAE can include temporary variables. The source operation after extraction may include statements which declare temporary variables. Therefore, all the leaf nodes that do not subsume EXTRACT METHOD invocations in the SOAE will be ex- amined to see if they are variable declaration statements (VDS). If all the statements in the source operation after extraction are solely used to declare temporary variables and the two previous conditions also hold true, we report the Introduce Alternative Method Signature motivation.
   There are no exceptional cases to examine for the detection of the Alternative Method Signature motivation.
   Figure 3 shows the decision tree for the EXTRACT METHOD refactoring operations that are performed to Introduce Alternative Method Signature.


Figure 3: Introduce Alternative Method Signature Decision Tree

Filtering the set of refactoring motivations

Precedence of Remove Duplication: In cases where a Remove Duplication motivation is also detected, we do not consider Alternative Method Signature. Furthermore, if Facilitate Extension was previously detected as a motivation it is discarded.

3.3.3 Decompose Method to Improve Readability

Developers consider readability and understandability as an important quality attribute and mainly adopt refactoring for the purpose of source code understandability (Vassallo et al., 2019). EXTRACT METHOD refactorings can also be used to improve the understandability and readability of source code. A piece of code with a distinct functionality can be ex- tracted to a separate method and this makes the original method more readable and easier to understand.
   Decomposition of the original method can be performed to break it into multiple smaller extracted methods. When decomposition is performed to extract multiple methods from the source operation, it is required to analyze the list of all EXTRACT METHOD and EXTRACT AND MOVE METHOD refactoring operations together. Decomposition can also be detected with a single extracted method, if the extracted piece of code makes the original method more readable after extraction.













Table 5: Decompose to Improve Readability Detection Rule

Decompose Multiple Method to Improve Readability:
? EM1, .., EMn 3 n > 1 ? mb1 /= .. /= mbn ? ma1 = .. = man = ma ? EM1  /=  ..  /=  EMn  ? editDistance(EMi.Mapping.Fragment1, subsume(ma! ? mb)) > 0.55
3 |EM.Mapping.L| = 1, 1 < i < n
Decompose Single Method to Improve Readability:
? EM 3
|ma.b| - |EM.Mapping.Fragment2| > 0 ?
ma.b - EM.Mapping.fragment2 ?/ EM.NotMapped.T1 ?
¬ (getter(mb) ? setter(mb) ?
editDistance(EM.Mapping.Fragment1, subsume(ma! ? mb)) > 0.55
3 |EM.Mapping.L| = 1 ?
|EM.Mapping.composite| > 0 ? |calls(ma! .b.C.Expression, mb)| > 0 ?
|callV ars(ma! , mb) n ma! .b.C.Expression.V ars| > 0 ? (|ma! .b.L| > 1 ? |calls(ma! .b.L.Return, mb)| > 0)
subsume(ma! ? mb): Statement subsuming a call from method ma! to method
mb
editDistance(s1, s2): Normalized Levenshtein Edit Distance of statements s1,s2. getter(mb): mb is a getter method , setter(mb): mb is a setter method. calls(ma! .b.C.Expression, mb): A set of calls from the set of composite statement expressions in ma! to method mb
ma! .b.C.Expression.V ars: Set of all the variables in the composite statement ex- pressions of ma!
callV ars(ma! , mb): Variables in method ma! that are initialized with an expression
that has a call to mb

Generic Motivation Detection Rules

The rule to detect the Decompose Method to Improve Readability motivation is shown in Table 5.
• Multiple EXTRACT METHOD Decompositions: We primarily check the decomposi- tion from one source method to multiple extracted methods. The detection of the extraction of multiple different pieces of code from a source method depends on analyzing together the groups of EXTRACT METHOD refactorings having the same source method in the same commit (e.g., Closure6).
• Single EXTRACT METHOD decompositions: The invocation to the extracted method is
1. in return statements,

2. in composite statement expressions of the source operation after extraction

3. in the initializer of a variable declaration, which is referenced in composite statement expressions of the source operation after extraction
4. the extracted piece of code has nested composite statements, and therefore its extraction can improve the readability and understandability of the Source Op- eration Before Extraction.

Exceptional cases

1. If there is only one statement in the extracted method, we compute the normalized Levenshtein (1966) distance between the statement calling the extracted method in the SOAE and the mapped statement in the source operation before extraction (with a threshold greater than 0.55) to determine whether the decomposition is performed
6https://github.com/google/closure-compiler/commit/ea96643

for the purpose of readability. If the distance shows that the text similarity between the statements before and after extraction is not much different (less than threshold) we do not consider these cases as Decompose to Improve Readability (e.g., jackson- databind7).
2. For every EXTRACT METHOD in the commit we check if the extracted method is either a getter or a setter method, and if so, the EXTRACT METHOD refactoring is not considered to have the Decompose to Improve Readability Readability motivation.
3. For single method decompositions we should not have all statements in the source operation mapped to the extracted method statements.

Figure 4: Decompose Method To Improve Readability Decision Tree

   The decision tree to detect Decompose Method to Improve Readability motivation is demonstrated in Figure 4. Tree nodes show all the circumstances and criteria to detect multiple or single decompositions for the Improve Readability motivation.
7https://github.com/FasterXML/jackson-databind/commit/cfe88fe3

Filtering the set of refactoring motivations

No filtering is applied for this motivation.


3.3.4 Facilitate Extension

Refactoring is a complex process that is performed for various purposes like facilitating feature additions or supporting bug fixes (Ferreira, 2018). EXTRACT METHOD refactor- ings can facilitate the implementation of features or bug fixes by adding extra code in the extracted operation or in the original method. The added statements will be verified and filtered to ensure they are used to facilitate extension.
Table 6: Facilitate Extension Detection Rule

? EM 3 |EM.NotMapped.T2| > 0 ?
|calls(mb.T2, mbi )| = 0, i >= 1 ?
{mb.T2 n EM.NotMapped.T1} = 0 ? {mb.T2.L n EM.Mapping.Fragment2} = 0 ?
{invocationExpression(EM.NotMapped.T 2.Child) n mb.P } = 0 ?
|mb.T2.Neutral| = 0 ?
|calls(ma! .T2, mbi )| = 0, i >= 1 ?
{ma! .T2 n EM.NotMapped.T1} = 0 ?
{invocationExpression(EM.NotMapped.T 2.Parent) n mb.P } = 0 ?
|ma! .T2.Neutral| = 0 ? {declaredV ariables(ma! ) n mb.P } = 0 ?
|{ma! .T2 n callScope(mb)}| = |ma! .T2|
? |ternary(EM.Mapping.Fragment2)| > 0
invocationExpression(EM.NotMapped.T2.Child):Set of invocation expression in the added statements of method mb
invocationExpression(EM.NotMapped.T2.Parent):Set of invocation expression in the
added statements of method ma!
mb.T2.Neutral:Set of added statements in method mb that are neutral (i.e. BLOCK,RETURN_STATEMENT or other code element types like IF_STATEMENT , EXPRESSION_STATEMENT, VARIABLE_DECLARATION_STATEMENT if there is no
invocation to mb and other methods in them)
declaredV ariables(ma! ): Set of all the declared variables in the method ma! callScope(mb): Set of all the statements that are located in the body of the innermost composite statement that encompasses the invocation to mb
ternary(EM.Mapping.Fragment2): Set of added ternary statements in the mapped code statements after extraction


Generic Motivation Detection Rules

The rule to detect the Facilitate Extension motivation is shown in Table 6.

• Added statements (leaf or composite) exist in the extracted operation

• Or Added statements (leaf or composite) exist in the Source Operation After Extrac- tion (SOAE)

Exceptional cases

To handle specific scenarios we examine the following exceptional cases to find valid added nodes (i.e., leaf or composite statements) in the extracted method or SOAE:
1. There should be no invocation to the Extracted Method in the added nodes. In cases that there is an invocation to the extracted method the added node is excluded.
2. Added nodes should not be neutral statements (i.e., block, return statement, if statement, expression statement, or variable declaration statement, if there is no in- vocation to the extracted method in them)
3. If all or part of an added node is in the deleted statements of the source operation before extraction we do not consider it as an added node.
4. The added nodes that contain invocation expressions, or declare variables that are used in the parameters of the extract method invocation are excluded.
5. In cases where a new ternary operator is utilized in a statement mapping between the source operation and the extracted method, then this statement will be considered as an added node (e.g., intellij-community 8).
6. Added nodes (leaf/composite) in Source Operation After Extraction (SOAE) should be located in the EXTRACT METHOD call scope. If added nodes in the SOAE are off the scope of extracted method call site, we do not consider them as added code for facilitating extension. We determine the valid scope for added statements in the
8https://github.com/JetBrains/intellij-community/commit/a973419

SOAE to be the inner-most composite statement that includes the extracted method invocation.

Figure 5: Facilitate Extension Decision Tree

   The decision tree to detect EXTRACT METHOD with the Facilitate Extension motiva- tion is demonstrated in Figure 5. Validity rules are used to filter the added statements in the Extracted Method (EM) and Source Operation after Extraction (SOAE) and to ensure they are related to facilitating extension.

Filtering the set of refactoring motivations

This motivation is removed if Alternative Method Signature, Replace Method for Back- ward Compatibility, Improve Testability, Enable Overriding or Introduce Factory Method motivations are detected for the same refactoring instance.
   Furthermore, to determine the validity of the Facilitate Extension motivation, we per- form post-processing to examine if this refactoring instance is included in any of the ex- tracted operations removing duplication. In that case Facilitate Extension will be elimi- nated (e.g., j2objc9).
9https://github.com/google/j2objc/commit/fa3e6fa

3.3.5 Remove Duplication

Code duplication can occur to reuse some existing functionality and can potentially be destructive for the evolution and maintainability of the software (Kaur and Mittal, 2017). Various refactoring patterns exist for removing clones and EXTRACT METHOD refactor- ings can be applied on duplicate code (Chen et al., 2018). Duplication can be removed from a single method or from multiple methods. The duplicated piece of code will be extracted into a new method and there will be an invocation to the extracted method replacing each code duplicate.
Table 7: Remove Duplication Detection Rule

Single Method Remove Duplication:
? EM1, .., EMn 3 n > 1 ? mb1 = .. = mbn = mb ?
EM1 = .. = EMn = EM ? (|EM.Mapping| > 1, n = 2)

Multiple Method Remove Duplication:
? EM1, .., EMn 3 n > 1 ? ma1 /= .. /= man ? mb1 = .. = mbn = mb? EM1 /= .. /= EMn ? (|EM1.Mapping| = |EM2.Mapping| = s > 1, n = 2)


Generic Motivation Detection Rules

The rule to detect the Remove Duplication motivation is shown in Table 7.

• Single Method Duplication Removal: If a single EXTRACT METHOD is used several times to extract a duplicated piece of code from different parts of the same source operation. In this case we will have repetitive EXTRACT METHOD refactorings in the same commit.
• Multiple Method Duplication Removal: In this case, there are more than one EX- TRACT METHOD refactoring instances, which have different source operations, but all of them have a common extracted method.

Exceptional cases

For both, single method and multiple method cases of Duplication Removal, we have set a threshold for the number of mapped statements in the extracted method to be more than one, when there are only two EXTRACT METHOD refactoring instances that are used to remove duplication.

Figure 6: Remove Duplication Decision Tree

   The decision tree in Figure 6 shows how single method and multiple method Duplica- tion Removal is detected.

Filtering the set of refactoring motivations

This motivation has priority over Decompose Method motivation detected for the same EXTRACT METHOD refactoring instance.

3.3.6 Replace method Preserving Backward Compatibility

Software library maintainers use backwards compatibility to create a replacement for dep- recated methods, which provides the clients of the library an option between the backwards

compatible version and the new version (Perkins, 2005). EXTRACT METHOD can be a use- ful refactoring tool for library maintainers to introduce new changes to previous methods. The new method can have a better name or remove some unused parameters. The original method is marked as deprecated and delegates to the extracted method.
Table 8: Replace Method Preserving Backwards Compatibility Detection Rule

(ma! .P /= mb.P ? ma! .n /= mb.n) ?
|calls({ma! }, mb)| = 1 ?
accessModifier(ma! ) /= protected ? accessModifier(ma! ) /= private ?
? leaf ? ma! .b.L | {calls(ma! .leaf ) n calls({ma! }, mb)} = 0 ?
¬codeElementType(ma! .leaf ) = V ARIABLE_DECLARATION _STATEMENT ?
annotation(ma! ) = @Deprecated

accessModifier(ma! ) Access modifier of the ma!
annotation(ma! ) JavaDoc annotation of the method ma!


Generic Motivation Detection Rules

The rule to detect the Replace method Preserving Backward Compatibility motivation is shown in Table 8.
• The Extracted Operation and Source Operation After Extraction (SOAE) parameters are not equal. Therefore, either the number of the parameters or their types change in the extracted method. OR Extracted Operation and SOAE have different names.
• AND SOAE should be a delegate and hand over the responsibility to the extracted method. Hence, we look for the invocations to the extracted method in the source operation after extraction. The number of the invocations to the extracted method should be equal to one to consider it as delegation.
• AND the access modifier of the source operation after extraction should not be pro- tected or private to provide access to the backward compatible version of the method.
• AND SOAE can include temporary variables. The source operation after extraction may include statements which declare temporary variables. Therefore, all the leaf

nodes that do not include invocations to the extracted method within the SOAE will be examined to see if they are variable declaration statements (VDS).
• OR SOAE uses the @deprecated annotation. When the @deprecated anno- tation is not available the JavaDoc is also checked to find usages of the keyword "deprecated".

Exceptional cases

In some cases the developer forgets to use the @deprecated annotation for the SOAE. The detection rule enables us to detect backward compatibility even when @deprecated annotation is not provided.

Figure 7: Replace Method Preserving Backwards Compatibility Decision Tree

   The decision tree to detect EXTRACT METHOD refactorings that are performed to Re- place Method for Backward Compatibility is shown in Figure 7 in which the logic to detect this EXTRACT METHOD motivation is demonstrated.

Filtering the set of refactoring motivations

• Precedence of Remove Duplication.

• Removing Introduce alternative Method, Improve Testability and Facilitate Exten- sion if they are detected for the same refactoring instance.

3.3.7 Improve Testability

To achieve the highest confidence in software quality, it is essential to utilize techniques like refactoring to improve the design for testability (Tarlinder, 2016). EXTRACT METHOD refactorings can facilitate the unit testing of a method by isolating a piece of code for testability purposes in a separate method.
Table 9: Improve Testability Detection Rule

? mt ? mb | mt ? {C+ ? C~} ?



Generic Motivation Detection Rules

The rule to detect the Improve Testability motivation is shown in Table 9.

• All changed classes and newly added classes are examined to find test methods. We check the annotations of test methods and also their signature to find if they are test methods in a test class (i.e., include "test" in their name).
• And there is an invocation from a test method to the extracted method.

Exceptional cases

1. The invocation should be located in a different class than the extracted method class. If the test method is inside the extracted method class, it has already been part of the test code and therefore the extract method is not used for testability purposes.

2. And the invocation should be in the added or edited nodes. Otherwise we do not consider that invocation valid for improving testability (e.g., VoltDB10).

Figure 8: Improve Testability Decision Tree

   Figure 8 shows the decision tree that is used to detect the EXTRACT METHOD refactor- ings that are used to improve testability.

Filtering the set of refactoring motivations

• Precedence of Remove Duplication

• Removing Decompose to Improve Readability if they are detected for the same refac- toring instance.
• Removing Facilitate Extension if they are detected for the same refactoring instance.


3.3.8 Enable Overriding

EXTRACT METHOD refactorings can be utilized to edit a stable piece of code for the pur- pose of preparing it to be overridden in a subclass. The extracted method will be overridden
10https://github.com/VoltDB/voltdb/commit/e58c9c3

in a separate method with a more specialized behaviour when it is intended to enable over- riding. Sometimes developers do not override the extracted method, but simply extract the method for future changes related to overriding the method.
Table 10: Enable Overriding Rule

? mv ? mb | mv ? {C+ ? C~} ? subType(mV .C, mb.C) ? mv.n = mb.n
? |{overridingRelatedKeywords() n commentTokens(mb)}| > 0
subType(mV .C, mb.C): returns true if class of the virtual method mv.C is a sub- type of the class of the extracted method mb.C overridingRelatedKeywords(): Returns set of all overriding-related keywords like virtual, override, etc.
commentTokens(mb): Set of all words in the extracted method comment.

HAS


FALSE


FALSE
NO




TRUE
YES

TRUE
YES

Figure 9: Enable Overriding Decision Tree


Generic Motivation Detection Rules

The rule to detect the Enable Overriding motivation is shown in Table 9.

• All the changed classes and newly added classes will be examined.

• A method with equal signature to the extracted method is found in a Subtype class of the extracted method class. This method implements the specialized behaviour for the extracted method.

Exceptional cases

We further check the following scenarios to improve the precision of the detected instances:

1. The extracted method behaviour is sometimes specialized in an anonymous class. Therefore we also check anonymous classes that override the extracted method.
2. And we further check the comments of the extracted method to find certain keywords in the Javadoc (e.g., virtual, override, etc.) that shows the intention of the developer to override the extracted method behaviour in the future.
Figure 9 depicts the decision tree to detect EXTRACT METHOD refactorings that are used to enable overriding. The extracted method might not be overridden in the current commit but it may have keywords to show that it is intended to be utilized for such a purpose in the future.

Filtering the set of refactoring motivations

• Precedence of Remove Duplication

• Removing Reusable Method, Facilitate Extension, Introduce Factory Method and Remove Decompose to Improve Readability if they are detected for the same refac- toring instance.

3.3.9 Enable Recursion

Recursive structures make it easier for developers to understand, remember and imple- ment algorithms that are more elegantly expressed (Kourie and Watson, 2012). EXTRACT METHOD refactorings can be intended to extract a piece of code to implement a recursive method.
Table 11: Enable Recursion Detection Rule

? ma, mb 3 |calls({ma}, ma)| = 0 ? |calls(mb, mb)| > 0 ?
|{invocationExpression(mb.b) n {this, null}}| > 0
invocationExpression(mb.b):Set of invocation expressions in all of the mb statements.

Generic Motivation Detection Rules

The rule to detect the Enable Recursion motivation is shown in Table 11.

• Source Operation After Extraction (SOAE) should be recursive (with at least one matching invocation inside itself)

Exceptional cases

1. The Source Operation Before Extraction should not be recursive.

2. To ensure that the recursive invocation in the extracted method is calling the extracted method, we check the expression of the matched invocations in the extracted method to ensure that it is either null or this.

Figure 10: Enable Recursion Decision Tree

The decision tree for detecting recursive extracted methods is shown in Figure 10.

Filtering the set of refactoring motivations

No filtering is applied for the detection of this motivation.

3.3.10 Introduce Factory Method

Factory methods create objects and have a non-void return type corresponding ot the type of the object being created (Seng et al., 2006). EXTRACT METHOD refactorings can be utilized to create factory methods by extracting the constructor call to a separate method.
Table 12: Introduce Factory Detection Rule

? mb 3 |newOperator(mb.b.L.Return)| > 0 ?
|{newExpressionType(mb.b) n {mb.t ? subType(mb.t)}}| > 0 ?
{objectCreationV ars(mb.b.L.Return) n mb.t} > 0 ? allFactoryRelated(V ars(mb)) ?
¬factoryMethod(ma)
mb.t:Return type of the method mb.t subType(t): Set of subtypes of the type t
newOperator(S): Statements in S that have only one new object creation operator newExpressionType(S): Set of types of new expressions in the set of statements S objectCreationV ars(S): Set of variables in the set of statements S that are initialized with object-creation expression
V ars(mb):Set of all variables in method mb
allFactoryRelated(V ):Returns true if all the variables in V are used for object creation or state setting
factoryMethod(m): Reutrns true if method m is a factory method.

Generic Motivation Detection Rules

The rule to detect the Introduce Factory Method motivation is shown in Table 12.

• There is an object created in the return statement of the extracted method using the
new keyword.

• And the object that is created in the return statement has an equal type or is a subtype of the extracted method return type.
• OR, All variables appearing in the return statements are initialized with an object creation that is equal to the return type of the extracted method.

Exceptional cases

1. Source Operation Before Extraction (SOBE) should not be a Factory Method. If SOBE is already a factory method the extracted method will not be accepted as a

valid factory method.

2. To ensure that extra statements in the extracted method are contributing to the factory method implementation, we also further ensure that all the statements in the extracted method are related to object creation. This means they should help create the object for the first time or change its state after it is created. Otherwise, we do not consider the extracted method a valid factory method.


Figure 11: Introduce Factory Method Decision Tree

   The decision tree in Figure 11 shows the detection logic to find the motivation of the developer when EXTRACT METHOD refactoring is used to Introduce Factory Method.

Filtering the set of refactoring motivations

• Facilitate Extension and Remove Duplication motivations are removed if they are detected for the same refactoring instance.

3.3.11 Introduce Async Operation

In concurrent programming, developers usually utilize low-level constructs to implement asynchronous operations. Vendors are interested to know the code transformations done by developers to introduce better constructs for improving performance and reducing execu- tion time (Pinto et al., 2015). For instance, Asynchronizer, is a refactoring tool that extracts a piece of sequential code into a concurrent one (Lin et al., 2014). It is important to know how EXTRACT METHOD refactorings are used to extract a piece of code to introduce an asynchronous operation that is executable in a separate thread.
Table 13: Enable Async Detection Rule

?EM 3 |{calls(ma! , mb) n calls(anonymousRunnableClass(ma! ), mb)}| > 0

anonymousRunnableClass(m): Set of anonymous runnable classes in method m

Generic Motivation Detection Rules

The rule to detect the Introduce Async Operation motivation is shown in Table 13.

• There is at least one anonymous class implementing the Runnable interface in the Source Operation After Extraction (SOAE)
• And there is an invocation from within the anonymous class to the extracted method.


Figure 12: Introduce Async Operation Decision Tree

   The detection rules are used to form the decision tree of the EXTRACT METHOD refac- torings which are intended to enable an async operation and shown in Figure 12.

Filtering the set of refactoring motivations

No filtering is applied for this motivation.


3.4 Step 2: Applying the Detection Rules in Large Scale

In the second step, we use the optimized detection rules to detect the developer motivations in a large number of open source projects. We built our dataset based on the projects that were selected in the two most prominent studies on refactoring motivations (Silva et al., 2016; Pantiuchina et al., 2020). More specifically, we included all 185 projects used in the Silva et al. (2016) dataset and all 150 projects used in the Pantiuchina et al. (2020) dataset, reaching a total of 325 projects (as 10 projects were commonly used in both studies). For each one of the 325 projects, we mined their entire commit history (master branch), excluding merge commits, as the analysis of merge commits introduces duplicate refactoring instances (Tsantalis et al., 2013). This resulted in 346K EXTRACT METHOD and EXTRACT AND MOVE METHOD refactoring instances, which were detected in 132,897 commits. We used RefactoringMiner 2.0 as it is the current state-of-the-art refactoring mining tool with the highest precision and recall and fastest execution time among the currently available tools.
   We have also defined certain flags that show the characteristics of code elements and changes in the commit that are related to each EXTRACT METHOD motivation. These flags can be considered as the motivation detection triggers and are logged by the tool. We can manually validate each motivation by checking the type of changes and code characteristics that confirm the corresponding detection rule. Table 14 shows the motivation flags that are used for all the EXTRACT METHOD motivations.

Table 14: Extract Method Refactoring Motivation Flags
Motivation Flags	Flag Description

1. Reusable Method Flags:
EM_INVOCATION_IN_REMOVE_DUPLICATION	EM Invocation is in the SOAE of RemoveDuplication Refactoirng EM_INVOCATION_IN_TEST_OPERATION	EM Invocation is in test Opeataion
SOAE_IS_TEST_OPERATION	SOAE is a Test Operation
EM_INVOCATION_EQUAL_MAPPING	Equal Mapping of invocations in SOBE AND SOAE EM_EQUAL_SINATURE_INVOCATIONS	Invocations to an operation with an equal signature to Extracted Mewthod EM_NESTED_INVOCATIONS	Invocation to an EM from within a Nested Extrcted Method
EM_SOAE_INVOCATIONS	Invocations to the EM from SOAE
EM_NONE_SOAE_INVOCATIONS	Invocations to the EM from Non SOAE methods
2. Introduce Alternative Method Signature Flags:
EM_SOAE_EQUAL_PARAMETER_TYPES	EM has Euqal Paramater Types with SOAE
SOAE_IS_DELEGATE_TO_EM	EM is a delegate to the SOAE
SOAE_IS_ALL_TEMP_VARIABLES	All SOAE statements h temporary variables
EM_HAS_ADDED_PARAMETERS	Extracted Operation has more parameters than the Source Operation
3. Introduce Decompose Method to Improve Readability Flags:
EM_DECOMPOSE_SINGLE_METHOD	Single Method Decomposition to Improve Readability EM_DECOMPOSE_MULTIPLE_METHODS	Multiple Method Decomposition to Improve Readability EM_WITH_SAME_SOURCE_OPERATION	EMs Have same source operation
EM_DISTINCT	EMs are Distinct
EM_INVOCATION_EDIT_DISTANCE_THRESHOLD_SM	EM Invocation Edit Distance in Single Decomposition EM_INVOCATION_EDIT_DISTANCE_THRESHOLD_MM	EM Invocation Edit Distance in Multiple Decomposition EM_ALL_SOURCE_OPERATION_NODES_MAPPED	All the nodes in the source operation are mapped to the Extracted Operation EM_GETTER_SETTER	EM is a Getter/Setter Method
EM_MAPPING_COMPOSITE_NODES	Number of Composite Nodes in the Mapping EM_COMPOSITE_EXPRESSION_INVOCATIONS	Invocations to the EM from the Composite expression EM_COMPOSITE_EXPRESSION_CALLVAR	callVar to the EM exists in the Composite expression EM_RETURN_STATEMENT_INVOCATIONS	Invocations to the EM from the Return Statement
4. Facilitate Extension Flags :
EM_MAPPING_FRAGMENT2_TERNARY	Ternary operator is used in the fragment 2 of the EM Mapping SOAE_NOT_MAPPED_T2_UNFILTERED	Unfiltered notMapped T2 nodes in the SOAE
EM_NOTMAPPED_T2_UNFILTERED	Unfiltered notMapped T2 nodes in the Extracted Operation
SOAE_NOTMAPPED_T2_FILTERED	Filtered notMapped T2 nodes in the SOAE
EM_NOTMAPPED_T2_FILTERED	Filtered notMapped T2 nodes in the Extracted Operation
EM_T2_IN_T1	NotMapped T2 Nodes exist in NotMapped T2 Nodes in the EM
SOAE_T2_IN_T1	NotMapped T2 Nodes exist in NotMapped T2 Nodes in the SOAE
EM_T2_IN_MAPPING	T2 NotMapped Nodes are in the EM(child) mapping
SOAE_T2_IN_MAPPING	T2 NotMapped Nodes are in the SOAE mappings
EM_T2_IE_IN_EM_PARAMETERS	EM T2 NotMapped Nodes Invocation Expressions are in the EM Parameters
SOAE_T2_IE_IN_EM_PARAMETERS	SOAE T2 NotMapped Nodes Invocation Expressions are in the EM Parameters
EM_T2_NEUTRAL	EM T2 NotMapped Nodes are Neutral
SOAE_T2_NEUTRAL	SOAE T2 NotMapped Nodes are Neutral
SOAE_T2_DV_IN_EM_PARAMETERS	SOAE Declared Variable are in the EM Parameters
EM_T2_EM_INVOCATIONS	Extracted Operation T2 has invocation to the EM
SOAE_T2_EM_INVOCATIONS	SOAE T2 has invocation to the EM
5. Remove Duplication Flags :
EM_SAME_EXTRACTED_OPERATIONS	EM operations with the same extracted operations
EM_MAPPING_SIZE	EM Mapping Size
EM_NUM_METHODS_USED_IN_DUPLICATION_REMOVAL	Number of EMs used in Duplication Removal
6. Replace Method Preserving Backwards Compatibility Flags :
EM_SOAE_EQUAL_PARAMETER_TYPES	EM has Euqal Paramater Types with SOAE
SOAE_IS_DELEGATE_TO_EM	EM is a delegate to the SOAE
EM_SOAE_EQUAL_NAMES	EM has equal names with the SOAE
SOAE_DEPRECATED	SOAE is Deprecated
SOAE_PRIVATE	SOAE is Private
SOAE_PROTECTED	SOAE is Protected
7. Improve Testability Flags :
EM_INVOCATION_IN_TEST_OPERATION	EM Invocation is in test Opeataion EM_TEST_INVOCATION_CLASS_EQUAL_TO_EM_CLASS	EM test invocation class is equal to the EM class EM_TEST_INVOCATION_IN_ADDED_NODE	EM test invocation is in added nodes
8. Enable Overriding Flags :
EM_EQUAL_OPERATION_SIGNATURE_IN_SUBTYPE	Equal Operation Signature in SubType of the EM exists EM_OVERRIDING_KEYWORD_IN_COMMENT	EM has overriding keywords in its comment
9. Enable Recursion Flags :
SOBE_RECURSIVE	Source Operation Before Extraction is recursive
EM_RECURSIVE	EM is recursive
10. Introduce Factory Method Flags :
EM_HAS_RETURN_STATEMENTS	EM has return statements EM_RETURN_STATEMENT_NEW_KEYWORDS	new keywords exist in the return statement. EM_RETURN_EQUAL_NEW_RETURN	EM return type equals the object creation type in the return statement EM_OBJECT_CREATION_VARIABLE_RETURNED	Variabale initialized with Object Creation is returned in the EM EM_VARS_FACTORY_METHOD_RELATED	EM Variables are related to object creation for factory method SOBE_FACTORY_METHOD	SOBE is Factory Method
11. Introduce Async Operation Flags :
SOAE_STATEMENTS_CONTAIN_RUNNABLE_TYPE	SOAE statements have Runnable Type SOAE_ANONYMOUS_CLASS_RUNNABLE_EM_INVOCATION SOAE has an anonymous class and runnable type that has invocation to EM
EM : Extracted Method	44
SOAE: Source Operation After Extraction
SOBE: Source Operation Before Extraction



Chapter 4 Experiment Results
In this chapter we present the results of our large scale study on EXTRACT METHOD refac- toring motivations. We conducted an experiment on 346K instances of EXTRACT METHOD and EXTRACT AND MOVE METHOD refactoring operations in 132,897 commits of 325 open-source repositories hosted in GitHub.

4.1 RQ1: Accuracy of Automatic Motivation Extractor

To evaluate the accuracy of our motivation detection rules, we used two separate oracles of developer motivations based on previous studies (Silva et al., 2016; Pantiuchina et al., 2020) as our training and testing datasets, respectively.

4.1.1 Accuracy on the Training Dataset

The oracle provided by Silva et al. (2016) is very convenient for the purpose of training and evaluating our motivation detection rules, as the refactoring motivations are provided at commit-level based on the responses of the actual developers who performed the refac- torings. We filtered all commits that include at least one EXTRACT METHOD refactoring

instance, and then mapped the motivations related to EXTRACT METHOD (i.e., the 11 mo- tivation themes shown in Table 1) to the actual refactoring instances. This process resulted in a total of 261 EXTRACT METHOD refactoring instances assigned with 320 motivation labels, as some instances are involved in multiple motivations.
   After following the process explained in Section 3.2 to optimize our motivation detec- tion rules, we reached the precision and recall shown in Table 15. The overall precision and recall of our tool on the training dataset is 97.2% and 95.9%, respectively.
Table 15: Extract Method Motivations Detection Precision and Recall
Motivation Type
TP
FP
FN
Precision
Recall
1. Reusable Method
72
2
4
0.973
0.947
2. Alternative Method Signature
37
3
3
0.925
0.925
3. Improve Readability
54
0
3
1.000
0.947
4. Facilitate Extension
50
0
0
1.000
1.000
5. Remove Duplication
39
3
1
0.929
0.975
6. Backwards Compatibility
10
1
0
0.909
1.000
7. Improve Testability
8
0
1
1.000
0.889
8. Enable Overriding
3
0
1
1.000
0.750
9. Enable Recursion
2
0
0
1.000
1.000
10. Introduce Factory Method
31
0
0
1.000
1.000
11. Introduce Async Method
1
0
0
1.000
1.000
Total
307
9
13
0.972
0.959


   The high precision and recall achieved on the training dataset might be the result of overfitting (i.e., the detection rules overfit the characteristics of the refactoring instances in the training dataset). Therefore, to ensure that our tool has no overfitting problems, we compute its accuracy on an another refactoring motivation oracle with refactoring instances from different projects.

4.1.2 Accuracy on the Test Dataset

The motivations of refactoring activities were also manually analyzed and tagged at pull request level in a study by Pantiuchina et al. (2020). We used the oracle available in this

study as our test dataset to further validate the accuracy of our motivation detection tool. We filtered the pull requests from the test oracle, which contained only EXTRACT
METHOD or EXTRACT AND MOVE METHOD refactoring instances. Since the motivation labels are assigned at the pull request level, it would be very risky to map the motivation labels to the actual refactoring instances within commits in pull request containing multiple different refactoring types in addition to the EXTRACT METHOD refactoring type. There- fore, we ended up with 56 pull requests in total and assigned the motivation labels to the individual refactoring instances detected in the commits of each pull request. Moreover, be- cause Pantiuchina et al. (2020) use in some cases different names for the motivation themes, we did a mapping between the motivation themes used in the studies by Pantiuchina et al. (2020) and Silva et al. (2016), as shown in Table 16.
   We automatically detected 170 motivations using our tool in these 56 pull requests. 61 motivations were matched according to the mappings shown in Table 16. For the remain- ing 109 motivations that were not matched (i.e., potential false positives), we manually analyzed the commits to ensure that the automatically detected motivation is present in the commits of each pull request.
   Motivation Extractor provides extra information about the detection of a certain moti- vation. For instance, in the case of Reusable Method motivation, we have the fully qualified name of the operation(s) where the extracted method is reused. Furthermore, the detection rule contains numerous flags that are logged during the motivation detection process. These flags that are shown in Table 14 indicate how code elements in the refactoring context are related to the detected motivation. We used these flags along with the information retrieved from the Motivation Extractor to accurately trace each motivation in the pull request com- mits and validate the 109 detected motivations that were not matched with the assigned motivation labels by Pantiuchina et al. (2020).
After manual analysis we found that only 3 out of 109 unmatched motivations were

Table 16: Pull Request Motivation Mapping to Extract Method Motivations
Extract Method Motivations
Pull Request Motivations
Resuable Method
Adhere to DRY principle
Foster code reuse

Introduce Alternative Method Signature
Replace Method Preserving Backwards Compatibility
To adhere to naming convention
To keep consistency in naming To simplify API usage
To use more specific names
To promote API compatibility

Decompose Method to Improve Readability
Better distribute responsibilities
Improve Understandability & Readability Refactoring confusing code

Facilitate Extension
For implementing a new feature
Improve error messages and logging Improve exception handling Improve extensibility
To improve performance
Remove Duplication
Remove Clones

Improve Testability
Improve organization of test directory
Improve quality of test code
To ensure a better mapping between test & production code To simplify testing activities
Enable Overriding
Facilitate subclassing
Enable Recursion
None
Introduce Factory Method
None
Introduce Async Operation
None



None
Cleanup code
Fix warnings from static analysis tools Other motivations
Improve modularization Remove unnecessary code Unclear
Improve Maintainability
To expand abbreviations


actually false positives. The remaining 106 unmatched motivations were actually true pos- itives. We suspect that these motivations were missed by Pantiuchina et al. (2020), because their analysis was done at pull request level, and thus some refactoring instances might be missed from the analysis or considered less significant than others at the pull request level. We also further analyzed the motivations that were tagged in the test oracle at pull request level, but were not detected by our tool (i.e., potential false negatives). For the examined 56 pull requests we had 103 manually tagged motivations among which 21 mo- tivations were matched according to the mappings shown in Table 16. We divided the
remaining 82 unmatched motivations into five categories as following:

1. Super-Motivation: Motivations in the test oracle that were more general than the automatically detected motivations.
2. Sub-Motivation: Motivations in the test oracle that were more specific than the auto- matically detected motivations.
3. Non EXTRACT METHOD Motivation: Motivations in the test oracle that were not related to the EXTRACT METHOD refactoring instances found in the pull request commits.
4. Filtered-out Motivation: Motivations in the test oracle that were filtered out in the automatic motivation detection process due to the fact that another motivation with higher priority was detected.
5. False Negative (FN): Motivations in the test oracle that were not detected by our automatic motivation detection tool. This can be attributed to missed refactoring instances by RefactoringMiner. For instance, if RefactoringMiner detects only one of the instances involved in the removal of duplicated code, then our detection rules cannot infer the Remove Clone motivation.
   Table 17 shows the number of motivations from the Pantiuchina et al. (2020) study that were not detected by our detection rules categorized in the aforementioned categories. To compute the overall precision and recall of the tool we consider the super-motivations and sub-motivations as true positives since it is basically the different approach the two studies followed coming up with more general/specific themes. Filtered motivations are consid- ered as false negatives since our tool does not report them at the end and Non EXTRACT METHOD motivations are removed from the oracle. We consider as false negatives the instances in the FN column.
   Table 18 shows the accuracy of our motivation detection rules on the testing dataset. The Auto column refer to FNs that resulted by analyzing the motivations detected by our













Table 17: Pull Request Motivations
	Unmached Motivations	
Pull Request Motivations	All  Matched(TP)  Super  Sub  Non-EM Motivation  Filtered  FN
1- Adhere to DRY principle
1
1
0
0
0
0
0
2- Foster code reuse
12
4
0
0
4
3
1
3- To adhere to naming convention
1
0
0
0
1
0
0
4- To keep consistency in naming
1
0
0
0
0
0
1
5- To simplify API usage
1
0
0
1
0
0
0
6- To use more specific names
1
0
0
0
1
0
0
7- To promote API compatibility
1
0
0
0
1
0
0
8- Better distribute responsibilities
6
2
0
0
2
0
2
9- Improve Understandability & Readability
15
3
6
0
5
1
0
10- Refactoring confusing code
4
1
0
1
2
0
0
11-To better reflect code responsibility
1
1
0
0
0
0
0
12- For implementing a new feature
6
0
0
3
3
0
0
13- Improve exception handling
3
1
0
1
0
0
1
14- Improve extensibility
3
1
0
0
1
0
1
15- To improve performance
3
0
0
0
3
0
0
16- Remove Clones
7
4
0
0
1
0
2
17- Improve organization of test directory
1
0
0
0
1
0
0
18- Improve quality of test code
1
1
0
0
0
0
0
19- To ensure a better mapping between test & production code
1
0
1
0
0
0
0
20- To simplify testing activities
3
1
1
0
1
0
0
21- Facilitate subclassing
1
1
0
0
0
0
0
22- Cleanup code
9
0
0
2
7
0
0
23- Fix warnings from static analysis tools
1
0
0
0
1
0
0
24- Other motivations
4
0
0
0
4
0
0
25- Improve modularization
2
0
0
2
0
0
0
26- Remove unnecessary code
2
0
0
1
1
0
0
27- Unclear
9
0
0
0
9
0
0
28- Improve Maintainability
2
0
0
0
2
0
0
29- To expand abbreviations
1
0
0
0
1
0
0
Total
103
21
8
12
50
4
8

tool. The Manual columns refer to TPs and FNs, respectively, that resulted by analyzing the motivations manually labelled in the test oracle. The Validated column refers to TPs that resulted by analyzing the motivations detected by our tool, for which no matching motivation label was found in the test oracle.
Table 18: Precision and Recall of Extract Method Motivations in Pull Requests
	TP	 	FN	
Motivation Type	Matched  Validated  Manual   Auto  Filtered  Manual  FP  TN  Precision  Recall
1. Reusable Method
5
20
0
0
3
1
0
4
1.000
0.862
2. Alternative Method Signature
0
16
2
1
0
1
3
2
0.857
0.900
3. Improve Readability
14
6
7
0
1
1
0
9
1.000
0.931
4. Facilitate Extension
12
24
4
0
0
3
0
7
1.000
0.930
5. Remove Duplication
27
31
0
0
0
2
0
1
1.000
0.967
6. Backwards Compatibility
0
3
0
0
0
0
0
0
1.000
1.000
7. Improve Testability
2
3
2
0
0
0
0
2
1.000
1.000
8. Enable Overriding
1
0
0
0
0
0
0
0
1.000
1.000
9. Enable Recursion
0
1
0
0
0
0
0
0
1.000
1.000
10. Introduce Factory Method
0
1
0
0
0
0
0
0
1.000
1.000
11. Introduce Async Method
0
1
0
0
0
0
0
0
1.000
1.000
12. None
0
0
5
0
0
0
0
25
-
-
Total
61
106
20
1
4
8
3
50
0.984
0.935


   There are 21 manually tagged motivations at pull request level that were matched with 61 automatically detected motivations at refactoring instance level. Overall, we identi- fied 187 refactoring-level true positives including 61 matched, 106 Validated, 20 Manual
(12 sub-motivations and 8 super-motivations), and 13 false negatives (4 Filtered-out, 1 Auto, 8 Manual). Finally, we eliminated 50 tagged motivations as true negatives. These motivations were not specifically related to EXTRACT METHOD refactoring instances in the pull requests. These tags were either related to the pull request comments and dis- cussions, or code changes not involving Extract Method refactorings. In some cases the motivation was related to EXTRACT METHOD refactorings, but the manual analysis of the changes in the pull request resulted in an incorrect motivation. For instance, instances re- lated to Remove Duplication or Improve Testability motivations were mistakenly classified as Reusable Method, because the analysis of the extracted method call sites was not prop- erly done. We computed an overall precision of 98.4% and recall of 93.5% on the testing dataset, which are very close to the precision and recall achieved on the training dataset (97.2% and 95.9%, respectively).





4.2 RQ2: Most Prevalent Motivations for Extract Method Refactoring Operations
In Figure 13, we present the ranking of the motivations we automatically extracted by ana- lyzing 346K instances of EXTRACT METHOD and EXTRACT AND MOVE METHOD refac- toring operations in 132,897 commits of 325 open-source repositories hosted in GitHub. Moreover, we show side-by-side the ranking of the motivations obtained from the survey by Silva et al. (2016) based on developer answers. With red arrows we highlight some notable differences or similarities in the two rankings.
Automatic Extract Method Motivations	Survey Results (FSE 2016)




Introduce Async Method Enable Recursion Enable Overriding Backwards Compatibility Improve Testability
          Introduce Factory Method Alternative Method Signature Decompose to Improve Readability
 Facilitate Extension Remove Duplication Reusable Method

Introduce Async Method Introduce Factory Method Enable Recursion
Enable Overriding Backwards Compatibility Improve Testability Remove Duplication Facilitate Extension
Decompose to Improve Readability Alternative Method Signature Reusable Method


Figure 13: Comparison of our automatically extracted motivation ranking with Silva et al. (2016) survey ranking

   Reusable Method is the top-most frequent EXTRACT METHOD refactoring motivation in both studies. About 41% of the examined refactoring instances are intended for reusing the extracted code within the same commit. Unfortunately, there is a huge research gap in recommendation systems for extracting reusable methods and components. Therefore,

researchers that are building refactoring recommendation tools should focus on developing techniques that find opportunities to extract reusable methods. The characteristics of the reusable extracted methods that we found in our dataset using our motivation detection rules can be a useful resource to better understand the factors driving this particular motivation. In a recent study, AlOmar et al. (2020) found that extracting reusable functionality is increasing the number of methods in a class. Therefore, developers can pull up reusable methods to a superclass to share them with all subclasses, or relocate and move them in classes outside their inheritance hierarchy. At the same time, the visibility of the reusable
methods can be changed to allow other objects and methods to access their functionality. We found that 31% of the reusable extracted methods are a result of EXTRACT AND
MOVE METHOD refactorings and 69% of them are a result of EXTRACT METHOD refactor- ings. We also analyzed the access modifiers of the extracted methods and compared them with the access modifiers of the methods from which they were extracted. We ordered access modifiers from higher to lower visibility levels (public > protected > package > pri- vate) and recorded whether the visibility of the extracted methods increased, decreased, or remained the same compared to the visibility of the original methods from which they were extracted.
   Figure 14 shows the percentages of resuable extracted methods with increased, de- creased and the same visibility for EXTRACT METHOD and EXTRACT AND MOVE METHOD refactorings, respectively. About 58% of the extracted methods in EXTRACT METHOD refactorings have a decreased visibility and about 4% have an increased visibility. More- over, in 74% of the EXTRACT METHOD refactorings with decreased visibility, the extracted methods are private, while the methods from which they were extracted are public. This shows that locally extracted methods are more intended for local reuse within the class in which they are extracted. On the other hand, in the case of EXTRACT AND MOVE METHOD refactorings about 18% of the extracted methods have a decreased visibility and about 27%





100000






80000






60000






40000






20000






0
Extract Method	Extract And Move Method Refactoring Type
Figure 14: Reusable Extracted Methods Visibility Changes

of them have an increased visibility. Therefore, when the extracted methods are moved to other classes the intention is to make reusable in more classes other than the class from which they were extracted.
   Liu and Liu (2016) suggest that practitioners should consider the cost and benefits of EXTRACT METHOD refactoring. They find that extracted methods have a small chance to be reused in the future and on the average 17% of them are reused in the future. But at the same time they find that 39% of the extracted methods are reused immediately and are also reused in the future in order to remove clones. Therefore, the development of techniques for recommending the extraction of reusable methods and components has great merit. In our study we found that about 30% of reusable methods are also used to remove duplication in the same commit. We will further discuss the multiplicity of EXTRACT METHOD motivations in Section 4.4.
   Remove Duplication motivation is ranked in the second place, while it is ranked fifth place in the Silva et al. (2016) study. About 25% (89111 instances) of all examined EX- TRACT METHOD and EXTRACT AND MOVE METHOD refactorings have Remove Dupli- cation as their motivation, which shows that the refactoring of clones is an important reason for extracting methods.
   Duplication removal can be performed from multiple different source methods, as well as from a single source method. 64% of the Remove Duplication motivations are re- lated to EXTRACT METHOD and the remaining 36% are related to EXTRACT AND MOVE METHOD refactorings. Figure 15 shows the number of Remove Duplication motivation in- stances, in which the involved EXTRACT METHOD and EXTRACT AND MOVE METHOD refactorings have been extracted from multiple methods or a single method, respectively. We can see that 37% of all instances are extracted from a single source method, while 63% are extracted from multiple source methods.
Due to the importance of clone detection and refactoring many researchers developed












50000



40000



30000



20000



10000



0
Multiple Method	Single Method
Duplication Removal
Figure 15: Multiple Method vs. Single Method Duplication Removal

methods and tools to eliminate clones. Tairas and Gray (2012) propose CeDAR that uni- fies clone detection and refactoring by filtering out clones and reporting only those clones that can be refactored with EXTRACT METHOD refactorings. Mazinanian et al. (2016) extended JDeodorant to import the clone detection results of CCFinder, NICAD, ConQat, CloneDR and Deckard clone detection tools, perform refactorability analysis (Tsantalis et al., 2015), and support multiple clone refactorings, such as EXTRACT METHOD, EX- TRACT AND PULL UP METHOD, INTRODUCE TEMPLATE METHOD, and INTRODUCE
UTILITY METHOD. In all these refactorings the differences between the refactored clones can be parameterized with Lambda expressions if needed. Chen et al. (2018) propose a pattern-based clone refactoring technique to summarize refactorings of duplicated codes and find clones that are not consistently refactored. Yue et al. (2018) propose a learning- based approach and a tool named CREC that uses 34 features for the characterization of clones to automatically suggest EXTRACT METHOD refactorings for eliminating clones. Our findings and dataset about single method and multiple method duplication removals can be used to better train the clone detection models and find features that are useful to eliminate clones using various clone-removal-related refactorings.
   Facilitate Extension motivation is ranked third in our large scale study and is ranked fourth in the survey by Silva et al. (2016). 20.51% of refactoring instances in our study are detected having Facilitate Extension as motivation. Among these refactoring instances, 81.70% are EXTRACT METHOD and the remaining 18.30% are EXTRACT AND MOVE METHOD refactorings. We discuss the characteristics of EXTRACT METHOD refactorings that are used to Facilitate Extension in more detail in Section 4.3.
   Although the have been some recent efforts in the recommendation of refactorings based on feature requests (Nyamawe et al., 2019, 2020), this area is still under-researched. Developers spend most of their time trying to fix bugs and implement new requirements.

Therefore, there is a great need in recommending refactorings based on the particular main- tenance task they are currently working on that could help them to complete their task faster and improve the overall quality of the code involved in the maintenance task.
   Decompose Method to Improve Readability motivation is ranked fourth in our large scale study and third in the survey by Silva et al. (2016). About 18% of all refactoring instances are detected having Decompose to Improve Readability as motivation. Among these instances, about 78% are EXTRACT METHOD and the remaining 12% are EXTRACT AND MOVE METHOD refactorings.
   Figure 16 shows the number of Decompose Method motivation instances, in which the involved EXTRACT METHOD and EXTRACT AND MOVE METHOD refactorings introduce multiple extracted methods, and a single method, respectively. in 65% of these instances the source method is decomposed into multiple methods, while in the remaining 35% only a single method is extracted. This results shows that in general when the developers' in- tention is to decompose a method, they typically extract multiple methods from the source method. In the literature, there as many refactoring recommendation systems targeting the decomposition of methods. Some of the most well-known refactoring recommendation tools supporting the Decompose Method motivation are JDeodorant (Tsantalis and Chatzi- georgiou, 2011), JExtract (Silva et al., 2014), SEMI (Charalampidou et al., 2017), and GEMS (Xu et al., 2017).
   Introduce Alternative Method Signature motivation is ranked fifth in our large scale study and second in the survey by Silva et al. (2016). To the best of knowledge, there are no recommendation systems targeting this particular motivation for EXTRACT METHOD refactoring. Therefore, the development of techniques for recommending the extraction of methods with an alternative signature to support different input parameter types or output types has a lot of merit.
Finally, we can observe in Figure 13 that the Introduce Factory Method motivation is










40000


35000


30000


25000


20000


15000


10000


5000


0
Multiple Method	Single Method
Decomposition to Improve Readability
Figure 16: Multiple and Single Method Decomposition to Improve Readability

ranked sixth in our study with a much larger representation compared to the survey by Silva et al. (2016), in which it is ranked last among the 11 motivations. The development of recommendation systems supporting this motivation is quite straightforward, as it in- volves finding object creations or builder call chains that are complex or long enough to be extracted into a separate method. Factory methods are also great for extendibility, as they allow the consumers to create new objects without having to know the details of how they are created, or what their dependencies are.















4.3 RQ3: What are the characteristics of the EXTRACT METHOD refactorings having Facilitate Extension as motivation
Among all 346k EXTRACT METHOD refactoring instances, 71136 (20.51%) are related to the Facilitate Extension motivation, i.e., refactorings performed to facilitate the addition of new features or fixing of bugs.
   To better understand how developers use EXTRACT METHOD refactorings for exten- sion, we analyzed the added AST nodes (i.e., new statements) in the extracted method and in the source method after extraction.
Among the 71136 refactoring instances having Facilitate Extension as motivation, 26.66%

have added nodes in the Source Operation After Extraction (SOAE), 61.30% have added nodes in the Extracted Method and 8.41% have added nodes in both SOAE and Extracted Method. Also in 3.74% of the instances the extension is done with the use of a ternary operator in the extracted method. Therefore, the extensions are more often performed in the extracted method than the source operation after extraction.
Table 19: Self-affirmed refactoring patterns

Patterns

Added more checks for quality factors
Fix quality issue
Reformat
Antipattern bad for performances
Fixing naming convention
Remove
Avoid future confusion
Flaw
Remove dependency
Chang
Formatted
Remove redundant code
Change design
Get rid of
Removed poor coding practice
Change package structure
Getting code out of
Renam
Cleaned up unused classes
Improv
Renamed for consistency
Cleanup
Improve naming consistency
Reorganiz
Code cleansing
Improvement
Reorganize project structures
Code maintenance for refactoring
Inlin
Replac
Code optimization
Inlined unecessary classes
Replace it with
Code quality
Issue
Restructur
Code redundancies
Make maintenance easier
Rework
Code reformat
Minor enhancement
Rewrit
Code reordering
Modify
Simplification
Code revision
Modularize the code
Simplify
Cosmetic
mov
Simplify code redundancies
Creat
Moved and gave clearer names to
Simplify internal design
Decompos
Moved more code out of
Simplify the code
Deleting a lot of old stuff
Naming improvement
Smell
Easily extend
Nicer name
Split
Encapsulat
Nonfunctional
Structural change
Enhanc
Polishing code
Structure
Enhanced code beauty
Pull some code up
Technical debt
Extend
Quality factor
Unnecessary code
Extract
Redesign
Unused code
Fix
Reduc
Use a safer method
Fix a desgin flaw
Refactor
Use better name
Fix module structure
Refactor bad designed code
Use less code
Fix quality flaw
Refactoring towards nicer name analysis



   We further used the self-affirmed refactoring patterns catalogued by AlOmar et al. (2019a) to find commit messages that are related to self-affirmed refactoring activities. Ta- ble 19 shows 89 refactoring patterns that we used. These are some keywords and phrases that are commonly used by developers when they document their refactoring activities in

commit messages.
   For some patterns we used a more general subset to find matching commit messages. For instance, patterns like Fix a design flaw or Fix a quality flaw had 0 and 7 matches, respectively, but we found similar variations of the pattern by applying the general Flaw keyword with 14 matches. We excluded some patterns like Merg and Add. Merg was used in many automatically generated merge commit messages for pull requests. Add is also a general keyword that might not be directly associated with a certain task like adding a new feature besides refactoring. The remaining keywords were utilized to understand what kind of maintenance tasks are supported by the EXTRACT METHOD refactoring instances having Facilitate Extension as motivation.














Remove












Mov

Figure 17: Top SAR patterns in message of commits that include EXTRACT METHOD with
Facilitate Extension as motivation

   Figure 17 shows the top-10 most used keywords in the message of commits including EXTRACT METHOD refactorings having Facilitate Extension as motivation. About 28%

of instances with self-affirmed keywords include the Fix keyword, which is the most fre- quent pattern that can be associated with a bug-fixing task. This suggests that EXTRACT METHOD refactoring is a very useful tool for fixing bugs.
   The remaining self-affirmed refactoring patterns in the top-10 list cannot be directly associated with bug fixing or the implementation of a new feature, but they are clearly related with some maintenance task affecting the functionality of the project, as indicated from Chang, Issue, or the organization of the project, as indicated from Mov, Remove and Cleanup. The remaining 80 self-affirmed patterns constitute 17% of the matched keywords.















4.4 RQ4: Multiple concurrent EXTRACT METHOD Moti- vations
In this section, we analyze the co-existence of multiple motivations for the same EXTRACT METHOD refactoring instance. Among all refactoring instances, 56% of them have only a single motivation, while 37% of them have multiple motivations. More specifically, 34% of all refactoring instances have two different motivations, and about 2.5% have 3 dif- ferent motivations. Figure 18 shows the percentages of refactoring instances falling into four different categories, namely single motivation, two motivations, three motivations, no motivation.

3 Motivations





2 Motivations



Single Motivation






Figure 18: Extract Motivation Motivation Detection Rate

   In about 7% of the instances, no specific motivation is automatically detected. We fur- ther manually analyzed a random selection of these instances and found that about 50% of them only contain one statement in the extracted method. In about 25% of them, the entire body of the source method was extracted, and the remaining 25% were not related to any of the 11 motivation categories considered in our study. These instances can serve for further improving the recall of our motivation detection rules, as some motivations were missed, but also discover new motivation categories, which were not previously documented in the literature.
   We used association rule mining to better understand the relationship between multi- ple refactoring motivations co-existing in EXTRACT METHOD refactoring instances. The association rules we mined are shown in Table 20. We found the association of motiva- tions among all instances with multiple motivations, and also separately for categories with only two or three motivations. We found consistently in all categories a strong associa- tion between Remove Duplication and Reusable Method. This means that when developers remove duplication, they also tend to find opportunities to reuse the extracted duplicated code.

Table 20: Association Rules for EXTRACT METHOD concurrent motivations
Multiplicity	Rule
Support
Confidence
Lift
All	RD ?RM
0.326
0.888
1.143
FE?RM
0.243
0.679
0.874
2 Motivations	RD ?RM
0.328
0.882
1.152
FE?RM
0.222
0.647
0.846
RD ?RM
0.290
0.988
1.057
3 Motivations	DIR ?RM
0.688
0.946
1.012
FE ?RM
0.528
0.944
1.010
FE ?DIR
0.523
0.934
1.284
RM : Reusable Method
RD : Remove Duplication
DIR : Decompose Method to Improve Readability
FE : Facilitate Extension










Chapter 5

Threats to Validity

5.1 Internal Validity

The internal threats to the validity of our study involve biases or errors. We validated and improved our refactoring motivation detection rules based on a primary oracle that was built from the actual developer responses in 222 commits (Silva et al., 2016). However, to evaluate the accuracy of our detection rules, we compared the motivations detected from our tool with manually validated motivations found in pull requests (Pantiuchina et al., 2020). The number of refactoring instances were limited for some motivation categories, such as Enable Async Operation and Enable Overriding. Therefore, the detection rules for these motivations may be further improved, if more cases are discovered in the future studies.
   One more threat is that we had to reconstruct the two available motivation oracles from commit level (Silva et al., 2016), and pull request level (Pantiuchina et al., 2020), respec- tively, to refactoring instance level. This means we had to map the motivations given at commit and pull request levels to specific refactoring instances found in the commits. How- ever, as explained in Section 4.1, we followed a very careful and systematic process when performing the mapping of the motivations from a higher level of granularity (i.e., commit

and pull request) to a lower one (i.e., refactoring instance).


5.2 Construct Validity

The construct threats to validity are mainly concerned with the accuracy that Refactor- ingMiner detects EXTRACT METHOD and EXTRACT AND MOVE METHOD refactoring operations in the commit history of a project, as well as the level of detail Refactoring- Miner provides for the context in which the refactoring operations are applied (e.g., the call sites of the extracted method within the project's code base, the added statements in the bodies of the extracted and source methods). The accuracy of RefactoringMiner in the detection of EXTRACT METHOD has been shown to be very high with a precision of 99.8% and recall of 95.8% (Tsantalis et al., 2020), which gives us confidence that the vast majority of the detected refactoring instances in our dataset are correct (almost zero false positives), and that we didn't miss many true instances (i.e., low number of false negatives). Moreover, RefactoringMiner matches the refactored code at statement level, and provides a fine-grained level of details about the call sites of the extracted methods, the statements matched between the source and the extracted methods, and the newly added statements in the bodies of the extracted and source methods. This level of detail allowed us to build very accurate motivation detection rules taking into account several special cases.

5.3 External Validity

In this study, we focused on EXTRACT METHOD and EXTRACT AND MOVE METHOD refactoring types, but there is a large catalogue of refactoring types, which we did not ex- amine. This is due to the fact that EXTRACT METHOD in very common and extensively applied by developers. Moreover, it is the only refactoring type that serves so many differ- ent motivations. Our general method can be extended to detect the motivations for other

refactoring types as well.
   We selected popular Java systems of various sizes and domains that were previously used in other researches to know why developers refactor source code. In our large scale study we analyzed all the commits in 325 projects to have a better understanding of the motivations driving EXTRACT METHOD refactoring. Our approach is utilizing Refactor- ingMiner, which only supports Java systems. But our motivation detection rules can be generalized to other programming languages, provided that the refactoring mining tools supporting other languages can provide thorough information about the context of the de- tected refactoring operations to replicate this study.







Chapter 6

Conclusion and future work

Refactoring is a well-known practice among developers to improve the quality of a software system by altering its internal structure without changing the external behaviour. Among all refactoring activities, EXTRACT METHOD is widely utilized to refactor code for various motivations. In this study we propose a method for the automatic detection of the motiva- tions driving the application of EXTRACT METHOD and EXTRACT AND MOVE METHOD refactoring operations. We conducted a large scale study on 325 open-source java projects, which included 346k EXTRACT METHOD and EXTRACT AND MOVE METHOD instances. We developed detection rules to automatically detect 11 major motivations of these refac- torings. The automated motivation detection results are validated against an oracle with manually labelled motivations on refactorings taking place in pull requests with a precision of 98% and recall of 93%.
   In out large-scale empirical study, we found that Reusable Method, Remove Duplica- tion, Facilitate Extension and Decompose Method to Improve Readability are the the most common reasons for applying EXTRACT METHOD refactorings. We further studied the EXTRACT METHOD instances that have more than one motivation. About 56% of the refactoring instances had a single motivation and about 36% had multiple motivations, while 7% had no detection motivation. We performed association rule mining and found

that the Remove Duplication and Reusable Method motivations have a strong association. This means that when developers remove duplication, they also tend to find opportunities to reuse the extracted duplicated code.
   The results show the feasibility and effectiveness of our approach to detect the motiva- tions related to the EXTRACT METHOD refactoring type. In the future, we aim to extend our tool to automatically detect the motivation of other refactoring types besides EXTRACT METHOD. This will provide to researchers and tool-makers the empirical evidence required to build refactoring recommendation systems tailored to the developer needs and practices.






Bibliography

Aalizadeh, M. S. (2021). Motivation extractor. https://github.com/mosaliza/ RefactoringMiner.
Abid, C., Kessentini, M., Alizadeh, V., Dhouadi, M., and Kazman, R. (2020). How does refactoring impact security when improving quality? a security-aware refactoring ap- proach. IEEE Transactions on Software Engineering, pages 1-1.
AlOmar, E., Mkaouer, M. W., and Ouni, A. (2019a). Can refactoring be self-affirmed? an exploratory study on how developers document their refactoring activities in commit messages. In 2019 IEEE/ACM 3rd International Workshop on Refactoring (IWoR), pages 51-58.
AlOmar, E. A., Mkaouer, M. W., and Ouni, A. (2021). Toward the automatic classification of self-affirmed refactoring. Journal of Systems and Software, 171:110821.
AlOmar, E. A., Mkaouer, M. W., Ouni, A., and Kessentini, M. (2019b). On the impact of refactoring on the relationship between quality attributes and design metrics. In 2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measure- ment (ESEM), pages 1-11.
AlOmar, E. A., Rodriguez, P. T., Bowman, J., Wang, T., Adepoju, B., Lopez, K., Newman, C., Ouni, A., and Mkaouer, M. W. (2020). How do developers refactor code to improve code reusability? In International Conference on Software and Software Reuse, pages 261-276. Springer.
Arcelli, D., Cortellessa, V., and Di Pompeo, D. (2018). Performance-driven software model refactoring. Information and Software Technology, 95:366-397.
Bavota, G., De Lucia, A., Di Penta, M., Oliveto, R., and Palomba, F. (2015). An experi- mental investigation on the innate relationship between quality and refactoring. J. Syst. Softw., 107(C):1-14.
Bogart, A., AlOmar, E. A., Mkaouer, M. W., and Ouni, A. (2020). Increasing the trust in refactoring through visualization. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops, pages 334-341.

Charalampidou, S., Ampatzoglou, A., Chatzigeorgiou, A., Gkortzis, A., and Avgeriou, P. (2017). Identifying extract method refactoring opportunities based on functional rele- vance. IEEE Transactions on Software Engineering, 43(10):954-974.
Chávez, A., Ferreira, I., Fernandes, E., Cedrim, D., and Garcia, A. (2017). How does refactoring affect internal quality attributes? a multi-project study. In Proceedings of the 31st Brazilian Symposium on Software Engineering, SBES'17, page 74-83, New York, NY, USA. Association for Computing Machinery.
Chen, J., Xiao, J., Wang, Q., Osterweil, L. J., and Li, M. (2016). Perspectives on refactoring planning and practice: An empirical study. Empirical Softw. Engg., 21(3):1397-1436.
Chen, Z., Kwon, Y.-W., and Song, M. (2018). Clone refactoring inspection by summarizing clone refactorings and detecting inconsistent changes during software evolution. Journal of Software: Evolution and Process, 30(10):e1951.
Derezin´ska, A. (2017). A structure-driven process of automated refactoring to design pat- terns. In International Conference on Information Systems Architecture and Technology, pages 39-48. Springer.
Dig, D., Comertoglu, C., Marinov, D., and Johnson, R. (2006). Automated detection of refactorings in evolving components. In Thomas, D., editor, ECOOP 2006 - Object- Oriented Programming, pages 404-428, Berlin, Heidelberg. Springer Berlin Heidelberg.
Dig, D. and Johnson, R. (2006). How do apis evolve? a story of refactoring. Journal of software maintenance and evolution: Research and Practice, 18(2):83-107.
Ferreira, I. V. (2018). Assessing the Bug-Proneness of Refactored Code: Longitudinal Multi-Project Studies. PhD thesis, PUC-Rio.
Hora, A. C. and Robbes, R. (2020). Characteristics of method extractions in java: a large scale empirical study. Empir. Softw. Eng., 25(3):1798-1833.
Ivers, J., Ozkaya, I., Nord, R. L., and Seifried, C. (2020). Next generation automated soft- ware evolution refactoring at scale. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Soft- ware Engineering, ESEC/FSE 2020, page 1521-1524, New York, NY, USA. Association for Computing Machinery.
Kaur, P. and Mittal, P. (2017). Impact of clones refactoring on external quality attributes of open source softwares. International Journal of Advanced Research in Computer Science, 8(5).
Kaya, M., Conley, S., Othman, Z. S., and Varol, A. (2018). Effective software refactoring process. In 2018 6th International Symposium on Digital Forensic and Security (ISDFS), pages 1-6.

Kim, M. and Notkin, D. (2009). Discovering and representing systematic code changes. In Proceedings of the 31st International Conference on Software Engineering, ICSE '09, page 309-319, New York, NY, USA. Association for Computing Machinery.
Kim, M., Zimmermann, T., and Nagappan, N. (2014). An empirical study of refactor- ingchallenges and benefits at microsoft. IEEE Trans. Softw. Eng., 40(7):633-649.
Kourie, D. G. and Watson, B. W. (2012). Procedures and recursion. In The Correctness- by-Construction Approach to Programming, pages 161-195. Springer.
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707-710. Doklady Akademii Nauk SSSR, V163 No4 845-848 1965.
Lin, Y., Radoi, C., and Dig, D. (2014). Retrofitting concurrency for android applications through refactoring. In Proceedings of the 22nd ACM SIGSOFT International Sympo- sium on Foundations of Software Engineering, pages 341-352.
Liu, W. and Liu, H. (2016). Major motivations for extract method refactorings: analysis based on interviews and change histories. Frontiers of Computer Science, 10(4):644- 656.
LUO, T., bo GUO, Y., hui HAO, Y., and LI, H. (2011). Method verifying the correctness of code refactoring program. Journal on Communications, 32(11A):152.
Mazinanian, D., Tsantalis, N., Stein, R., and Valenta, Z. (2016). Jdeodorant: clone refac- toring. In Proceedings of the 38th international conference on software engineering companion, pages 613-616.
Mohan, M. and Greer, D. (2017). Multirefactor: automated refactoring to improve software quality. In International Conference on Product-Focused Software Process Improvement, pages 556-572. Springer.
Nasagh, R. S., Shahidi, M., and Ashtiani, M. (2021). A fuzzy genetic automatic refac- toring approach to improve software maintainability and flexibility. Soft Computing, 25(6):4295-4325.
Negara, S., Chen, N., Vakilian, M., Johnson, R. E., and Dig, D. (2013). A compara- tive study of manual and automated refactorings. In Proceedings of the 27th European Conference on Object-Oriented Programming, ECOOP'13, pages 552-576, Berlin, Hei- delberg. Springer-Verlag.
Nyamawe, A. S., Liu, H., Niu, N., Umer, Q., and Niu, Z. (2019). Automated recommenda- tion of software refactorings based on feature requests. In 2019 IEEE 27th International Requirements Engineering Conference (RE), pages 187-198. IEEE.

Nyamawe, A. S., Liu, H., Niu, N., Umer, Q., and Niu, Z. (2020). Feature requests-based recommendation of software refactorings. Empirical Software Engineering, 25(5):4315- 4347.
Nyamawe, A. S., Liu, H., Niu, Z., Wang, W., and Niu, N. (2018). Recommending refactor- ing solutions based on traceability and code metrics. IEEE Access, 6:49460-49475.
Opdyke, W. F. (1990). Refactoring : An aid in designing application frameworks and evolving object-oriented systems. Proc. SOOPPA '90 : Symposium on Object-Oriented Programming Emphasizing Practical Applications.
Paixão, M., Uchôa, A., Bibiano, A. C., Oliveira, D., Garcia, A., Krinke, J., and Arvonio,
E. (2020). Behind the Intents: An In-Depth Empirical Study on Software Refactoring in Modern Code Review, page 125-136. Association for Computing Machinery, New York, NY, USA.
Pantiuchina, J., Zampetti, F., Scalabrino, S., Piantadosi, V., Oliveto, R., Bavota, G., and Penta, M. D. (2020). Why developers refactor source code: A mining-based study. ACM Trans. Softw. Eng. Methodol., 29(4).
Perkins, J. H. (2005). Automatically generating refactorings to support api evolution. In proceedings of the 6th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering, pages 111-114.
Pinto, G., Torres, W., Fernandes, B., Castor, F., and Barros, R. S. (2015). A large-scale study on the usage of java's concurrent programming constructs. Journal of Systems and Software, 106:59-81.
Prete, K., Rachatasumrit, N., Sudan, N., and Kim, M. (2010). Template-based reconstruc- tion of complex refactorings. In Proceedings of the 2010 IEEE International Conference on Software Maintenance, ICSM '10, page 1-10, USA. IEEE Computer Society.
Seng, O., Stammel, J., and Burkhart, D. (2006). Search-based determination of refactorings for improving the class structure of object-oriented systems. In Proceedings of the 8th annual conference on Genetic and evolutionary computation, pages 1909-1916.
Silva, D., Silva, J., De Souza Santos, G. J., Terra, R., and Valente, M. T. O. (2020). Refd- iff 2.0: A multi-language refactoring detection tool. IEEE Transactions on Software Engineering, pages 1-1.
Silva, D., Terra, R., and Valente, M. T. (2014). Recommending automated extract method refactorings. In Proceedings of the 22nd International Conference on Program Compre- hension, ICPC 2014, page 146-156, New York, NY, USA. Association for Computing Machinery.
Silva, D., Tsantalis, N., and Valente, M. T. (2016). Why we refactor? confessions of github contributors. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium

on Foundations of Software Engineering, FSE 2016, page 858-870, New York, NY, USA. Association for Computing Machinery.
Silva, D. and Valente, M. T. (2017). Refdiff: Detecting refactorings in version histories. In 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR), pages 269-279.
Silva, I. P., Alves, E. L., and Machado, P. D. (2018). Can automated test case generation cope with extract method validation? In Proceedings of the XXXII Brazilian Symposium on Software Engineering, pages 152-161.
Stefano, M. D., Pecorelli, F., Tamburri, D. A., Palomba, F., and Lucia, A. D. (2020). Refac- toring recommendations based on the optimization of socio-technical congruence. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), pages 794-796.
Tairas, R. and Gray, J. (2012). Increasing clone maintenance support by unifying clone detection and refactoring activities. Information and Software Technology, 54(12):1297- 1307.
Tanhaei, M. (2020). A model transformation approach to perform refactoring on software architecture using refactoring patterns based on stakeholder requirements. AUT Journal of Mathematics and Computing, 1(2):179-216.
Tarlinder, A. (2016). Developer testing: Building quality into software. Addison-Wesley Professional.
Tsantalis, N. and Chatzigeorgiou, A. (2011). Identification of extract method refactor- ing opportunities for the decomposition of methods. Journal of Systems and Software, 84(10):1757-1782.
Tsantalis, N., Guana, V., Stroulia, E., and Hindle, A. (2013). A multidimensional empirical study on refactoring activity. In Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research, CASCON '13, pages 132-146, Riverton, NJ, USA. IBM Corp.
Tsantalis, N., Ketkar, A., and Dig, D. (2020). Refactoringminer 2.0. IEEE Transactions on Software Engineering, pages 1-1.
Tsantalis, N., Mansouri, M., Eshkevari, L. M., Mazinanian, D., and Dig, D. (2018). Accu- rate and efficient refactoring detection in commit history. In Proceedings of the 40th In- ternational Conference on Software Engineering, ICSE '18, pages 483-494, New York, NY, USA. ACM.
Tsantalis, N., Mazinanian, D., and Krishnan, G. P. (2015). Assessing the refactorability of software clones. IEEE Transactions on Software Engineering, 41(11):1055-1090.

Vashisht, H., Bharadwaj, S., and Sharma, S. (2018). Analysing of impact of code refac- toring on software quality attributes. IJ Scientific Research and Engineering Trends, 4:1127-1131.
Vassallo, C., Grano, G., Palomba, F., Gall, H. C., and Bacchelli, A. (2019). A large-scale empirical exploration on refactoring activities in open source software projects. Science of Computer Programming, 180:1-15.
Wang, Y. (2009). What motivate software engineers to refactor source code? evidences from professional developers. In 2009 IEEE International Conference on Software Main- tenance, pages 413-416.
Xu, S., Sivaraman, A., Khoo, S.-C., and Xu, J. (2017). Gems: An extract method refactor- ing recommender. In 2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE), pages 24-34.
Yang, L., Liu, H., and Niu, Z. (2009). Identifying fragments to be extracted from long methods. In 2009 16th Asia-Pacific Software Engineering Conference, pages 43-49. IEEE.
Yue, R., Gao, Z., Meng, N., Xiong, Y., Wang, X., and Morgenthaler, J. D. (2018). Auto- matic clone recommendation for refactoring based on the present and the past. In 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME), pages 115-126. IEEE.



MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG and Multi-Agent LLM Collaboration

Yisen Xu
SPEAR Lab, Concordia University Montreal, Canada yisen.xu@mail.concordia.ca

Feng Lin
SPEAR Lab, Concordia University Montreal, Canada feng.lin@mail.concordia.ca

Jinqiu Yang
O-RISA Lab, Concordia University Montreal, Canada jinqiu.yang@concordia.ca





ABSTRACT

Tse-Hsun (Peter) Chen SPEAR Lab, Concordia University Montreal, Canada peterc@encs.concordia.ca

Nikolaos Tsantalis
Department of Computer Science and Software Engineering, Concordia University
Montreal, Canada nikolaos.tsantalis@concordia.ca
ACM Reference Format:

Maintaining and scaling software systems relies heavily on effective code refactoring, yet this process remains labor-intensive, requir- ing developers to carefully analyze existing codebases and prevent the introduction of new defects. Although recent advancements have leveraged Large Language Models (LLMs) to automate refac- toring tasks, current solutions are constrained in scope and lack mechanisms to guarantee code compilability and successful test execution. In this work, we introduce MANTRA, a comprehensive LLM agent-based framework that automates method-level refac- toring. MANTRA integrates Context-Aware Retrieval-Augmented Generation, coordinated Multi-Agent Collaboration, and Verbal Reinforcement Learning to emulate human decision-making dur- ing refactoring while preserving code correctness and readability. Our empirical study, conducted on 703 instances of "pure refactor- ings" (i.e., code changes exclusively involving structural improve- ments), drawn from 10 representative Java projects, covers the six most prevalent refactoring operations. Experimental results demonstrate that MANTRA substantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8% success rate (582/703) in pro- ducing code that compiles and passes all tests, compared to just 8.7% (61/703) with RawGPT . Moreover, in comparison to IntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50% improvement in generating Extract Method transformations. A usability study involving 37 professional developers further shows that refactorings performed by MANTRA are perceived to be as read- able and reusable as human-written code, and in certain cases, even more favorable. These results highlight the practical advantages of MANTRA and emphasize the growing potential of LLM-based systems in advancing the automation of software refactoring tasks.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Conference'17, July 2017, Washington, DC, USA
(c) 2025 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn

Yisen Xu, Feng Lin, Jinqiu Yang, Tse-Hsun (Peter) Chen, and Nikolaos Tsantalis. 2025. MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG and Multi-Agent LLM Collaboration. In Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Refactoring is the process of improving the overall design and structure of the code without changing its overall behavior [16]. The goal of refactoring is to improve maintainability and facilitate future functionality extension [34], making it essential for adapting to ever-evolving software requirements. However, despite its benefits, many developers hesitate to refactor due to the time and effort involved [45]. Developers often need to first analyze the possibility of refactoring, then modify the code to refactor, and finally, ensure that refactoring does not introduce new issues.
   To assist developers with refactoring, researchers and Integrated Development Environment (IDE) developers (e.g., Eclipse and In- tellij IDEA) have proposed automated refactoring techniques. For example, Tsantalis and Chatzigeorgiou [57, 58] proposed JDeodor- ant to detect code smells, such as Feature Envy and Long Method, and apply refactoring operations. WitchDoctor [15] makes refactor- ing recommendations by monitoring whether code changes trigger predefined specifications. One common characteristic of these tools is that they are based on pre-defined rules or metrics. Although useful, they lack a deep understanding of the project's domain- specific structure and cannot produce refactorings similar to those written by developers, resulting in low acceptance in actual devel- opment [38, 53].
  Recent research on Large Language Models (LLMs) has shown their great potential and capability in handling complex program- ming tasks [30, 62, 66, 68], making them a possible solution for overcoming prior challenges, (i.e., generating high-quality refac- tored code similar to human-written ones). Several studies [5, 13, 34, 41, 51] have already explored the use of LLMs for refactoring, demonstrating their strong ability to analyze refactoring opportu- nities and applying code changes. However, existing techniques primarily rely on simple prompt-based refactoring generation, fo- cus on a limited set of refactoring types, and lack proper verification


through compilation checks and test execution. Moreover, these approaches have not fully utilized the self-reflection [50] and self- improvement capabilities of large language models, resulting in limited effectiveness and performance that has yet to match human- level proficiency in code refactoring.
  In this paper, we propose MANTRA (Multi-AgeNT Code RefAactoring), an end-to-end LLM agent-based solution for automated method-
level refactoring. Given a method to refactor and a specified refac- toring type, MANTRA generates fully compilable, readable, and test-passing refactored code. MANTRA includes three key compo- nents: (1) Context-Aware Retrieval-Augmented Refactored Code Generation, which constructs a searchable database to provide few- shot examples for improving refactoring quality; (2) Multi-Agent Refactored Code Generation, which employs a Developer Agent and Reviewer Agent to simulate real-world refactoring processes and produce high-quality refactoring; and (3) Self-Repair Using Verbal Reinforcement Learning, which iteratively identifies and corrects issues that cause compilation or test failures using a verbal reinforcement learning framework [50].
   We evaluated MANTRA using 10 Java projects used in prior refac- toring studies [21, 22, 28]. These projects cover diverse domains, have rich commit histories, and contain many tests. Since most refactoring changes are accompanied by unrelated code changes (e.g., bug fixes or feature additions) [53], we collected "pure refac- toring changes" (i.e., no code changes other than refactoring) to eliminate noise when evaluating MANTRA's refactoring ability. We applied PurityChecker [36] to filter commits, ultimately obtaining 703 pure refactorings for our experiments, which cover six of the most common refactoring activities [20, 40, 53]: Extract Method, Move Method, Inline Method, along with related compound refactor- ing activities: Extract and Move Method, Move and Inline Method, and Move and Rename Method. Using these refactorings, we compare MANTRA with our LLM baseline (RawGPT ), IntelliJ's LLM-based refactoring tool (EM-Assist [42]), and human-written refactored code. Furthermore, we conducted a user study to receive developer feedback on MANTRA-generated code and an ablation study to evaluate the contribution of each component within MANTRA.
  Our results demonstrate that MANTRA outperforms RawGPT in both functional correctness and human-likeness (how similar it is compared to developer refactoring). MANTRA achieves a sig- nificantly higher success rate of 82.8% (582/703) in generating compilable and test-passing refactored code, compared to only 8.7% for RawGPT . Against EM-Assist, MANTRA shows a 50% im- provement in generating Extract Method refactorings. Compared to human-written code, our user study (with 37 developers) found that MANTRA and human refactorings share similar readability and reusability scores. However, MANTRA performs better in Extract & Move and Move & Rename refactorings due to its clear comments and better code naming. In contrast, humans do better in Inline Method refactoring by making additional improvements. Finally, our ablation study highlights that removing any component from MANTRA results in a noticeable performance drop (40.7% - 61.9%), with the Reviewer Agent contributing the most to overall effective- ness.
We summarize the main contributions as follows:


• We proposed an end-to-end agent-based refactoring solu- tion MANTRA, which considers compilation success and functional correctness in the refactoring process. MANTRA leverages Context-Aware Retrieval-Augmented Generation to learn developer refactoring patterns, integrates multiple LLM agents to simulate the developer's refactoring process, and adopts a verbal reinforcement learning framework to improve the correctness of the refactored code.
• We conducted an extensive evaluation, and MANTRA suc- cessfully generated 582/703 compilable and test-passing refac- torings, significantly outperforming RawGPT , which only produced 61 successful refactorings. Compared with EM- Assist, a state-of-the-art LLM-based technique primarily fo- cused on Extract Method refactoring, MANTRA achieved a 50% improvement.
• We conducted a user study to compare MANTRA 's gener- ated and human-written refactored code. The analysis of 37 responses shows that the refactored code generated by the MANTRA is similar to the developer-written code in terms of readability and reusability. Moreover, MANTRA 's gener- ated code has better advantages in method naming and code commenting.
• We made the data and code publicly available [6].
Paper Organization. Section 2 discusses related work. Section 3 details the design and implementation of MANTRA. Section 4 presents the evaluation results of MANTRA. Section 5 discusses the limitations and potential threats to validity. Finally, Section 6 summarizes our findings and outlines directions for future work.

2 RELATED WORK
Traditional Refactoring Approaches. Refactoring plays a criti- cal role in software development and greatly influences software quality. Traditional research in refactoring generally focuses on two main aspects: identifying refactoring opportunities and recom- mending refactoring solutions. In terms of opportunity identification, existing approaches have explored various methods, such as calcu- lating distances between entities and classes for Move Method refac- toring [57], assessing structural and semantic cohesion for Extract Class opportunities [10], and utilizing logic meta-programming techniques to uncover refactoring possibilities [56]. On the other hand, solution recommendation often involves automated tech- niques. [37] introduced a tool CODe-Imp, which uses search-based techniques to perform refactoring. WitchDoctor [15] and BeneFac- tor [19] monitor code changes and automatically suggest refactor- ing operations. Nevertheless, these approaches are often limited by their rule-based nature, which may restrict them to certain types of refactoring or cause them to encounter unhandled issues during the refactoring process.
LLM-Based Techniques for Generating Refactored Code. Re- cent research on LLMs has demonstrated their remarkable ability to handle complex tasks, making them promising solutions to over- come the limitations of traditional refactoring approaches. Existing studies utilize LLMs for various refactoring tasks: directly prompt- ing GPT-4 for refactoring tasks [13, 41], providing accurate iden- tification of refactoring opportunities through carefully designed prompts [31], and recommending specific refactoring types such

as Extract Method [52]. Other studies further improve LLM-based refactoring by emphasizing prompt clarity [5], using carefully se- lected few-shot examples [51], and applying structured prompting techniques [65]. Additionally, hybrid approaches combining rule- based systems and LLMs have achieved superior outcomes com- pared to single-method techniques [69]. Automated frameworks and tools, such as the Context-Enhanced Framework for Automatic Test Refactoring [17] and tools like EM-Assist [43], further illustrate practical implementations of these techniques, highlighting the value of integrating LLMs into comprehensive, feedback-driven refactoring workflows. EM-Assist even outperforms all prior tools on Move Method refactoring [43].
  While these techniques leverage LLMs for automated refactor- ing, they typically focus on only one or two types of refactoring, neglecting compound or repository-level transformations (e.g., Ex- tract and Move Method) and failing to ensure that the refactored code compiles and passes all tests. In contrast, MANTRA uses LLM agents to emulate developers' refactoring process and integrate tra- ditional tools to provide feedback. MANTRA generates refactored code for a broader range of refactoring activities and ensures the generated code can compile and pass all tests.
LLM-Based Approaches for Code Quality Improvement. Pre- vious studies have also explored using LLM to improve other aspects of software quality, such as security, performance, and design. For instance, Lin et al. [30] proposed leveraging Software Process Mod- els to enhance the design quality in code generation tasks. Ye et al.
[68] introduced LLM4EFFI, conducting comprehensive research on improving code efficiency. Wadhwa et al. [62] proposed CORE, an approach utilizing instruction-following LLMs to assist developers in addressing code quality issues through targeted revisions. Wu et al. [66] presented iSMELL, which integrates LLMs for the detec- tion and subsequent refactoring of code smells, thus systematically enhancing software quality. Inspired by these studies, we designed MANTRA to also consider code readability by integrating with code style checkers (i.e., CheckStyle [11]) in the generation process.

3 METHODOLOGY


refactorings as few-shot examples to guide MANTRA. The Refac- tored Code Generation component uses a multi-agent framework that harnesses LLMs' planning and reasoning abilities to generate refactored code. Finally, the Self-Repair component implements a verbal reinforcement learning framework [50] to automatically identifies and corrects issues in the generated refactored code.
3.1 Context-Aware Retrieval Augmented Refactored Code Generation
Figure 1 shows an overview of MANTRA's RAG construction pro- cess. RAG provides LLM with relevant examples for few-shot learn- ing, thus improving its ability to generate accurate and contextually relevant output [23, 51]. RAG combines information retrieval and text generation by integrating external knowledge through a two- stage process - retrieval and generation [18]. Below, we discuss the details of MANTRA's RAG design.
Constructing a Database of Pure Refactoring Code Examples. We aim to build a database containing only pure refactoring code changes, which involve improving code structure without altering functionality. However, in reality, refactoring is often accompanied by unrelated code changes such as bug fixes or feature additions [53]. These changes contain noise that makes such refactoring code changes unusable as few-shot examples to guide LLM for generating general refactored code.
   We build the pure refactoring database using the Refactoring Ora- cle Dataset [61], which contains 12,526 refactorings (mostly impure refactorings) collected from 547 commits across 188 open-source Java projects (2011 to 2021). We selected this dataset because it includes various projects and refactoring types and was used as a benchmark to evaluate the accuracy of refactoring detection tools (e.g., RefDiff [54] and RefactoringMiner [60, 61]), making it highly suitable for our purpose. We incorporated PurityChecker [36], an extension of RefactoringMiner that specializes in assessing the pu- rity of method-level refactorings, excluding those associated with functional feature modifications. We choose to use Refactoring- Miner and PurityChecker because they are well-maintained and known for their high detection accuracy. RefactoringMiner has an

In this section, we introduce MANTRA (Multi-AgeNT Code RefAactoring),average precision and recall of 99% and 94%, respectively [61] and

an LLM-based, agent-driven solution for automated code refactor- ing. MANTRA focuses on method-level refactorings because of their wide adoption in practice [29, 35]. In particular, we implement a total of six refactoring activities, composing three of the most pop- ular refactoring activities [20, 40, 53]: Extract Method, Move Method, and Inline Method; and three of their compound refactoring ac- tivities: Extract And Move Method, Move And Inline Method, and Move And Rename Method. These refactoring activities consider both straightforward and intricate refactoring scenarios.
  MANTRA takes as input the code of a method to be refactored and the specified refactoring type. It then automatically finds refac- toring opportunities in the method and generates fully compil- able and highly readable refactored code that can pass all the tests. MANTRA consists of three key components: 1) RAG: Context-Aware Retrieval-Augmented Refactored Code Generation, 2) Refactored Code Generation: Multi-Agent Refactored Code Generation, and 3) Repair: Self-Repair Using Verbal Reinforcement Learning. The RAG component constructs a searchable database that contains prior

PurityChecker has an average precision and recall of 95% and 88%, respectively [36] on the Refactoring Oracle Dataset. At the end of this phase, we extracted 905 pure refactorings along with their associated metadata (e.g., Class Name, Method Signature, File Path, and Call Graph) from the GitHub repositories.
Incorporating Code Description and Caller-Callee Relation- ships for Context-Aware RAG Retrieval. Using only source code to construct a RAG database presents several challenges. First, code with similar structures does not necessarily share the same functionality or logic, both of which influence refactoring strategies. For instance, in Move Method refactoring, a test method should only be moved to a test class, not to production code. If the context does not clearly indicate the class type, relying solely on source code structure for retrieval may result in incorrect matches. Second, code dependencies play a crucial role in refactoring, as refactorings like Extract Method and Move Method are often driven by code depen- dencies [59]. Without capturing these dependencies, the retrieved examples may fail to align with the intended refactoring process.


   
Pure Refactoring
Figure 1: An overview of how MANTRA constructs a database containing only pure-refactoring for RAG.


  Therefore, in MANTRA, we incorporate 1) a natural language description of the refactored code and 2) all direct callers and callees of the refactored method as code-specific context to enhance RAG's retrieval capabilities. To generate the natural language description, we follow a recent study in the NLP community [7]. We use LLMs to generate a contextual description for every refactoring and concate- nate this description with the corresponding code to construct the contextual database. To guide the LLM in generating these descrip- tions, we use a simple prompt: "{Code}{Caller/Callee}{Class Info} Please give a short, succinct description to situate this code within the context to improve search retrieval of the code.", where {Code} refers to the code before refactoring, {Caller/Callee} denotes the direct callers/callees, and {Class Info} presents the structure information for the Class containing the code to be refactored, such as Package Name, Class Name, Class Signature. The prompt includes the direct callers and callees of method to be refactored to assist in description genera- tion, as dependent methods may influence refactoring decisions. Specifically, the prompt contains the method signatures and bodies of all direct callers and callees, enabling the retrieval mechanism to account for such dependencies.
Retrieving the Most Similar Code As Few-Short Examples. In the retrieval stage of RAG, two main retrieval methods are com- monly combined and fused for the best outcome [23], namely sparse retrieval and dense trieval. Sparse retrieval uses textual similarity to efficiently retrieve relevant documents, and dense retrieval re- lies on semantic similarity. MANTRA leverages sparse and dense retrieval separately for producing two similarity-ranked lists, then combines the lists to create a unified ranking list. For sparse re- trieval, MANTRA leverages BM25 [48], a robust ranking technique to obtain a ranked list based on textual similarity. For dense re- trieval, MANTRA employs all-MiniLM-L6-v2, a pre-trained model from Sentence Transformers [46] known for its speed and quality, for embedding generation. MANTRA then computes the cosine similarity between embeddings of the input refactoring request and stored refactoring examples to obtain an additional ranked list based on semantic similarity.Finally, MANTRA uses the Reciprocal Rank Fusion (RRF) algorithm [12, 49] to combine the sorted lists and re-rank the results.
3.2 Multi-Agent Refactored Code Generation MANTRA emulates how real-world code refactoring happens through a multi-agent collaboration among the Developer Agent, the Re-
viewer Agent, and the Repair Agent. As shown in Figure 2, MANTRA adapts a mixture-of-agents architecture [63] to organize the agent communication in two layers. In the first layer, the Developer Agent

is responsible for generating and improving the code, while the Reviewer Agent reviews the code and provides feedback or sugges- tions to the Developer Agent. If the refactored code fails to compile or pass tests, the generated code enters the second layer of the agent architecture, where the Repair Agent tries to repair any com- pilation or test failures based on LLM-based reasoning and verbal reinforcement learning [50].

3.2.1 Developer Agent. Given a method to refactor and a specified refactoring type, the Developer Agent first autonomously extracts the necessary information (e.g., repository structure, class informa- tion) based on the observation (i.e., the refactoring type and the provided inputs) by invoking our custom static analysis. It then retrieves similar refactorings using our contextual RAG approach (Section 3.1) as few-shot examples to enhance code generation. Finally, it generates the refactored code using the extracted infor- mation and the retrieved examples.
Dev-Agent-1: Using static code analysis to extract repository and source code structures. The Developer Agent has access to our custom static analysis tools to analyze the repository and extract code and project structural information. The code structural information includes the class hierarchies, inheritance relationship, method signatures and their implementation in a class, and inter- procedural method call graph. The project structural information includes the project directory structure and the specific Java file content. Among these, method signatures and their implementation are mandatory for all refactoring types, whereas other information is only necessary for specific types of refactoring.
  To reduce static analysis overhead and unneeded information to the LLM, the Developer Agent autonomously decides which analyses to perform based on the given refactoring type and the target code. For example, for the Move Method refactoring, the Developer Agent first calls get_project_structure to retrieve the overall project structure. Based on this information, it determines the relevant file directories to inspect. It then calls get_file_content to retrieve the source code files from the directory and assesses whether the method should be moved to the target class/file. The Developer Agent then uses the analysis results in the next phase to guide the refactoring process and generate the refactored code.
Dev-Agent-2: Automated refactored code generation via RAG and chain-of-thought. Given the static analysis result from the previous step, the Developer Agent 1) retrieves similar refactorings as few-shot examples using RAG and 2) generates the refactored code using chain-of-thought reasoning [64]. The Developer Agent provides the code to be refactored, the generated code description, and direct callers/callees as input to the RAG database to retrieve



Figure 2: An overview of MANTRA.


similar refactorings for few-shot learning. Then, the Developer Agent follows a structured chain-of-thought reasoning approach, analyzing the provided information sequentially. Below, we provide a simplified prompt example to show the code generation process.


  As shown in the example, the Developer Agent analyzes the method source code and the entire class in which it resides. Then, the agent autonomously decides whether to further analyze broader contextual information, including direct callers/callees, the inheri- tance graph, and the repository structure. After collecting source code and other structural information, the agent then generates a list of potential refactoring opportunities for the given refactoring

type, such as which parts of a method can be extracted (e.g., for Ex- tract Method) or possible classes to move a method to (e.g., for Move Method). Finally, the agent generates the refactored code based on the retrieved few-shot examples and the list of potential refactoring opportunities. Finally, the agent selects the most probable refac- toring opportunity from the list and generates the refactored code based on the retrieved few-shot examples.
3.2.2 Reviewer Agent. The Reviewer Agent is responsible for evalu- ating the refactored code generated by the Developer Agent, ensur- ing its correctness in two key aspects: refactoring verification and code style consistency to ensure readability. The Reviewer Agent first verifies whether the code has undergone the specified type of refactoring by leveraging RefactoringMiner to detect refactoring ac- tivities in the code. If the code fails this check, the Reviewer Agent immediately generates a feedback report containing the refactor- ing verification results and returns it to the Developer Agent for correction. If the refactoring verification is successful, the Reviewer Agent proceeds with code style consistency analysis, which is con- ducted using the static analysis tool CheckStyle [11]. If any code style issues are detected, such as non-standard variable naming or formatting inconsistencies, the Reviewer Agent generates a feed- back report highlighting these problems and sends it back to the Developer Agent for correction. Once the refactored code passes the refactoring verification and code style consistency check, the Reviewer Agent will compile and test it. If there is no failure and all tests pass, the generation process is done, and MANTRA returns the final refactored code. If there is any failure, the generated code and error log will be forwarded to the next phase.


3.2.3 Communication between the Developer and Reviewer Agents. In MANTRA, the Developer Agent and the Reviewer Agent work together in an iterative process, simulating human team collab- oration. After generating refactored code, the Developer Agent submits it for review. The Reviewer Agent then verifies both the refactoring and code style, providing structured feedback. If any issues are found, the Developer Agent refines and resubmits the code, repeating the cycle until all required standards are met.
  After verifying the refactoring activity and code style, the Re- viewer Agent triggers the compilation and testing phase. The agent integrates the generated refactored code into the project and creates compilation commands based on the project's build system (Maven or Gradle). It then executes the commands to compile the project and run the tests. If there are any compilation issues or test failures, the issues are escalated to the Repair Agent for fixing.

3.3 Self-Repair Using Verbal Reinforcement Learning
The Repair Agent iteratively fixes refactored code that fails to com- pile and pass tests by leveraging verbal reinforcement learning. To enhance its self-repairing capability, we integrate and adapt the Reflexion framework [50], which systematically guides the repair process through four distinct phases: initial analysis, self-reflection, planning, and acting. Reflexion is designed to enable systems to self-improve by incorporating iterative feedback.
  The Repair Agent starts an (1) initial analysis, where the Repair Agent examines the refactored code, the entire class where the code resides, and associated error logs. Then, it generates an initial patch based on the problematic code segments and corresponding error descriptions. Subsequently, the agent applies the patch, re- compiles the code, and re-runs the tests. If there are any issues, the agent enters the (2) self-reflection phase, critically self-reviewing the compilation and testing result. The agent generates error reasoning by comparing the code and the associated error messages before and after applying the patch. For instance, if the previous patch fails to resolve a null pointer exception, the agent reflects on the absence or inadequacy of necessary null checks by explicitly referencing the corresponding lines in the stack trace.
Based on the self-reflection result, the Repair Agent enters the
(3) planning phase, where it generates a refined repair strategy, specifying concrete code modifications required to resolve identi- fied issues (e.g., adding necessary null checks, correcting variable declarations, or revising incorrect method calls). Finally, in the (4) acting phase, the Repair Agent applies the planned patch, followed by compilation and test execution. This is an iterative process that continues the code successfully compiles and passes all tests or reaches a predefined maximum number of repair attempts (i.e., 20). To ensure the code semantic remains unchanged, during the repair process, we specify in the prompt that the agent should not modify the code's functionality and only focus on repair.

4 EVALUATION
In this section, we first present the studied dataset and evalua- tion metrics. Then, we present the answers to the four research questions.

Table 1: The studied Java projects for evaluating MANTRA.

Project
# Star
# Commits
# Pure Refactoring
checkstyle
8,462
14,606
91
pmd
4,988
29,117
125
commons-lang
2,776
8,404
59
hibernate-search
512
15,716
89
junit4
8,529
2,513
18
commons-io
1,020
5,455
93
javaparser
5,682
9,607
56
junit5
6,523
8,990
105
hbernate-orm
6,091
20,638
63
mockito
15,032
6,236
4
Total
59,615
121,282
703

Studied Dataset. Table 1 shows the Java projects that we use to evaluate MANTRA. We select these 10 Java projects used in prior change history tracking studies [21, 22, 28] based on three key considerations. First, the projects cover diverse application domains, providing a broad representation of software development practices. Second, each project has a substantial commit history, with over 2,000 commits, indicating a richer development history and a higher likelihood of identifying commits that involve refactoring activities. Third, we chose projects where we could manually resolve the compilation issues and successfully execute the tests to verify the quality of the generated refactoring.
  We evaluate and compare the refactored code generated by MANTRA with that produced by human developers. To reduce noise from unrelated changes, such as bug fixes, we analyze only "pure refactoring changes" (i.e., no code changes other than refac- toring) from these projects. Similar to Section 3.1, we apply Purity- Checker [36] to select only the commits containing pure refactoring. Since we want to evaluate the functional correctness of the gen- erated refactoring by running the tests, we compile every pure refactoring commit and its parent commit (to ensure there were no compilation or test failures before refactoring), selecting only the commits that could be successfully compiled and pass the tests.
  We analyze test coverage to verify whether the tests cover the refactored code (i.e., it is testing the refactored code's functional be- havior). We execute the test cases using Jacoco [26] to collect code coverage information and filter out the commits where the refac- toring changes have no coverage. Finally, we verify the existence of target classes for the Move Method, Extract and Move Method, and Move and Inline Method refactorings. This step was necessary because the Move Method operation may move a method to newly created classes, and it is difficult for MANTRA to predict the newly created classes. After applying all the above data selection steps, we identified 703 pure refactorings across the 10 Java projects.
Evaluation Metrics. We evaluate the refactored code along two dimensions: functional correctness and human-likeness. For func- tional correctness, we assess the code using 1) compilation success,
2) test pass, and 3) RefactoringMiner verification (i.e., Refactoring- Miner detects that the refactoring activity indeed happened). Specif- ically, we integrate the generated refactored code into the project. We then compile it and execute the tests to verify if the builds are successful. Because of LLMs' hallucination issues [24], the gener- ated code may pass the test cases but do not accurately perform the intended refactoring. Hence, we verify whether the generated code contains the target refactoring using RefactoringMiner [60].


  Even though we give a target method as input to MANTRA, it still needs to find the specific part of the code that can be refactored. Hence, we evaluate the human-likeness of MANTRA's generated code to compare its refactoring decisions with that of developers. We employ the CodeBLEU metric [47] and Abstract Syntax Tree (AST) Diff Precision and Recall [3] to measure the difference. Code- BLEU evaluates the grammatical and semantic consistency between human-written refactored code and MANTRA-generated code. AST Diff represents a set of mappings that capture code changes be- tween the original and refactored code. Each mapping consists of a pair of matched AST nodes from the diff between the origi- nal and refactored versions. These mappings are obtained using RefactoringMiner. To evaluate structural similarity, we compare the mappings produced by MANTRA's refactored code with those of the developer-written code. The number of MANTRA's mappings that match the developer-written mappings is treated as true posi- tives (TP). Precision is calculated as the ratio of TP to all mappings produced by MANTRA, indicating how accurate MANTRA's map- pings are. Recall is the ratio of TP to all developer-written mappings, reflecting how much of the developer's refactoring was successfully captured. The values for CodeBLEU and AST Precision/Recall range from 0 to 1, where 1 means a perfect match.
Environment. We selected OpenAI's ChatGPT model [2] for our experiment due to its popularity and ease of integration through the OpenAI-API. Specifically, we utilized the gpt-4o-mini-2024-07-18 version, as it offers a balance of affordability and strong perfor- mance. We implemented MANTRA using version 0.2.22 of Lang- Graph [25] and various static analysis tools that we implemented using a combination of APIs from RefactoringMiner, a modified version of RefactoringMiner, and Eclipse JDT [14]. On average, one complete refactored code generation (from querying the database, code generation, and test execution to fixing compilation and test failures) takes less than a minute on a Linux machine (Intel(r) Core(tm) i9-9900K CPU @ 3.60GHz, 64GB Memory), costing less than $0.10.


RQ1: How effective is MANTRA in refactoring code?
Motivation. In this RQ, we evaluate MANTRA's generated refac- tored code along two dimensions: functional correctness and human- likeness. We also analyze MANTRA's performance across differ- ent refactoring types. This RQ offers insights into how effective MANTRA is at performing refactoring tasks and the specific types of refactoring tasks where MANTRA is most effective.
Approach. We evaluate MANTRA using the 703 pure refactoring commits collected from the 10 studied Java projects. First, we use git checkout on the commit before each pure refactoring commit, allowing us to extract the original code before refactoring opera- tions. The code repository is then fed into MANTRA to generate the refactored code. For comparison, we include RawGPT as the baselines (it uses the same LLM as MANTRA). RawGPT directly sends a simple prompt to the LLM to perform code refactoring. We input RawGPT with basic code information, i.e., the same infor- mation generated by the static-analysis component of MANTRA's Developer Agent (e.g., class content and project structure). RawGPT does not have the multi-agent component and is not prompted with


few-shot examples retrieved from RAG. The complete prompt can be found online [6].
Result. MANTRA successfully generated 582/703 (82.8%) of the refactored code that is compilable, passed all the tests, and ver- ified by RefactoringMiner, while RawGPT could only generate 61/703 (8.7%) successfully. As shown in Table 2, MANTRA can generate significantly more refactored code than RawGPT . Of the 703 refactorings, 636 generated by MANTRA successfully compiled and passed the test cases, and 604 refactorings were further verified by RefactoringMiner as true refactoring operations. In contrast, only 100 refactorings generated by RawGPT can compile and pass the test cases, yet only 61 were verified by RefactoringMiner. Note that there can be some generated refactored code that is verified by RefactoringMiner but does not pass compilation/tests, or vice versa, so the total successful refactoring is 582.
  RawGPT has difficulties in generating Move Method, Extract and Move Method, and Move and Rename Method, where it cannot gener- ate any refactoring. When doing these refactorings, RawGPT always ignores the project structure information in the prompt and cannot move the method to the correct class. In contrast, MANTRA has the Reviewer Agent that gives feedback on the refactoring verification to guide the Developer Agent to perform the Move operation. Nev- ertheless, even for refactorings that do not require repository or class structures (i.e., Extract Method and Inline Method), MANTRA achieves a much higher success rate (317 v.s. 47 and 22 v.s. 8, re- spectively).
MANTRA outperforms RawGPT in code similarity, producing refactored code more similar to humans. Across all success- ful refactorings, MANTRA achieved a CodeBLEU score of 0.640, compared to RawGPT 's 0.517, showcasing MANTRA's ability to generate code that closely aligns with human-written refactorings. Regarding structural accuracy, MANTRA achieved an AST Diff pre- cision of 0.781, surpassing RawGPT 's 0.773, while its AST Diff recall reached 0.635, notably higher than RawGPT 's 0.574. These results indicate that MANTRA's generated code is more similar and aligns better with the structural transformation of developer- written refactoring.
MANTRA's results are closer to developers' decisions, where 18% (105/582) of MANTRA's generated refactored code is iden- tical to developer's refactoring, compared to RawGPT's 13.1% (8/61). We further analyze the distribution of refactored code iden- tical to the developer's implementation across different refactoring types. MANTRA correctly generated 84 Move Method refactorings, whereas RawGPT failed to produce any valid refactorings for this category. Additionally, MANTRA applied 11 Extract Method and eight Inline Method that were the same as developers' refactor- ing, while RawGPT only managed four for both types. MANTRA was also able to generate two composite refactorings (Extract and Move Method) that were identical to those of developers. In short, MANTRA's results match closer to developers' refactoring decisions, likely due to its retrieval-augmented generation (RAG) component, which provides similar past refactorings as few-shot examples.

Table 2: Refactoring results of RawGPT and MANTRA. The table presents the number of refactorings to perform, compile-and- test success rates, refactoring verification (RM Verification), and code similarity metrics with human-written refactorings (Code BLEU and AST Precision/Recall). Successful Refactoring refers to the number of refactorings that compile, pass tests, and are verified by RefactoringMiner. We compute the average for Code BLEU and AST Precision/Recall, and total for all other fields.

Approach
Project
     # Pure Refactoring
Compile&Test	RM Code	AST AST Success Verification BLEU Precision Recall
 Successful Refactoring
Extract Inline Move Extract And	Move And	Move And Method Method Method Move Method Rename Method Inline Method

checkstyle
91
14
9
0.667
0.603
0.233
6
4
2
0
0
0
0

pmd
125
35
30
0.502
0.791
0.338
19
17
2
0
0
0
0

commons-lang
59
2
13
0.640
0.67
0.363
2
2
0
0
0
0
0

hibernate-search
89
14
27
0.368
0.792
0.632
11
5
1
0
0
0
5

junit4
18
10
10
0.486
0.856
0.859
9
8
1
0
0
0
0
RawGPT
commons-io
93
8
22
0.773
0.804
0.95
6
5
1
0
0
0
0

javaparser
56
11
11
0.441
0.777
0.96
4
2
1
0
0
0
1

junit5
105
2
5
0
0
0
0
0
0
0
0
0
0

hibernate-orm
63
17
2
0.263
0.756
0.386
2
2
0
0
0
0
0

mockito
4
2
2
0.678
0.659
0.746
2
2
0
0
0
0
0
Total/Average
703
100
146
0.517
0.773
0.574
61 (8.7%)
47
8
0
0
0
6

checkstyle
91
90
86
0.624
0.514
0.501
85
31
4
9
39
1
0

pmd
125
119
108
0.676
0.725
0.766
106
49
2
28
24
3
0

commons-lang
59
56
46
0.567
0.815
0.257
46
42
1
0
3
0
0

hibernate-search
89
81
82
0.538
0.929
0.734
74
35
10
6
13
9
1

junit4
18
13
15
0.61
0.879
0.676
12
9
1
2
0
0
0
MANTRA
commons-io
93
87
81
0.623
0.873
0.662
80
62
2
9
7
0
0

javaparser
56
51
51
0.645
0.859
0.681
46
31
1
7
6
0
1

junit5
105
79
76
0.852
0.814
0.787
74
9
0
48
15
2
0

hbernate-orm
63
56
55
0.524
0.805
0.474
55
46
1
0
7
0
1

mockito
4
4
4
0.754
0.861
0.736
4
3
0
0
1
0
0
Total/Average
703
636
604
0.64
0.781
0.635
582 (82.8%)
317
22
109
115
15
3


Table 3: The number of generated refactored code (compilable and pass all test cases) that is identical to that of developer's.

Refactoring Type
RawGPT
MANTRA
Extract Method
4
11
Inline Method
4
8
Move Method
0
84
Extract And Move Method
0
2
Move And Rename Method
0
0
Move And Inline Method
0
0
Total
8
105

Table 4: The number of successful Extract Method refactor- ings by EM-Assist and MANTRA-3.5-turbo.

Project
# Extract Method
EM-Assist
MANTRA-3.5-turbo
checkstyle
34
14
34
pmd
55
24
48
commons-lang
54
18
41
hibernate-search
28
25
27
junit4
11
3
9
commons-io
68
33
49
javaparser
35
22
29
junit5
9
6
6
hibernate-orm
52
39
40
mockito
3
1
1
Total
359
185
277

RQ2: How does MANTRA compare to IntelliJ's LLM-based refactoring tool?
Motivation.We compare the code generated by MANTRA to the code produced by IntelliJ's LLM-based refactoring tool. IntelliJ IDEA
[27] provides a plugin, called EM-Assist [43, 44], which uses LLM for one specific type of refactoring, i.e., Extract Method. EM-Assist utilizes in-context learning, providing all necessary instructions within the prompt, including the task definition and relevant con- textual information. The input for EM-Assist is the method to be refactored, and it outputs a list of suggestions that include the start and end lines to be extracted, along with the new method's name. To ensure the suggestions are valid, EM-Assist uses IntelliJ's static analysis abilities to filter out suggestions that would cause compila- tion errors. Once valid suggestions are identified, EM-Assist applies the refactorings via the IntelliJ IDEA API based on the AST. Given its superior performance compared to tools like JDeodorant [32] and GEMS [67], we selected EM-Assist as our comparison baseline. Approach. We use the latest version of EM-Assist 0.7.5 [42] to per- form Extract Method refactoring. This version of EM-Assist utilizes gpt-3.5-turbo-0125 for refactoring. Since EM-Assist 0.7.5 is fully integrated into IntelliJ IDEA and lacks an interface to change the LLM version, we also used the same version of GPT model (i.e., gpt- 3.5-turbo-0125) to run MANTRA on all Extract Method refactorings. Result. MANTRA (3.5-turbo) is able to refactor 77.1% (277/359) of the Extract Method refactoring, while EM-Assist can only refactor 51.5% (185/359), providing almost 50% improvement. Table 4 shows the result of Extract Method Refactoring of EM- Assist and MANTRA. EM-Assist refactors 185 out of 359 methods successfully. In comparison, MANTRA outperforms EM-Assist by successfully refactoring 277 methods. Among the refactorings per- formed, 142 methods were successfully refactored by both tools.


The finding shows that MANTRA is also complementary to EM- Assist, as it successfully handled a significantly different subset of refactoring cases and demonstrates the potential to combine both techniques. We also see a decrease when changing MANTRA's un- derlying LLM from GPT-4o-mini to 3.5-turbo, where the number of successfully refactored Extract Method decreased from 317 (Table 2) to 277 (12.6% decrease). This performance drop shows the impact of the underlying LLM, but the overall result is still promising.


RQ3: How does MUARF-generated refactored code compare to human-written code?
Motivation. In prior RQs, we show that MANTRA can successfully generate many refactorings. However, it is also important that the refactored code is understandable and aligned with human coding practices. Hence, in this RQ, we conduct a user study to compare MANTRA's generated and human-written refactored code.
Approach. We randomly select 12 refactorings from the studied projects used, composing two refactorings for each of the six refac- toring types. For each sampled refactoring, we prepare the 1) code before refactoring, 2) refactored code generated by MANTRA, and
3) developer's refactored code. To increase the sample richness and reduce the developer's evaluation time, we divide these 12 samples into two separate survey questionnaires [8, 9], each questionnaire covers samples from the six refactoring types. For each sample, the participants compare two code snippets (MANTRA-generated and developer-written). We follow prior studies [1, 4, 33, 55] and ask the participants to assess the code's readability and reusability (on a scale from one to five, where five means highly readable or reusable) and selecting the one they find more intuitive and well- structured. For readability, we ask the participants how readable is the refactored code. For reusability, we ask the participants how easy it is to reuse or extend the refactored code. To avoid biases, we do not specify which one is written by humans or generated by LLM until the participants complete the survey. We then ask for their opinion in free-form text after revealing this information. Result. We shared the questionnaires through social media, and in total, we collected 37 responses for the two questionnaires (20 and 17, respectively). We combine the results from the two ques- tionnaires and present the results below. Overall, the participants have programming experience ranging from 1 year (5.4%) to over 5 years (54.1%), and over 45% of the participants use Java as their primary programming language.
On average, the participants find that MANTRA-generated code has similar readability and reusability compared to human- written code. As shown in Figure 3, LLM-generated code achieves average readability and reusability scores of 4.15 and 4.13, respec- tively, compared to human-written code, which scored 4.02 and
3.97. We further applied a student's t-test, and we did not find a statistically significant difference. This finding indicates, when considering all refactoring types, the participants find MANTRA's


generated code has similar readability and reusability compared to human-written code.
  However, for specific types of refactoring (i.e., Extract & Move and Move & Rename), MANTRA-generated code shows roughly a 20% improvement in readability and reusability, and it is preferred 185% more than human-written code, with statisti- cally significant difference (p-value < 0.001). In contrast, human developers achieve higher scores for Inline Method, an average of 8.5% higher in readability and 14.5% in reusability, and is 439% more preferred than MANTRA-generated refactored code (statistically sig- nificant with p-value < 0.05). Although human developers achieve slightly higher average readability and reusability scores for Move and Inline Method, the difference is not statistically significant.
   Figure 4 shows an example where the participants prefer the MANTRA-generated code (readability and reusability score of 4.50 and 4.35, compared to 3.65 and 3.75 for human-refactored code, respectively). Both MANTRA and a human developer performed Extract and Move refactoring by extracting the same code snip- pet into the same superclass. However, they differ in the method names, comments, and parameters type. One participant says that "the [LLM-generated code] is clearly easier to understand. From the comments and names, I can guess the functionality of the code... [Human-written code] is likely a very generic skeleton code.".
  Overall, we find a trend based on the participants' scores and responses: MANTRA-generated code typically includes detailed comments and refactoring that more closely aligns with its intended purpose. For instance, MANTRA tends to use more de- scriptive method names that improve clarity. For instance, one participant mentioned that "[LLM-generated code] is more struc- tured and clear, and it seems more detailed and easier to understand.". In comparison, developer-refactored code sometimes improves code readability when doing specific refactoring types, especially during Inline Method. For example, in one of the Inline Method code snip- pets in the questionnaire, the developer improved the code during refactoring, while MANTRA's generated refactored code simply moved the code directly. A participant mentioned: "[LLM-generated code] added two lines of code instead of one, making it harder to maintain in the future." The study shows that participants gener- ally prefer MANTRA-generated code for its clarity and detailed comments. However, in some cases, such as Inline Method, human developers tend to write more readable and maintainable code by making additional improvements beyond direct refactoring.


RQ4: What is the contribution of each component in MANTRA?
Motivation. In this RQ, we conduct an ablation study to examine the contribution of each component to the overall effectiveness of MANTRA. The results will highlight the importance of each




Readability & Reusability
5



100


Preference Distribution Across Methods


4	80

60
3
40
2
20

1
0


Figure 3: Left panel: Boxplots depicting the readability and reusability scores from the questionnaire, comparing MANTRA- generated code with human-written code. White markers indicate the mean score for each refactoring category. Right panel: A visualization of participants' preferences regarding which code they favor.



1: public class OrFileFilter extends AbstractFileFilter {
2:  /* ... */
3:  @Override
4:  public String toString() {
5:	final StringBuilder buffer = new StringBuilder();
6:	buffer.append(super.toString());
7:	buffer.append("(");
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:  }
19:}



public abstract class AbstractFileFilter {
/** Appends the string representation of the file... **/
protected void appendFileFilters(StringBuilder buffer,
                List<AbstractFileFilter> fileFilters) { if (fileFilters != null) {
for (int i = 0; i < fileFilters.size(); i++) {
/* ... */
}
}
}
}	MANTRA-Refactored Code



highlighting their contribution to MANTRA's ability. Figure 5 shows the contribution of each component. Removing the RAG com- ponent alone reduces the number of successfully compiled/tested refactored code and the number of successful refactorings to 405 and 345 (36.3% and 40.7% decrease), respectively. The findings show that a high-quality database for RAG has a non-negligible impact on the generated refactored code. Similarly, removing the Repair Agent significantly reduces the number of successfully compiled/tested refactored code from 636 to 376 (40.9% decrease), as the Repair

Figure 4: An example illustrating how MANTRA and human
developers implemented the Extract & Move refactoring.

Compile&Test Success	Successful refactorings
700 	

600

500

400

300

200

100

0
MANTRA	w/o RAG	w/o Repair	w/o Reviewer
Model

Figure 5: Contribution of each component in MANTRA. Com- pile&Test Success shows the number of generated code that compiles and passes all tests. Successful refacotorings means the number of verified code that contains the specific refac- toring.

component, inspiring future research on adapting them for related tasks.
Approach. Our ablation study examines three key components: RAG, the Reviewer Agent, and the Repair Agent. We define three configurable models to evaluate the impact of key components in MANTRA: MANTRA w/o RAG , MANTRA w/o Reviewer , and MANTRA w/o Repair . Each of the configure removes the corresponding key component, which allows us to assess the contribution of each component to MANTRA 's overall performance.
Result. Removing a component reduces the number of success- ful refactoring from 582 to 222-345 (40.7%-61.9% decrease),

Agent is responsible for fixing compilation issues and test failures. Removing the Repair Agent also reduces the number of successful refactoring from 582 to 287 (50.7% decrease), as the final-stage repair process plays a crucial role in finalizing the refactoring changes. Among the three components, removing the Reviewer Agent has the most impact on the number of generated refactored that pass compilation/test (decrease from 636 to 359) and the number of suc- cessful refactoring (decrease from 582 to 222). The Reviewer Agent leverages traditional tools to provide feedback in the refactoring process. Our result highlights that without feedback from external tools such as RefactoringMiner, MANTRA encounters challenges in generating valid refactored code. Our finding also shows a promis- ing direction in combining traditional software engineering tools to guide LLMs in producing better results.

5 THREATS TO VALIDITY
Internal validity. Due to the generative nature of LLMs, their responses may vary across different runs and model versions. In our experiments, we set the temperature value to 0 to reduce variability in the result. We used LLMs from OpenAI (i.e., 4o-mini and 3.5- turbo) for our experiment. Future studies are needed to study the impact of LLMs on generating the refactored code.
External validity. We focused on method-level refactorings due to their popularity [29, 35]. Although we included both straightfor- ward and compound refactorings, the results may not generalize to


other types of refactoring, such as class-level. Such refactorings are less common and often involve other code changes (e.g., bug fixes) [39], making data collection difficult. Further research is needed to assess MANTRA's effectiveness in broader refactoring scenarios. We focused on Java since it has extensive literature on refactoring- related research. Future work should evaluate MANTRA across multiple languages.
Construct validity. We use CodeBLEU and AST Precision/Recall to evaluate the similarity between MANTRA-generated and developer- written refactoring. However, although informative, these metrics may still miss some differences in the code. Therefore, we conducted a user study to compare MANTRA-generated and developer-written code. While we gathered feedback from 37 developers with different experience levels, some findings can be subjective. To avoid biases, we do not tell the participants which code is refactored by MANTRA or human developers until they finish the questionnaire.
6 CONCLUSION
In this paper, we introduced MANTRA, an end-to-end LLM agent- based solution for automated method-level code refactoring. By leveraging Context-Aware Retrieval-Augmented Generation, Multi- Agent Collaboration, and Verbal Reinforcement Learning, MANTRA generates human-like refactored code while ensuring correctness and readability. Our evaluation on 703 real-world refactorings across 10 diverse Java projects demonstrates that MANTRA signifi- cantly outperforms LLM-based refactoring baseline by achieving an 82.8% success rate in generating compilable and test-passing code-far surpassing. It also has a 50% improvement over IntelliJ's LLM-based tool (EM-Assist). Furthermore, our user study with 37 developers reveals that MANTRA-refactored code is as readable and reusable as human-written code, with better code for some specific refactoring types. In short, our findings highlight the potential of LLM-based refactoring tools in automating software maintenance.
REFERENCES
[1] Chaima Abid, Vahid Alizadeh, Marouane Kessentini, Thiago do Nascimento Ferreira, and Danny Dig. 30 years of software refactoring research: A systematic literature review. arXiv preprint arXiv:2007.02194, 2020.
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren- cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[3] Pouria Alikhanifard and Nikolaos Tsantalis. A novel refactoring and semantic aware abstract syntax tree differencing tool and a benchmark for evaluating the accuracy of diff tools. ACM Transactions on Software Engineering and Methodology, sep 2024. ISSN 1049-331X. doi: 10.1145/3696002. URL https://doi.org/10.1145/ 3696002. Just Accepted.
[4] Eman Abdullah AlOmar, Philip T Rodriguez, Jordan Bowman, Tianjia Wang, Ben- jamin Adepoju, Kevin Lopez, Christian Newman, Ali Ouni, and Mohamed Wiem Mkaouer. How do developers refactor code to improve code reusability? In Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2-4, 2020, Proceedings 19, pages 261-276. Springer, 2020.
[5] Eman Abdullah AlOmar, Anushkrishna Venkatakrishnan, Mohamed Wiem Mkaouer, Christian Newman, and Ali Ouni. How to refactor this code? an exploratory study on developer-chatgpt refactoring conversations. In Proceed- ings of the 21st International Conference on Mining Software Repositories, pages 202-206, 2024.
[6] Anonymous. Data and code of muarf, 2025. URL to-be-updated. Accessed: March 13, 2025.
[7] Anthropic. Introducing contextual retrieval, 2024. URL https://www.anthropic. com/news/contextual-retrieval. Accessed: Sep. 19, 2024.
[8] Paper Authors. Refactoring survey 1, 2024. URL https://prettyform.addxt.com/a/ form/vf/1FAIpQLSdJt97mC3NVAPPvvgdtlvFNqW9Xi3p6SwmexXNdhZ1WH-
_M0w. Accessed: Jun. 23, 2024.
[9] Paper Authors.	Refactoring survey 2, 2024.	URL https:// prettyform.addxt.com/a/form/vf/1FAIpQLSeVf1qD2oUIo6RQMWGKf_


5wH2CzJpRAu0TkNGhib5IQxvtHLQ. Accessed: Jun. 23, 2024.
[10] Gabriele Bavota, Andrea De Lucia, and Rocco Oliveto. Identifying extract class refactoring opportunities using structural and semantic cohesion measures. J. Syst. Softw., 84(3):397-414, 2011. doi: 10.1016/J.JSS.2010.11.918. URL https://doi. org/10.1016/j.jss.2010.11.918.
[11] Checkstyle Team. Checkstyle, 2024. URL https://checkstyle.org/index.html. Accessed: 2024-11-20.
[12] Gordon V. Cormack, Charles L. A. Clarke, and Stefan Büttcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In James Allan, Javed A. Aslam, Mark Sanderson, ChengXiang Zhai, and Justin Zobel, editors, Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009, pages 758-759. ACM, 2009. doi: 10.1145/1571941.1572114. URL
https://doi.org/10.1145/1571941.1572114.
[13] Kayla DePalma, Izabel Miminoshvili, Chiara Henselder, Kate Moss, and Eman Ab- dullah AlOmar. Exploring chatgpt's code refactoring capabilities: An empirical study. Expert Systems with Applications, 249:123602, 2024.
[14] Eclipse Foundation. Eclipse jdt (java development tools), 2024. URL https:
//github.com/eclipse-jdt/. Accessed: March 13, 2025.
[15] Stephen R Foster, William G Griswold, and Sorin Lerner. Witchdoctor: Ide support for real-time auto-completion of refactorings. In 2012 34th international conference on software engineering (ICSE), pages 222-232. IEEE, 2012.
[16] Martin Fowler. Refactoring - Improving the Design of Existing Code. Addison Wesley object technology series. Addison-Wesley, 1999. ISBN 978-0-201-48567-7. URL http://martinfowler.com/books/refactoring.html.
[17] Yi Gao, Xing Hu, Xiaohu Yang, and Xin Xia. Context-enhanced llm-based frame- work for automatic test refactoring. arXiv preprint arXiv:2409.16739, 2024.
[18] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. CoRR, abs/2312.10997, 2023. doi: 10.48550/ARXIV.2312.10997. URL https://doi.org/10.48550/arXiv.2312.10997.
[19] Xi Ge, Quinton L DuBose, and Emerson Murphy-Hill. Reconciling manual and automatic refactoring. In 2012 34th International Conference on Software Engineering (ICSE), pages 211-221. IEEE, 2012.
[20] Yaroslav Golubev, Zarina Kurbatova, Eman Abdullah AlOmar, Timofey Bryksin, and Mohamed Wiem Mkaouer. One thousand and one stories: A large-scale survey of software refactoring, 2021. URL https://arxiv.org/abs/2107.07357.
[21] Felix Grund, Shaiful Alam Chowdhury, Nick C. Bradley, Braxton Hall, and Reid Holmes. Codeshovel: Constructing method-level source code histories. In 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021, pages 1510-1522. IEEE, 2021. doi: 10.1109/ICSE43902.2021.
00135. URL https://doi.org/10.1109/ICSE43902.2021.00135.
[22] Mohammed Tayeeb Hasan, Nikolaos Tsantalis, and Pouria Alikhanifard. Refactoring-aware block tracking in commit history. IEEE Transactions on Soft- ware Engineering, 50(12):3330-3350, 2024. doi: 10.1109/TSE.2024.3484586.
[23] Pengfei He, Shaowei Wang, Shaiful Chowdhury, and Tse-Hsun Chen. Explor- ing demonstration retrievers in RAG for coding tasks: Yeas and nays! CoRR, abs/2410.09662, 2024. doi: 10.48550/ARXIV.2410.09662. URL https://doi.org/10. 48550/arXiv.2410.09662.
[24] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. CoRR, abs/2311.05232, 2023. doi: 10.48550/ARXIV. 2311.05232. URL https://doi.org/10.48550/arXiv.2311.05232.
[25] LangChain Inc. Langgraph, 2024. URL https://langchain-ai.github.io/langgraph/. Accessed: 2024-12-02.
[26] Jacoco. Jacoco, 2009. URL https://www.jacoco.org/jacoco/trunk/index.html.
Accessed: Jun. 1, 2009.
[27] JetBrains. Intellij idea, 2024. URL https://www.jetbrains.com/idea/. Accessed: Jun. 10, 2024.
[28] Mehran Jodavi and Nikolaos Tsantalis. Accurate method and variable tracking in commit history. In Proceedings of the 30th ACM Joint European Software Engi- neering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, page 183-195, New York, NY, USA, 2022. Association for Com- puting Machinery. ISBN 9781450394130. doi: 10.1145/3540250.3549079. URL
https://doi.org/10.1145/3540250.3549079.
[29] Miryung Kim, Thomas Zimmermann, and Nachiappan Nagappan. An empirical study of refactoringchallenges and benefits at microsoft. IEEE Transactions on Software Engineering, 40(7):633-649, 2014.
[30] Feng Lin, Dong Jae Kim, Tse-Husn, and Chen. Soen-101: Code generation by emulating software process models using large language model agents, 2024. URL https://arxiv.org/abs/2403.15852.
[31] Bo Liu, Yanjie Jiang, Yuxia Zhang, Nan Niu, Guangjie Li, and Hui Liu. An empirical study on the potential of llms in automated software refactoring. arXiv preprint arXiv:2411.04444, 2024.
[32] Davood Mazinanian, Nikolaos Tsantalis, Raphael Stein, and Zackary Valenta. Jdeodorant: clone refactoring. In Laura K. Dillon, Willem Visser, and Laurie A.



Williams, editors, Proceedings of the 38th International Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016 - Companion Volume, pages 613-616. ACM, 2016. doi: 10.1145/2889160.2889168. URL https://doi.org/ 10.1145/2889160.2889168.
[33] Raimund Moser, Alberto Sillitti, Pekka Abrahamsson, and Giancarlo Succi. Does refactoring improve reusability? In International conference on software reuse, pages 287-297. Springer, 2006.
[34] Emerson Murphy-Hill, Chris Parnin, and Andrew P Black. How we refactor, and how we know it. IEEE Transactions on Software Engineering, 38(1):5-18, 2011.
[35] Stas Negara, Nicholas Chen, Mohsen Vakilian, Ralph E. Johnson, and Danny Dig. A comparative study of manual and automated refactorings. In 27th European Conference on Object-Oriented Programming, ECOOP'13, pages 552- 576, Berlin, Heidelberg, 2013. Springer-Verlag. ISBN 978-3-642-39037-1. doi:
10.1007/978-3-642-39038-8_23.
[36] Pedram Nouri. PurityChecker: A Tool for Detecting Purity of Method-level Refactoring Operations. PhD thesis, Concordia University, 2023. URL https:
//spectrum.library.concordia.ca/id/eprint/993129/.
[37] Mark Kent O'Keeffe and Mel Ó Cinnéide. Search-based refactoring for software maintenance. J. Syst. Softw., 81(4):502-516, 2008. doi: 10.1016/J.JSS.2007.06.003.
URL https://doi.org/10.1016/j.jss.2007.06.003.
[38] Jevgenija Pantiuchina, Bin Lin, Fiorella Zampetti, Massimiliano Di Penta, Michele Lanza, and Gabriele Bavota. Why do developers reject refactorings in open-source projects? ACM Trans. Softw. Eng. Methodol., 31(2), December 2021. ISSN 1049- 331X. doi: 10.1145/3487062. URL https://doi.org/10.1145/3487062.
[39] Massimiliano Di Penta, Gabriele Bavota, and Fiorella Zampetti. On the relation- ship between refactoring actions and bugs: A differentiated replication, 2020. URL https://arxiv.org/abs/2009.11685.
[40] Anthony Peruma, Steven Simmons, Eman Abdullah AlOmar, Christian D. New- man, Mohamed Wiem Mkaouer, and Ali Ouni. How do i refactor this? an empirical study on refactoring trends and topics in stack overflow. Empirical Software En- gineering, 27(1), October 2021. ISSN 1573-7616. doi: 10.1007/s10664-021-10045-x.
URL http://dx.doi.org/10.1007/s10664-021-10045-x.
[41] Russell A Poldrack, Thomas Lu, and Gašper Beguš. Ai-assisted coding: Experi- ments with gpt-4. arXiv preprint arXiv:2304.13187, 2023.
[42] Dorin Pomian. Llm-powered extract method, 2024. URL https://plugins.jetbrains. com/plugin/23403-llm-powered-extract-method. Accessed: Jun. 23, 2024.
[43] Dorin Pomian, Abhiram Bellur, Malinda Dilhara, Zarina Kurbatova, Egor Bogo- molov, Timofey Bryksin, and Danny Dig. Next-generation refactoring: Combin- ing LLM insights and IDE capabilities for extract method. In IEEE International Conference on Software Maintenance and Evolution, ICSME 2024, Flagstaff, AZ, USA, October 6-11, 2024, pages 275-287. IEEE, 2024. doi: 10.1109/ICSME58944.
2024.00034. URL https://doi.org/10.1109/ICSME58944.2024.00034.
[44] Dorin Pomian, Abhiram Bellur, Malinda Dilhara, Zarina Kurbatova, Egor Bo- gomolov, Andrey Sokolov, Timofey Bryksin, and Danny Dig. Em-assist: Safe automated extractmethod refactoring with llms. In Marcelo d'Amorim, edi- tor, Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering, FSE 2024, Porto de Galinhas, Brazil, July 15-19, 2024, pages 582-586. ACM, 2024. doi: 10.1145/3663529.3663803. URL
https://doi.org/10.1145/3663529.3663803.
[45] Soumaya Rebai, Marouane Kessentini, Vahid Alizadeh, Oussama Ben Sghaier, and Rick Kazman. Recommending refactorings via commit message analysis. Inf. Softw. Technol., 126:106332, 2020. doi: 10.1016/J.INFSOF.2020.106332. URL
https://doi.org/10.1016/j.infsof.2020.106332.
[46] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.
[47] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sun- daresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297, 2020.
[48] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends(r) in Information Retrieval, 3(4):333-389, 2009.
[49] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. In Marine Carpuat, Marie-Catherine de Marneffe, and Iván Vladimir Meza Ruíz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 3715-3734. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.NAACL-MAIN.272. URL https://doi.org/10.18653/v1/2022.naacl-main.272.
[50] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,

Moritz Hardt, and Sergey Levine, editors, Advances in Neural Informa- tion Processing Systems 36: Annual Conference on Neural Information Pro- cessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10
- 16, 2023, 2023.  URL http://papers.nips.cc/paper_files/paper/2023/hash/
1b44b878bb782e6954cd888628510e90-Abstract-Conference.html.
[51] Atsushi Shirafuji, Yusuke Oda, Jun Suzuki, Makoto Morishita, and Yutaka Watanobe. Refactoring programs using large language models with few-shot examples. In 2023 30th Asia-Pacific Software Engineering Conference (APSEC), pages 151-160. IEEE, 2023.
[52] Danilo Silva, Ricardo Terra, and Marco Tulio Valente. Recommending automated extract method refactorings. In Proceedings of the 22nd international conference on program comprehension, pages 146-156, 2014.
[53] Danilo Silva, Nikolaos Tsantalis, and Marco Túlio Valente. Why we refactor? confessions of github contributors. In Thomas Zimmermann, Jane Cleland-Huang, and Zhendong Su, editors, Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016, Seattle, WA, USA, November 13-18, 2016, pages 858-870. ACM, 2016. doi: 10.1145/2950290.2950305.
URL https://doi.org/10.1145/2950290.2950305.
[54] Danilo Silva, João Paulo da Silva, Gustavo Santos, Ricardo Terra, and Marco Tulio Valente. Refdiff 2.0: A multi-language refactoring detection tool. IEEE Transactions on Software Engineering, 47(12):2786-2802, 2021. doi: 10.1109/TSE.2020.2968072.
[55] Yahya Tashtoush, Zeinab Odat, Izzat M Alsmadi, and Maryan Yatim. Impact of programming features on code readability. 2013.
[56] Tom Tourwé and Tom Mens. Identifying refactoring opportunities using logic meta programmin. In 7th European Conference on Software Maintenance and Reengineering (CSMR 2003), 26-28 March 2003, Benevento, Italy, Proceedings, pages 91-100. IEEE Computer Society, 2003. doi: 10.1109/CSMR.2003.1192416. URL
https://doi.org/10.1109/CSMR.2003.1192416.
[57] Nikolaos Tsantalis and Alexander Chatzigeorgiou. Identification of move method refactoring opportunities. IEEE Trans. Software Eng., 35(3):347-367, 2009. doi: 10.1109/TSE.2009.1. URL https://doi.org/10.1109/TSE.2009.1.
[58] Nikolaos Tsantalis and Alexander Chatzigeorgiou. Identification of extract method refactoring opportunities for the decomposition of methods. J. Syst. Softw., 84(10):1757-1782, 2011. doi: 10.1016/J.JSS.2011.05.016. URL https://doi. org/10.1016/j.jss.2011.05.016.
[59] Nikolaos Tsantalis, Victor Guana, Eleni Stroulia, and Abram Hindle. A multidi- mensional empirical study on refactoring activity. In James R. Cordy, Krzystof Czarnecki, and Sang-Ah Han, editors, Center for Advanced Studies on Collabo- rative Research, CASCON '13, Toronto, ON, Canada, November 18-20, 2013, pages 132-146. IBM / ACM, 2013. URL http://dl.acm.org/citation.cfm?id=2555539.
[60] Nikolaos Tsantalis, Matin Mansouri, Laleh M Eshkevari, Davood Mazinanian, and Danny Dig. Accurate and efficient refactoring detection in commit history. In Proceedings of the 40th international conference on software engineering, pages 483-494, 2018.
[61] Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig. Refactoringminer 2.0. IEEE Transactions on Software Engineering, 48(3):930-950, 2020.
[62] Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Aditya Kanade, Suresh Parthasarathy, and Sriram K. Rajamani. CORE: resolving code quality issues using llms. Proc. ACM Softw. Eng., 1(FSE):789-811, 2024. doi: 10.1145/3643762. URL https://doi.org/10.1145/3643762.
[63] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture- of-agents enhances large language model capabilities. CoRR, abs/2406.04692, 2024. doi: 10.48550/ARXIV.2406.04692. URL https://doi.org/10.48550/arXiv.2406.04692.
[64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903.
[65] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt. Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. In Generative AI for Effective Software Develop- ment, pages 71-108. Springer, 2024.
[66] Di Wu, Fangwen Mu, Lin Shi, Zhaoqiang Guo, Kui Liu, Weiguang Zhuang, Yuqi Zhong, and Li Zhang. ismell: Assembling llms with expert toolsets for code smell detection and refactoring. In Vladimir Filkov, Baishakhi Ray, and Minghui Zhou, editors, Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ASE 2024, Sacramento, CA, USA, October 27 - November 1, 2024, pages 1345-1357. ACM, 2024. doi: 10.1145/3691620.3695508.
URL https://doi.org/10.1145/3691620.3695508.
[67] Sihan Xu, Aishwarya Sivaraman, Siau-Cheng Khoo, and Jing Xu. Gems: An extract method refactoring recommender. In 2017 IEEE 28th International Sympo- sium on Software Reliability Engineering (ISSRE), pages 24-34, 2017.
[68] Tong Ye, Weigang Huang, Xuhong Zhang, Tengfei Ma, Peiyu Liu, Jianwei Yin, and Wenhai Wang. Llm4effi: Leveraging large language models to enhance code efficiency and correctness, 2025. URL https://arxiv.org/abs/2502.18489.
[69] Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu. Refac- toring to pythonic idioms: A hybrid knowledge-driven approach leveraging large language models. Proceedings of the ACM on Software Engineering, 1(FSE): 1107-1128, 2024.





How We Refactor and How We Document it? On the Use of Supervised Machine Learning Algorithms to Classify Refactoring Documentation
Eman Abdullah AlOmara,*, Anthony Perumaa, Mohamed Wiem Mkaouera,
Christian Newmana, Ali Ounib, Marouane Kessentinic
aRochester Institute of Technology, Rochester, NY, USA
bETS Montreal, University of Quebec, Montreal, QC, Canada
cUniversity of Michigan, Dearborn, MI, USA




Abstract
Refactoring is the art of improving the structural design of a software system without altering its external behavior. Today, refactoring has become a well es- tablished and disciplined software engineering practice that has attracted a sig- nificant amount of research presuming that refactoring is primarily motivated by the need to improve system structures. However, recent studies have shown that developers may incorporate refactoring strategies in other development-related activities that go beyond improving the design especially with the emerging challenges in contemporary software engineering. Unfortunately, these studies are limited to developer interviews and a reduced set of projects.
   To cope with the above-mentioned limitations, we aim to better understand what motivates developers to apply a refactoring by mining and automatically classifying a large set of 111,884 commits containing refactoring activities, ex- tracted from 800 open source Java projects. We trained a multi-class classifier to categorize these commits into three categories, namely, Internal Quality At- tribute, External Quality Attribute, and Code Smell Resolution, along with the traditional Bug Fix and Functional categories. This classification challenges the original definition of refactoring, being exclusive to improving software design and fixing code smells. Furthermore, to better understand our classification results, we qualitatively analyzed commit messages to extract textual patterns that developers regularly use to describe their refactoring activities.
   The results of our empirical investigation show that (1) fixing code smells is not the main driver for developers to refactoring their code bases. Refactoring is solicited for a wide variety of reasons, going beyond its traditional definition;
(2) the distribution of refactoring operations differ between production and test files; (3) developers use a variety of patterns to purposefully target refactoring-


*Corresponding author
    Email addresses: eman.alomar@mail.rit.edu (Eman Abdullah AlOmar), anthony.peruma@mail.rit.edu (Anthony Peruma), mwmvse@rit.edu (Mohamed Wiem Mkaouer), cdnvse@rit.edu (Christian Newman), ali.ouni@etsmtl.ca (Ali Ouni), marouane@umich.edu (Marouane Kessentini)


Preprint submitted to Journal of LATEX Templates	28th October 2020



related activities; (4) the textual patterns, extracted from commit messages, provide a better coverage for how developers document their refactorings.
Keywords: Refactoring, Software Quality, Software Engineering, Machine Learning


1. Introduction
   The success of a software system depends on its ability to retain high quality of design in the face of continuous change. However, managing the growth of the software while continuously developing its functionalities is challenging, and can account for up to 75% of the total development (Erlikh, 2000; Barry et al., 1981). One key practice to cope with this challenge is refactoring. Refactoring is the art of remodeling the software design without altering its functionalities (Fowler et al., 1999; AlDallal & Abdin, 2017). It was popularized by (Fowler et al., 1999), who identified 72 refactoring types and provided examples of how to apply them in his catalog.
   Refactoring is a critical software maintenance activity that is performed by developers for an amalgamation of reasons (Tsantalis et al., 2013; Silva et al., 2016; Palomba et al., 2017). Refactoring activities in the source code can be automatically detected (Dig et al., 2006; Tsantalis et al., 2013) providing a unique opportunity to practitioners and researchers to analyze how developers maintain their code during different phases of the development life-cycle and over large periods of time. Such valuable knowledge is vital for understanding more about the maintenance phase; the most costly phase in software develop- ment (Boehm, 2002; Erlikh, 2000). To detect refactorings, the state-of-the-art techniques (Dig et al., 2006; Tsantalis et al., 2013) typically search at the level of commits. As a result, these techniques are also able to group commit messages with their corresponding refactorings.
   Commit messages are the description, in natural language, of the code-level changes. To understand the nature of the change, recent studies have been using natural language processing to process commit messages for multiple reasons, such as classification of code changes (Hindle et al., 2008), change summariz- ation (McBurney et al., 2017), change bug-proneness (Xia et al., 2016), and developer's rationale behind their coding decisions (Alkadhi et al., 2018). That is, commit messages are a common way for researchers to study developer ra- tionale behind different types of changes to the code. There are two primarily challenges to using commit messages to understand refactorings: 1) the commit message does not have to refer to the refactoring that took place at all, 2) de- velopers have many ways of describing the same activity. For example, instead of explicitly stating that they are refactoring, a developer may instead state that they are performing code clean-up or simplifying a method. Developers are inconsistent in the way they discuss refactorings in commit messages. This makes it difficult to perform analysis on commit messages, since researchers may find it challenging to determine whether a commit message discusses the



refactoring(s) being performed or not. Thus, it is hard to determine when the commit message is discussing a refactoring at all and it is hard to determine how a commit message is discussing the refactoring.
   To cope with the above-mentioned challenges, the purpose of this study is to augment our understanding of the development contexts that trigger refactoring activities and enable future research to take development contexts into account more effectively when studying refactorings. Thus, the advantages of analyz- ing the textual description of the code change that was intended to describe refactoring activities are three-fold: 1) it improves our ability to study commit message content and relate this content to refactorings; a challenging task which posed a significant hurdle in recent work on contextualizing rename refactorings (Peruma et al., 2018, 2019b), 2) it gives us a stronger understanding of com- mit message practices and could help us improve commit message generation by making it clear how developers prefer to express their refactoring activities, 3) it provides us with a way of relating common words and phrases used to describe refactorings with one another. Typically frameworks like WordNet, which does not recognize refactoring phrases and terminology, are used for this task. Our dataset and methodology reduces the need to rely on frameworks which are not trained for natural language found in software projects.
   In this paper, we present a way to partially-automatically detect how de- velopers document their refactorings in commit messages, and classify these into categories that reflect the type of activity that refactoring was co-located with. The goal of this work is to create a data set of terms and phrases, used by de- velopers, to describe refactorings. Further, we group these words and phrases by maintenance-type (e.g., bug fix, external, code smell) to obtain a fine-grained and maintenance-type-specific dataset of terms and phrases. Recent studies have shown the feasibility of extracting insights of software quality from de- velopers inline documentation. For instance, mining developer's comments has unveiled how developers knowingly commit code that is either incomplete, tem- porary, or faulty. Such phenomenon is known as Self-Admitted Technical Debt (SATD) (Potdar & Shihab, 2014). Similarly, our previous study has intro- duced Self-Affirmed Refactoring (SAR) (AlOmar et al., 2019a, 2020a), defined as developer's explicit documentation of refactoring operations intentionally in- troduced during a code change.
To perform this analysis, we formulate the following research questions:
• RQ1. To what purposes developers refactor their code?
While previous surveys studied how developers apply refactorings in vary- ing development contexts, none of them have measured the ubiquity of these varying contexts in practice. Therefore, it is important to quantify the distribution of refactoring activities performed in varying development contexts to augment our understanding of refactoring in theory versus in practice.
• RQ1.1 Do software developers perform different types of refactoring op- erations on test code and production code between categories?



This question further explores the findings of the classification to see to what extent developers refactor production files differently from test files.
• RQ2. What patterns do developers use to describe their refactoring activ- ities?
Since there is no consensus on how to formally document refactoring changes, we intend to extract (from commit messages) words and phrases commonly used by developers in practice to document their refactorings. Such information is useful from many perspectives. First, it allows to un- derstand the rationale behind the applied refactorings, e.g., fixing code smells or improving specific quality attributes. Moreover, it may reveal what specific refactoring operations are being documented, and whether developers explicitly mention it as part of their documentation. Such de- tails are of crucial importance especially in modern code review the help code reviewers understand the rationale behind such refactorings. Little is known about how developers document refactoring as previous studies mainly rely on the keyword refactor to annotate such documentation.
• RQ2.1 Do commits containing the label Refactor indicate more refactor- ing activity than those without the label?
We revisit the hypothesis raised by (Murphy-Hill et al., 2008) about whether developers use a specific pattern, i.e., "refactor" when describing their re- factoring activities.
   The dataset of classified refactorings along with textual patterns are avail- able online (AlOmar, 2020 (last accessed October 20, 2020) for replication and extension purposes.
   The remainder of this paper is organized as follows. Section 2 discusses the notion of refactoring related documentation or Self-Affirmed Refactoring. Section 3 enumerates the previous related studies, and shows how we extracted the categories used for the classification. In Section 4, we give the design of our empirical study, mainly with regard to the construction of the dataset and classification. Section 5 presents the study results while further discussing our findings in Section 6. The next Section 7 reports threats to the validity of our experiments, before concluding the paper in Section 8.

2. Self-Affirmed Refactoring
   Commit messages are the description, in natural language, of the code-level changes. In this paper, we want to automatically detect how refactoring is doc- umented in the commit message, and classify it into categories that reflect the type of activity that refactoring was co-located with. Earlier studies were rely- ing on developer surveys for extracting such information. But multiple studies have been detecting the performed refactoring operations, e.g., rename class, move method etc. within committed changes to better understand how de- velopers cope with bad design decisions, also known as design antipatterns, and



to extract their removal strategy through the selection of the appropriate set of refactoring operations (Tsantalis et al., 2018). As the accuracy of refactor- ing detectors has reached a relatively high rate, mined commits' messages and their issues descriptions constitute a rich space to understand how developers describe, in natural language, their refactoring activities. Yet, such information retrieval can be challenging since there are no common standards on how de- velopers should be formally documenting their refactorings, besides inheriting all the challenges related to natural language processing (Tan et al., 1999).
   However, recent studies have shown the feasibility of extracting insights of software quality from developer's inline documentation. For instance, mining developers' comments has unveiled how developers knowingly commit code that is either incomplete, temporary, or faulty. Such phenomenon is known as Self- Admitted Technical Debt (SATD) (Potdar & Shihab, 2014). Similarly, our pre- vious study has introduced Self-Affirmed Refactoring (SAR) (AlOmar et al., 2019a, 2020a), defined as developers' explicit documentation of refactoring op- erations intentionally introduced during a code change.
   As explained later in the related work section, existing studies locate refact- oring documentation through the localization of the keyword "refactor", being the most intuitive and widely known. However, a recent study has also shown that the "refactor" can also be misused, and such information becomes mislead- ing (Zhang et al., 2018). Yet, such findings are mainly taken from interviews. In this study, we leverage the existence of a large set of refactorings, extracted form a wide variety of projects, to design an empirical study to classify the context in which it was performed, for that, we start with the traditional categorization of Swanson (Swanson, 1976), and we extend its "Perfective" category to cover what has been known by existing studies as drivers to recommend refactorings. This study also further explores how developers document refactorings, and extracts a new terminology that was found to be consistently used in refactoring-related commit messages.

3. Related Work
   This paper focuses on mining commits to initially detect refactorings and then to classify them. Thus, in this section, we are interested in exploring refactoring documentation, along with the research on refactoring motivations.
3.1. Refactoring Documentation
   A number of studies have focused recently on the identification and detec- tion of refactoring activities during the software life-cycle. One of the common approaches to identify refactoring activities is to analyze the commit messages in versioned repositories. (Stroggylos & Spinellis, 2007) opted for searching words stemming from the verb "refactor" such as "refactoring" or "refactored" to identify refactoring-related commits. (Ratzinger, 2007; Ratzinger et al., 2008) also used a similar keyword-based approach to detect refactoring activity between a pair of program versions to identify whether a transformation con- tains refactoring. The authors identified refactorings based on a set of keywords



Table 1: Existing works on refactoring identification.

Study
Year
Purpose
Approach
Source of Info.
Ref. Patterns
(Stroggylos & Spinellis, 2007)
2007
Identify refactoring commits
Mining commit logs
General commits
1 keyword
(Ratzinger et al., 2008; Ratzinger, 2007)
2007 & 2008
Identify refactoring commits
Mining commit logs
General commits
13 keywords
(Murphy-Hill et al., 2012)
2012
Identify refactoring commits
Ratzinger's approach
General commits
13 keywords
(Soares et al., 2013)
2013
Analyze refactoring activity
Ratzinger's approach
General commits
13 keywords



Manual analysis





Dynamic analysis


(Kim et al., 2014)
2014
Identify refactoring commits
Identifying refactoring branches
Refactoring branch
Top 10 keywords



Mining commit logs


(Zhang et al., 2018)
2018
Identify refactoring commits
Mining commit logs
General commits
22 keywords
(AlOmar et al., 2019a)
2019
Identify refactoring patterns
Detecting refactorings
Refactoring commits
87 keywords & phrases



Extracting commit messages




detected in commit messages, and focusing, in particular, on the following 13 terms in their search approach: refactor, restruct, clean, not used, unused, re- format, import, remove, replace, split, reorg, rename, and move.
   Later, (Murphy-Hill et al., 2012) replicated Ratzinger's experiment in two open source systems using Ratzinger's 13 keywords. They conclude that commit messages in version histories are unreliable indicators of refactoring activities. This is due to the fact that developers do not consistently report/document refactoring activities in the commit messages. In another study, (Soares et al., 2013) compared and evaluated three approaches, namely, manual analysis, com- mit message (Ratzinger et al.'s approach), and dynamic analysis (SafeRefactor approach (Soares et al., 2009)) to analyze refactorings in open source repositor- ies, in terms of behavioral preservation. The authors found, in their experiment, that manual analysis achieves the best results in this comparative study and is considered as the most reliable approach in detecting behavior-preserving trans- formations.
   In another study, (Kim et al., 2014) surveyed 328 professional software en- gineers at Microsoft to investigate when and how they do refactoring. They first identified refactoring branches and then asked developers about the keywords that are usually used to mark refactoring events in change commit messages. When surveyed, the developers mentioned several keywords to mark refactoring activities. Kim et al. matched the top ten refactoring-related keywords iden- tified from the survey (refactor, clean-up, rewrite, restructure, redesign, move, extract, improve, split, reorganize, rename) against the commit messages to identify refactoring commits from version histories. Using this approach, they found 94.29% of commits do not have any of the keywords, and only 5.76% of commits included refactoring-related keywords.
   (Zhang et al., 2018) performed a preliminary investigation of Self-Admitted Refactoring (SAR) in three open source systems. They first extracted 22 keywords from a list of refactoring operations defined in the Fowler's book (Fowler et al., 1999) as a basis for SAR identification. After identifying candidate SARs, they used Ref-Finder (Kim et al., 2010) to validate whether refactorings have been applied. In their work, they used code smells to assess the impact of SAR on the structural quality of the source code. Their main findings are the following
(1) SAR tends to enhance the software quality although there is a small per- centage of SAR that have introduced code smells, and (2) the most frequent code smells that are introduced or reduced depend highly on the nature of the



studied projects. They concluded that SAR is a signal that helps to locate re- factoring events, but it does not guarantee the application of refactorings. We summarize these state-of-the-art approaches in Table 1.
3.2. Refactoring Motivation
   (Silva et al., 2016) investigate what motivates developers when applying specific refactoring operations by surveying GitHub contributors of 124 soft- ware projects. They observe that refactoring activities are mainly caused by changes in the project requirements and much less by code smells. (Palomba et al., 2017) verify the relationship between the application of refactoring op- erations and different types of code changes (i.e., Fault Repairing Modification, Feature Introduction Modification, and General Maintenance Modification) over the change history of three open source systems. Their main findings are that developers apply refactoring to: 1) improve comprehensibility and maintain- ability when fixing bugs, 2) improve code cohesion when adding new features, and 3) improve the comprehensibility when performing general maintenance activities. On the other hand, (Kim et al., 2014) do not differentiate the mo- tivations between different refactoring types. They surveyed 328 professional software engineers at Microsoft to investigate when and how they do refactor- ing. When surveyed, the developers cited the main benefits of refactoring to be: improved readability (43%), improved maintainability (30%), improved ex- tensibility (27%) and fewer bugs (27%). When asked what provokes them to refactor, the main reason provided was poor readability (22%). Only one code smell (i.e, code duplication) was mentioned (13%).
   (Murphy-Hill et al., 2012) examine how programmers perform refactoring in practice by monitoring their activity and recording all their refactorings. They distinguished between high, medium and low-level refactorings. High-level re- factorings tend to change code elements signatures without changing their im- plementation e.g., Move Class/Method, Rename Package/Class. Medium-level refactorings change both signatures and code blocks, e.g., Extract Method, In- line Method. Low level refactorings only change code blocks, e.g., Extract Local Variable, Rename Local Variable. Some of the key findings of this study are that 1) most of the refactoring is floss, i.e., applied to reach other goals such as adding new features or fixing bugs, 2) almost all the refactoring operations are done manually by developers without the help of any tool, and 3) commit messages in version histories are unreliable indicators of refactoring activity be- cause developers tend to not explicitly state refactoring activities when writing commit messages. It is due to this observation that, in this study, we do not rely on commits messages to identify refactorings. Instead, we use them to identify the motivation behind the refactoring.
   (Moser et al., 2006) study the impact of refactoring on reusability. They showed that refactoring increases the reusability of classes in an industrial, agile environment. In a subsequent study, (Moser et al., 2007) question the effective- ness of refactoring on increasing the productivity in agile environments. They performed a comparative study of developers coding effort before and after re- factoring their code. They measured the developer's effort in terms of added



lines of code and time. Their findings show that not only does the refactored system improve in terms of coupling and complexity, but also that the coding effort was reduced and the difference is statistically significant.
   (Szoke et al., 2014) conduct 5 large-scale industrial case studies on the applic- ation of refactoring while fixing coding issues, they have shown that developers tend to apply refactorings manually at the expense of a large time overhead. (Szoke et al., 2017) extend their study by investigating whether the refactorings applied when fixing issues did improve the system's nonfunctional requirements with regard to maintainability. They noticed that refactorings performed manu- ally by developers do not significantly improve the system's maintainability like those generated using fully automated tools. They concluded that refactoring cannot be cornered only in the context of design improvement.
   (Tsantalis et al., 2013) manually inspect the source code for each detected refactoring with a text diff tool to reveal the main drivers that motivated the developers for the applied refactoring. Besides code smell resolution, they found that introduction of extension points and the resolution of backward compatib- ility issues are also reasons behind the application of a given refactoring type. In another study, (Wang, 2009) generally focuses on the human and social factors affecting the refactoring practice rather than on the technical motivations. He interviewed 10 industrial developers and found a list of intrinsic (e.g., respons- ibility of code authorship) and external (e.g., recognitions from others) factors motivating refactoring activity.
   Another study relevant to our work is by (Vassallo et al., 2019). They per- formed an exploratory study on refactoring activities in 200 projects, by mining their performed refactoring operations. Their findings show the need for better understanding the rationale behind these operations, and so our study focuses on contextualizing refactoring activities within typical software engineering activ- ities and questions whether such difference in developers' intentions would infer different refactorings strategies. Such investigation has not been investigated before in the literature. More recently, (Pantiuchina et al., 2020) present a mining-based study to investigate why developers are performing refactoring in the history of 150 open source systems. Particularly, they analyzed 551 pull re- quests implemented refactoring operations and reported a refactoring taxonomy that generalizes the ones existing in the literature. (Paixão et al., 2020) per- form an empirical study on refactoring activities in code review in which they captured Bug Fix and Feature refactoring categories. (AlOmar et al., 2020c) studied how developers refactor their code to improve its reuse by analyzing the impact of reusability refactorings on the state-of-the-art reusability metrics. Figure 1 depicts how our classification clusters the existing refactoring taxonomy reported in the literature (Moser et al., 2006, 2007; Tsantalis et al., 2013; Kim et al., 2014; Silva et al., 2016; Palomba et al., 2017; Vassallo et al., 2019; AlOmar et al., 2019b; Pantiuchina et al., 2020; Paixão et al., 2020; AlOmar et al., 2020c). As can be seen, our classification covers these categories. Furthermore, previous studies have shown that refactoring can be used outside of the design box, e.g., correction flaky tests, code naturalness, etc., therefore, our study is the first to engage the automated classification of commit messages in order to cluster the



refactoring effort that has been performed in non-design circumstances.
   All the above-mentioned studies have agreed on the existence of motivations that go beyond the basic need for improving the system's design. Refactoring activities have been solicited in scenarios that have been coined by the previous studies as follows: Functional, Bug Fix, Internal Quality Attribute, Code Smell Resolution, and External Quality Attribute. Since these categories are the main drivers for refactoring activities, we decided to cluster our mined refactoring operations according to these groups.
   Our proposal differs from commit classification-related studies, as their clas- sification targeted general maintenance activities (perfective, adaptive) and was not specific to commits containing messages describing refactoring activities. In this study, we subdivide what would have been considered "perfective" in previous studies, into three separate categories, namely, Internal Quality At- tribute, External Quality Attribute and Code Smell Resolution. This division is inherited from the analysis of previous papers whose detection of refactoring opportunities rely on the optimization of high-level design principles, structural metrics, and reduction of code smells. Thus, this is not a typical commit clas- sification since refactoring related commit messages contain a strong overlap in their terminology and so their classification is challenging. Moreover, as we previously stated, existing studies in recommending refactoring are based on (i) Internal Quality Attribute (ii) External Quality Attribute, and (iii) Code Smell Resolution. The classification of commits according to these categories, will be an empirical evidence of whether and to what extent these factors are being used in practice. To perform the classification, we use existing classifiers (e.g., Random Forest, Naive Bayes Multinominal, etc) that have been used by several studies (e.g., (Hindle et al., 2011; Kochhar et al., 2014; Levin & Yehudai, 2017; Hönel et al., 2019; AlOmar et al., 2020a)) in the context of commit classification and challenge them using our defined set of classes. Although several studies (Hattori & Lanza, 2008; Mauczka et al., 2012; Hindle et al., 2009; Amor et al., 2006; Levin & Yehudai, 2017; Hindle et al., 2008; Mauczka et al., 2015; Yan et al., 2016) have discussed how to automatically classify change messages into Swanson's general maintenance categories (i.e., Corrective, Adaptive, Perfect- ive), refactoring, in general, has been classified as a sub-type of "Perfective" in these maintenance categories. While we are motivated by the above-mentioned studies, our work is still different from them since we apply the machine learning technique to automatically classify commit messages into five main refactoring motivations defined in this study, i.e., 'Functional', 'Bug Fix', 'Internal Quality Attribute', 'Code Smell Resolution', and 'External Quality Attribute'.

4. Empirical Study Setup
   To answer our research questions defined in Section 1, we design a five-steps approach as shown in Figure 2. Our approach consists of: (1) data collection,
(2) refactoring detection, (3) automatic refactoring classification, (4) unit test files detection, and (5) refactoring documentation extraction.













Figure 1: Refactoring motivation.






Figure 2: Empirical study design overview.



4.1. Phase 1: Data Collection
   Our first step consists of randomly selecting 800 projects, which were cur- ated open-source Java projects hosted on GitHub. These curated projects were selected from a dataset made available by (Munaiah et al., 2017), while verifying that they were Java-based, the only language supported by Refactoring Miner (Tsantalis et al., 2018). The authors of this dataset selected "well-engineered software projects" based on the projects' use of software engineering best prac- tices such as documentation, testing, and project management. Additionally, these projects are non-forked (i.e., not cloned from other projects) as forked projects may impact our conclusions by introducing duplicate code and data. We cloned the 800 selected projects having a total of 748,001 commits, and a total of 711,495 refactoring operations from 111,884 refactoring commits. Ad- ditionally, these projects contain on average 935 commits and 19 developers. An overview of the project's statistics is provided in Table 2. This table shows the total number of Java projects used in this study (800), the total number of commits across all projects combined (748,001), the total number of refact- oring commits and the associated refactoring operations respectively, 111,884 and 711,495. The table also details the number of refactoring operations per code element at different levels of granularity, including method, attribute, class, variable, parameter, package, and interface, ordered from highest down to the lowest. Additionally, the standard deviation reported in the table shows that these projects are very diverse.
Table 2: Projects overview.

Item
Count
Standard Deviation
Total of projects
800
N/A
Total commits
748,001
1233.69
Refactoring commits
111,884
195.48
Refactoring operations
711,495
2402.12
Considered Projects - Refactored Code Elements
Code Element
# of Refactorings
Standard Deviation
Method
222,785
415.55
Attribute
201,791
1854.35
Class
121,625
273.24
Variable
115,717
383.91
Parameter
48,054
127.48
Package
2380
8.25
Interface
1742
6.01



4.2. Phase 2: Refactoring Detection
   To extract the entire refactoring history of each project, we used the Re- factoring Miner1 tool introduced by (Tsantalis et al., 2018). We decided to use Refactoring Miner as it has shown promising results in detecting refactorings compared to the state-of-the-art available tools (Tsantalis et al., 2018) and is suitable for a study that requires a high degree of automation since it can be used through its external API. The Eclipse plug-in refactoring detection tools (e.g., Ref-Finder (Kim et al., 2010)), in contrast, require user interaction to se- lect projects as inputs and trigger the refactoring detection, which is impractical since multiple releases of the same project have to be imported to Eclipse to identify the refactoring history.

4.3. Phase 3: Commits Classification
   After all refactoring operations are collected, we need to classify them. As part of the development workflow, developers associate a message with each com- mit they make to the project repository. These commit messages are usually written using natural language, and generally convey some information about the commit they represent. In this study, we aim to determine the type of refact- oring activity performed by the developer based on the message associated with a refactoring-based commit. We started by collecting the different motivations that drive developers to refactor their code as reported in the literature (Kim et al., 2014; AlDallal & Abdin, 2017; Fowler et al., 1999; Lanza & Marinescu, 2007; Silva et al., 2016; Tsantalis et al., 2013; Palomba et al., 2017; Murphy- Hill et al., 2012). Then, we search for common categories among the reported motivations. The following step involves identifying categories clustering func- tional requirements, quality attributes and software issues under the identified categories. This process resulted in five different categories. Hence, we aim to classify the refactoring commit, into one of five main categories: 'Functional ', 'Bug Fix ', 'Internal Quality Attribute', 'Code Smell Resolution', and 'External Quality Attribute'. Table 3 provides a description of each category.
   In this supervised multi-class classification problem, we followed a multi- staged approach to build our model for commit messages classification. The first stage consists of the model construction. In the second stage, we utilized the built model to classify the entire dataset of commit messages. An overview of our methodology is depicted in Figure 2. In the following subsections, we describe in detail the different steps in each stage.
Model Construction
   In the first stage of the experiment, our goal is to build a model from a corpus real world documented refactorings (i.e., commit message) to be utilized in the second stage to classify commit messages. The following subsections detail the different steps in the model construction phase.


1https://github.com/tsantalis/RefactoringMiner



Table 3: Classification categories.

Category	Description

Functional	Feature implementation, modification or removal
Bug Fix	Tagging, debugging, and application of bug fixes Restructuring and repackaging the system's code elements to improve its internal design such as coupling and cohesion
Removal of design defects that might violate the fundamentals

Code Smell Resolution

of software design principles and decrease code quality such as duplicated code and long method

Property or feature that indicates the effectiveness of a system such as testability, understandability, and readability

4.3.1. Data Annotation
   In order to construct a machine learning model, a gold set of labeled data is needed to train and test the model. To prepare this gold set, a manual annotation (i.e., labeling) of commit messages needs to be performed by sub- ject experts. To this end, we annotated 1,702 commit messages. This quantity roughly equates to a sample size with a confidence level of 95% and a confidence interval of 2. Confidence level and interval are utilized to obtain an accurate and statistically significant sample size from a population (Brownlee, 2018). The au- thors of this paper performed the annotation of the commit messages. Provided to each author was a random set of commit messages along with details defining the annotation labels. Each annotator had to label each provided commit mes- sage with a label of either 'Functional', 'Bug Fix', 'Internal Quality Attribute', 'Code Smell Resolution', and 'External Quality Attribute'. To mitigate bias in the annotation process, the annotated commit messages were peer-reviewed by the same group. All decisions made during the review had to be unanim- ous; discordant commit messages were discarded and replaced. In total, we annotated 348 commit messages as 'Functional', 'Bug Fix', 'Internal Quality Attribute', and 'Code Smell Resolution', while 310 messages were labeled as 'External Quality Attribute'.

4.3.2. Text Pre-Processing
   To better support the model in correctly classifying commit messages, we performed a series of text normalization activities. Normalization is a process of transforming non-standard words into a standard and convenient format (Jur- afsky & Martin, 2019). Similar to (Kochhar et al., 2014; Le et al., 2015), the activities involved in our pre-processing stage included: (1) expansion of word contractions (e.g., 'I'm' ? 'I am'), (2) removal of URLs, single-character words, numbers, punctuation and non-alphabet characters, stop words, and (3) redu- cing each word to its lemma. The lemmatization process either replaces the suf- fix of a word with a different one or removes the suffix of a word to get the basic word form (lemma) (Lane et al., 2019). In our work, the lemmatization pro- cess involves sentence separation, part-of-speech identification, and generating dictionary form. We split the commit messages into sentences, since input text could constitute a long chunk of text. The part-of-speech identification helps in filtering words used as features that aid in key-phrase extraction. Lastly, since



the word could have multiple dictionary forms, only the most probable form is generated. We opted to use lemmatization over stemming, as the lemma of a word is a valid English word (Lane et al., 2019). In relation to stopwords, we used the default set of stopwords supplied by NLTK (Bird, 2002) and also ad- ded our own set of custom stop words. To derive the set of custom stop words, we generated and manually analyzed the set of frequently occurring words in our corpus. Custom stop words include 'git', 'code', 'refactor', 'svn', 'gitsvnid', 'signedoffby', 'reviewedon', 'testedby', 'us', id', 'changeid', 'lot', 'small', 'thing', 'way'. Additionally, for more effective pre-processing, we tokenized each commit message. Tokenization is the process of dividing the text into its constituent set of words.

4.3.3. Training/Test Split
   To gauge the accuracy of a machine learning model, the implemented model must be evaluated on a never-seen-before set of observations with known labels. To construct this set of observations, the set of annotated commit messages were divided into two sub-datasets - a training set and a test set. The training set was utilized to construct the model while the test set was utilized to evaluate the classification ability of the model. For our experiment, we performed a shuffled stratified split of the annotated dataset. Our test dataset contained 25% of the annotated commit messages, while the training dataset contained the remaining 75% of annotated commit messages. This split results in the training dataset containing a total of 1,276 commit messages, which breaks down to 246 'Functional', 271 'BugFix', 255 'Internal', 276 'CodeSmell', and 228 'External' labeled commit messages. The stratification was performed based on the class (i.e., annotated label) of the commit messages. The use of a random stratified split ensures a better representation of the different types (i.e., labels) of commit messages and helps reduce the variability within the strata (Singh & Mangat, 2013).

4.3.4. Feature Extraction
   In order to create a model, we need to provide the classifier with a set of properties or features that are associated with the observations (i.e., commit messages) in our dataset. However, not all features associated with each obser- vation will be useful in improving the prediction abilities of the model. Hence, a feature engineering task is required to determine the set of optimum features (Zheng & Casari, 2018). In our study, we constructed our model using the text in the commit message. Hence, the feature for this model is limited to the commit message. We utilized Term Frequency-Inverse Document Frequency (TF-IDF) (Manning et al., 2008), commonly used in the literature (Lin et al., 2013; Le et al., 2015), to convert the textual data into a vector space model that can be passed into the classifier. In our experiments, we evaluate the accuracy of the model by constructing the TF-IDF vectors using different types of N-Grams and feature sizes. The N-Gram technique is a set of n-word that occurs in a text set and could be used as a feature to represent that text (Kowsari et al., 2019). In general, the N-Gram term has more semantic than an isolated word.



Some of the keywords (e.g., "extract ") do not provide much information when used on its own. However, when collecting N-Gram from commit message (e.g., Refactor createOrUpdate method in MongoChannelStore to extract methods and make code more readable), the keyword "extract" clearly indicates that this re- factoring commit belongs to Extract Method refactoring. In our classification, we use N-Grams since it is very common to enhance the performance of text classification (Tan et al., 2002). Using TF-IDF, we can determine words that are common and rare across the documents (i.e., commit messages) in our data- set; the model utilizes these words. In other words, The value for each N-Gram is proportional to its TF score multiplied by its IDF score. Thus, each prepro- cessed word in the commit message is assigned a value which is the weight of the word computed using this weighting scheme. TF-IDF gives greater weight (e.g., value) to words which occur frequently in fewer documents rather than words which occur frequently in many documents.

4.3.5. Model Training
   For our study, we evaluated the accuracy of six machine learning classifi- ers: Random Forest, Logistic Regression, Multinomial Naive Bayes, K-Nearest Neighbors, Support Vector Classification (C-Support Vector Classification based on LIBSVM (Deng et al., 2012; Chang & Lin, 2011)), and Decision Tree (CART (Breiman, 2017)). We selected these classifiers since they are widely adopted in several classification problems in software engineering, as reported in Sec- tion 3. It is important to note that the library containing the classification algorithms are capable of multiclass classification. As per the Python's SKlearn documentation, Random Forest, K-Nearest Neighbors, Logistic Regression, and Multinomial Naive Bayes are inherently multiclass (SKlearn, 2007a), while SVC utilizes a one-vs-one approach to handle multiclass (SKlearn, 2007b). Moreover, to ensure consistency, we ran each classifier with the same set of test and training data each time we updated the input features.

4.3.6. Model Tuning & Evaluation
   The purpose of this stage in the model construction process is to obtain the optimal set of classifier parameters that provide the highest performance; in other words, the objective of this task is to tune the hyperparameters. For example, for the K-Nearest Neighbors classifier, we tuned the number of neigh- bors hyperparameter (i.e., 'k') by evaluating the accuracy of the model as we increased the value of 'k' from 1 to 50 in increments of one. We tuned at least one hyperparameter associated with each classifier in our list. For numeric- based hyperparameters, we determined the bounds/range for testing through continuously running the classifier with a different range of values to identify the appropriate minimum and maximum value.
   We performed our hyperparameter tuning on the training dataset using a combination of 10-fold cross-validation and an exhaustive grid search (Dangeti, 2017). Our test dataset did not take part in the training process, which provides a more realistic model evaluation. This approach is also known to prevent overfitting that leads to incorrect conclusions. Grid search utilizes a brute force



technique to evaluate all combinations of hyperparameters to obtain the best performance. It is used to find the optimal hyperparameters of a model which results in the most accurate predictions. Since our classification is multiclass, we relied on the Micro-F1 score. The combination of hyperparameters that resulted in the highest Micro-F1 score was selected to construct the model. We provide, in Table 4, the optimal hyperparameter values for the classification algorithms in our study.
Table 4: Optimal parameter values for the classification algorithms.

Algorithm	Parameter  Value

max_depth	78

Random Forest


Support Vector Classification

n_estimators 500 criterion	gini
bootstrap	false
c	1.99
gamma	scale
kernel	linear


Decision Tree
criterion max_depth
gini 75

Logistic Regression
penalty solver
c
l1 liblinear 1.0
Multinomial Naive Bayes
alpha
2.63
K-Nearest Neighbors	n_neighbors  69



4.3.7. Optimized Model
   In this stage, the optimized model produced by the training phase is utilized to predict the labels of the test dataset. Based on the predictions, we measure the precision and recall for each label as well as the overall F1-score of the model. In Section 5, we detail our classification results.
Model Classification
   In this stage of our experiment, we utilized the optimized model that we created in the prior stage. However, to be consistent, before classifying each commit message, we performed the same text pre-processing activities, as in the prior stage, on the commit message. The result of this stage is the classification of each refactoring commit into one of the five categories. The output of this classification process was utilized in our experiments in order to answer our research questions.

4.4. Phase 4: Unit Test File Detection
   As part of our study, we distinguish between refactorings applied to pro- duction and unit test files and perform comparisons against both production- file-based refactorings versus test-file-based refactorings. To identify all test files that were refactored, we followed the same detection approach as (Peruma et al., 2019a). In this approach, following JUnit's file naming standards2, we


2https://junit.org/junit4/faq.html#running_15




first extracted all refactored Java source files where the filename either starts or ends with the word "test". Next, we utilized JavaParser3 to parse each extrac- ted file. By parsing the files, we were able to eliminate Java files that contained syntax errors and were able to detect if the file contained JUnit-based unit test methods accurately, thereby cutting down on false positives. Finally, to ensure that the files were indeed unit test files, we checked if the files contained unit test methods. As per JUnit specifications, a test method should have a public access modifier, and either has an annotation called @Test (JUnit 4), or the method name should start with "test" (JUnit 3).

4.5. Phase 5: Refactoring Patterns Extraction
   To identify self-affirmed refactoring patterns, we perform manual analysis similar to our previous work (AlOmar et al., 2019a). Since commit messages are written in natural language and we need to understand how developers doc- ument their refactoring activities, we manually analyzed commit messages by reading through each message to identify self-affirmed refactorings. We then ex- tracted these commit comments to specific patterns (i.e., a keyword or phrase). To avoid redundancy of any kind of patterns, we only considered one phrase if we found different forms of patterns that have the same meaning. For example, if we find patterns such as "simplifying the code", "code simplification", and "simplify code", we add only one of these similar phrases in the list of patterns. This enables having a list of the most insightful and unique patterns. It also helps in making more concise patterns that are usable for readers. We also analyzed the top 100 features, distilled by the classifier, for each category.
   The manual analysis process took approximately 20 days in total. In the first two weeks, the authors had regular meetings to discuss top features, ex- tracted from each category, to understand how each class was represented by its corresponding set of keywords, along with extracting any patterns that are most likely to be descriptive to refactoring, besides being another verification level of the classification accuracy. Moreover, during these meetings, the ex- traction of textual patterns from commit messages was also performed by the authors. Due to the subjective nature of this process, we opted to report as many keywords as possible for better coverage. When reporting keywords from top features, we kept the majority of keywords, for each category. keywords that were removed were either proper names of code elements (method names, identifiers, etc.), or languages and frameworks. For the identification of patterns from commit messages, the authors kept any keyword that can be either tightly or loosely coupled to refactoring. Such decision mitigates the selection bias, at the expenses of reporting keywords that may or may not be relevant to refactor- ing documentation. During the last week, two authors have finished analyzing the remaining commit messages. This step resulted in analyzing 59,745 com- mit messages. Then, we iterated over the set again while excluding the terms identified in our previous work, to identify additional self-affirmed refactoring


3https://javaparser.org/




patterns. We manually read through 21,193 commit messages. Our in-depth inspection resulted in a list of 513 potential self-affirmed refactoring candidates, identified across the considered projects, as illustrated later in Tables 8 and 9.

4.6. Phase 6: Manual Analysis
   To get a more qualitative sense of the classification results, we created five case studies that demonstrate GitHub developers' intentions when refactoring source code. Case study is one of the empirical methods used for studying phe- nomena in a real-life context (Wohlin et al., 2012). In our study, we performed a combination of manual analysis and quantitative analysis using custom-built scripts. For each case study, we provide the commit message and its corres- ponding refactoring operations detected by Refactoring Miner. We elaborate in detail these case studies in Section 5.2, where we report on our results.

5. Experimental Results
   This section reports and discusses our experimental results and aims to an- swer the research questions in Section 1.
   Replication package. We provide our comprehensive experiments package available in (AlOmar, 2020 (last accessed October 20, 2020) to further replicate and extend our study. The package includes the selected Java projects, the detailed refactoring and non-refactoring commits and documentation, manual commits classification, the automatic commits classification, and the JUnit file detection.

5.1. RQ1: To what purposes developers refactor their code?
   To answer this research question, we present the refactoring commit mes- sages classification results explained in Subsection 4.3. This section details the classification of 111,884 commit messages containing 711,495 refactoring oper- ations. The complete set of scores for all the classifiers including the Precision, Recall, and F-measure scores per class for each machine learning classifier is provided in Table 5. The best performing model was used to classify the test dataset. Based on our findings, we observed that Random Forest achieved the best F1 score: 87% which is higher than its competitors. Random Forest belongs to the family of ensemble learning machines, and has typically yielded superior predictive performance mainly due to the fact that it aggregates several learners. Hence, we utilized this machine learning algorithm (and its optimal set of hyperparameters) as the optimum model for our study. In order to compare classification algorithms performance, we use the McNemar test (Dietterich, 1998). We compare the performance of Random Forest against the other five classifiers. As shown in Table 6, the McNemar's test results show that there are statistically significant differences in the performance of the classifiers except for the classifier Support Vector Classification in which the difference is not statistically significant.



Table 5: Detailed classification metrics (Precision, Recall, and F-measure) of each classifier.

Random Forest
Support Vector Classification
Decision Tree
Category
Precision
Recall
F1
Category
Precision
Recall
F1
Category
Precision
Recall
F1
Bug Fix
0.83
0.79
0.81
Bug Fix
0.75
0.78
0.77
Bug Fix
0.77
0.80
0.78
Code Smell
0.93
0.95
0.94
Code Smell
0.93
0.94
0.93
Code Smell
0.89
0.91
0.90
External QA
0.85
0.91
0.88
External QA
0.92
0.89
0.90
External QA
0.77
0.90
0.83
Functional
0.81
0.91
0.86
Functional
0.77
0.88
0.82
Functional
0.92
0.83
0.87
Internal QA
0.95
0.81
0.87
Internal QA
0.95
0.84
0.89
Internal QA
0.91
0.80
0.85
Average F1
0.87
0.87
0.87
Average F1
0.87
0.86
0.86
Average F1
0.85
0.85
0.85

Logistic Regression
Multinomial Naive Bayes
K-Nearest Neighbors
Category
Precision
Recall
F1
Category
Precision
Recall
F1
Category
Precision
Recall
F1
Bug Fix
0.66
0.70
0.68
Bug Fix
0.63
0.77
0.69
Bug Fix
0.62
0.71
0.66
Code Smell
0.89
0.94
0.91
Code Smell
0.82
0.94
0.87
Code Smell
0.76
0.93
0.84
External QA
0.88
0.88
0.88
External QA
0.97
0.71
0.82
External QA
0.85
0.75
0.79
Functional
0.77
0.87
0.82
Functional
0.66
0.83
0.74
Functional
0.68
0.73
0.71
Internal QA
0.96
0.78
0.86
Internal QA
0.99
0.67
0.80
Internal QA
0.97
0.71
0.82
Average F1
0.83
0.83
0.83
Average F1
0.81
0.78
0.78
Average F1
0.78
0.77
0.76

Table 6: McNemar's test results.

Classifier	p-value
Support Vector Classification	0.1
Decision Tree	0.04
Logistic Regression	0.02
Multinomial Naive Bayes	0.01
K-Nearest Neighbors	0.01

   Figure 3 shows the categorization of commits, from all projects combined. We observe that all of the categories had almost a uniform distribution of refact- oring classes with low variability. For instance, Bug Fix, Functional, Internal Quality Attribute, External Quality Attribute, and Code Smell Resolution had commit message distribution percentages of 24.3%, 22.3%, 20.1%, 17.5%, and
15.9%, respectively.
   The first observation that we can draw from these findings is that developers do not solely refactor their code to fix code smells. They instead refactor the code for multiple purposes. Our manual analysis show that developers tend to make design-improvement decisions that include re-modularizing packages by moving classes, reducing class-level coupling, increasing cohesion by moving methods, and renaming elements to increase naming quality in the refactored design. Developers also tend to split classes and extract methods for: 1) sep- aration of concerns, 2) helping in easily adding new features, 3) reducing bug propagation, and 4) improving the system's non-functional attributes such as extensibility and maintainability
   Figure 4 depicts the distribution of refactoring commits for all production and test files for each refactoring motivation. As can be seen, developers tend to refactor these two types of source files for several refactoring intentions, and they care about refactoring the logic of the application and refactoring the test code that verifies if the application works as expected. Although developers usually handle production and test code differently, the similarity of the patterns shows that they refactor these source files for the same reasons with unnoticeable






Figure 3: Percentage of classified commits per category in all projects combined.


differences.
   Production code. Concerning refactorings applied in the production files, developers perform refactoring for several motivations. For the Bug Fix cat- egory, an interpretation for this comes from the nature of the debugging process that includes the disambiguation of identifier naming that may not reflect the appropriate code semantics or that may be infected with lexicon bad smells (i.e., linguistic anti-patterns (Abebe et al., 2011; Arnaoudova et al., 2013)). Another debugging practice would be the separation of concerns, which helps in reducing the core complexity of a larger module and reduces its proneness to errors (Tsantalis & Chatzigeorgiou, 2011). Regarding the Internal Qual- ity Attributes category, developers move code elements for design-level changes (Stroggylos & Spinellis, 2007; Alshayeb, 2009; Bavota et al., 2015; Mkaouer et al., 2015), e.g., developers tend to re-modularize classes to make packages more cohesive, and extract methods to reduce coupling between classes. As for the External Quality Attributes category, developers often optimize the code to improve the non-functional quality attributes such as readability, understandab- ility, and maintainability of the production files. For the Code Smell Resolution category, developers eliminate any bad practices and adhere to object-oriented design principles. Finally, for the Functional category, developers implement a new feature or modify the existing ones.
   Test code. With regards to test files, developers perform refactoring to improve the design of the code. An example can be shown by renaming a given code element such as a class, a package or an attribute. Finding better names for code identifiers serves the purpose of increasing the software's comprehens- ibility. Developers explicitly mention the use of the renaming operations for the purpose of disambiguation the redundancy of methods names and enhancing their usability. Another activity to refactor test files could be moving methods,




	


(a) Production file


(b) Test file


Figure 4: Percentage of classified commits per category in production and test files.


or pushing code elements across hierarchies, e.g., pushing up attributes. Each of these activities are performed to support several refactoring motivations.


5.2. Case Studies
   This subsection reveals more details with respect to our classified commits. As we validate our classification results, we have selected an example from each category. For each example, we checkout the corresponding commit to obtain the source code, then two authors manually analyze the code changes. The purpose is not to verify the consistency between the commit message and its corresponding changes, but to capture the context in which refactorings were applied. In each analyzed commit, we report its class, its message, the distri-



bution of its corresponding refactoring, along with our understanding of their usage context.

5.2.1. Case Study 1. Refactoring to improve internal quality attributes

Figure 5: Commit message stating the restructuring of code to improve its structure.






Move & Rename Class Move Source Folder Change Package
Move Class





Figure 6: Distribution of refactoring operations.


   This case study aims to demonstrate one of the five refactoring motivations reported in this study. The commit message mainly discussed two refactoring practices: (1) performing large refactorings, and (2) optimizing the structure of the codebase. It is apparent from this commit that the main intention behind refactoring the code is to improve the design. Specifically, Refactoring Miner detected 69 refactoring operations associated with this commit message. We observe there is consistency between what is documented in the commit message and the actual size of refactoring operations.
   Closer inspection of the nature and type of the 69 refactorings and the cor- responding source code shows that the GitHub commit author massively op-



timized the package structure within existing modularizations. Particularly, as Figure 6 shows, the developer performed four refactoring types, namely, Move Class, Change Package, Move Source Folder, and Move and Rename Class. A percentage of 80.16% of these refactorings were Move Class refactorings, 8.70% were Change Package, 7.25% were Move Source Folder, and 2.90% were com- posite refactorings (Move and Rename Class). As pointed out in Refactoring Miner documentation4, Change Package refactoring involves several package- level refactorings (i.e., Rename, Move, Split, and Merge packages).
   We observe that the developer is optimizing the design by performing repack- aging, i.e., extracting packages and moving the classes between these packages, merging packages that have classes strongly related to each other, and renaming packages to reflect the actual behavior of the package. The present observations are significant in at least two major respects: (1) improving the quality of pack- ages structure when optimizing intra-package (i.e., cohesion) and inter-package (i.e., coupling) dependencies and minimizing package cycles and (2) avoiding in- creasing the size of the large packages and/or merging packages into larger ones. Developer intention to distribute classes over packages, however, might depend on other design factors than package cohesion and coupling. This remodular- ization activity helps to identify packages containing classes poorly related to each other.
   In order to confirm the main refactoring intention when performing this refactoring, we emailed the GitHub contributor and asked about the main mo- tivation behind performing this massive refactorings in the commit message (Figure 5). The GitHub contributor confirmed that the intention was to im- prove the design and this motivation is best illustrated in the following response about the commit we examined:
"there are a few reasons for large refactorings: (1) the codebase is becoming increasingly difficult to evolve. Sometimes relatively small conceptual changes can make a huge difference, but requires a lot of changes in many places." and "(2) analysis of codebase dependencies, call sequences and so on reveal that the codebase is a mess and needs to be fixed to avoid current or future bugs."
   The most striking observation to emerge from the response was that as a program evolves in size it is vital to design it by splitting it into modules, so that developer does not need to understand all of it to make a small modification. Generally, refactoring to improve the design at different levels of granularity is crucial. This case study sheds light on the importance of refactoring at package-level of granularity and how it plays a crucial role in the quality and maintainability of the software. In future investigations, it might be possible to extend this work by learning from existing remodularization process and then recommending the right package for a given class taking into account the design quality (e.g., coupling, cohesion, and complexity). Future studies on


4https://github.com/tsantalis/RefactoringMiner




remodularization topic can develop refactoring tool which can refactor software systems at different levels of granularity.
5.2.2. Case Study 2. Refactoring to remove code smells


Figure 7: Commit message stating the removal of duplicate code.



Change Package Extract Method Extract Variable Rename Variable Move Class
Move & Rename Class Extract Class
Move Method Move Attribute Rename Method


Figure 8: Distribution of refactoring operations.


   This case study illustrates another developer's perception of refactoring which is mainly about code smell resolution. Figure 7 shows that the developer performed large-scale refactoring to eliminate duplicated code. Generally, code duplication belongs to the "Dispensable" code smell category, i.e., code frag- ments that are unneeded and whose absence would make the code cleaner and more efficient.
   Figure 8 depicts the 38 refactoring operations performed in which the de- veloper removed duplicated code. The developer performed 10 different types of refactorings associated with the commit message shown in Figure 7: Rename



Method, Move Attribute, Move Method, Extract Class, Move and Rename Class, Move Class, Rename Variable, Extract Variable, Extract Method, and Change Package.
   From the pie chart, it is clear that the majority of refactorings performed were Rename Method and Move Attribute with 34.21% and 21.05% respect- ively, followed by Move Method with 13.16% and Extract Class refactorings with 10.54%. Nearly 5% were Move Class and Move and Rename Class refact- orings and only a small percentage of refactoring commits were Change Package, Extract Method, Extract Variable, and Rename Variable.
   On further examination of the source code and the corresponding refactor- ings detected by the tool, we notice that there are a variety of cases in which the code fragments are considered duplicate. One case is when the same code structure is found in more than one place in the same class, and the other one is when the same code expression is written in two different and unrelated classes. The developer treated the former case by using Extract Method refactoring fol- lowed by the necessarily naming and moving operations and then invoked the code from both places. As for the latter case, the developer solved it by using Extract Class refactoring and the corresponding renaming and moving opera- tions for the class and/or attribute that maintained the common functionalities. The developer also performed Change Package refactorings when removing code duplication as a complementary step of refactoring, which could indicate mo- tivations outside of those described in the commit message.
   It appears to us that composite refactorings have been performed for resolv- ing this code smell. These activities help eliminate code duplication since mer- ging duplicate code simplifies the design of the code. Additionally, these activ- ities could help improving many code metrics such as the lines of code (LOC), the cyclomatic complexity (CC), and coupling between objects (CBO).

5.2.3. Case Study 3. Refactoring to improve external quality Attributes

Figure 9: Commit message stating the refactoring to improve code readability.








Parameterize Variable Rename Variable Extract Method Rename Parameter





Figure 10: Distribution of refactoring operations.



   This case study demonstrates another refactoring intention which is related to improving external quality attributes (i.e., indication of the enhancement of non-functional attributes such as readability and understandability of the source code). As shown in Figure 9, the developer stated that the purpose of performing this refactoring is to improve the readability of the source code by breaking large blocks of code into separate methods.
   Figure 10 illustrates the breakdown of 16 refactoring operations related to readability associated with this commit message. It can be seen that Rename Parameter and Extract Method refactorings have the highest refactoring-related commits with 37.50% and 31.25%, respectively. Rename Variable is the third most performed refactoring with 25%, in front of Parameterize Variable refact- orings at 6.25%. By analyzing the corresponding source code, it is clear that developer decomposed four methods for better readability, namely, handleIO(),
handleIO(sk SelectionKey), handleReads(sk SelectionKey, qa MemcachedNode), and attemptReconnects(). The name could also change for a reason (e.g., when
Extract Method is applied to a method, the method name and its parameters or variables also update as a result).
   To improve code readability, developer used Extract Method refactorings as a treatment for this case study to reduce the length of the method body. Additionally, renaming operations were used to improve naming quality in the refactored code and reflect the actual purpose of the parameters and variables. Converting variables to parameters could also make the methods more readable and understandable. To develop a full picture of how to create readable code, future studies will be needed to focus on code readability guidelines or rules for developers.



5.2.4. Case Study 4. Refactoring to add feature

Figure 11: Commit message stating the addition of a new functionality.





Move Method
Move & Rename Class Rename Method
Move Class Rename Attribute Rename Variable




Figure 12: Distribution of refactoring operations.



   This case study discusses another motivation of refactoring that is different than the traditional design improvement motivation. As shown in Figure 11, de- velopers interleaved refactoring practices with other development-related tasks, i.e., adding feature. Specifically, the developer implemented two new function- alities (i.e., allow the user to download files, and "check for updates" and "pref- erence option" features. Developers also performed other code changes which involved renaming, moving, etc.
   Figure 12 portrays the 21 refactoring operations performed in which the de- veloper added features and made other related code changes. With regards to



the type of refactoring operations used to perform these implementations, the developer mainly performed moving and renaming related operations that are associated with code elements related to that implementation. Overall, Rename Variable and Rename Attribute constitute the main refactoring operations per- formed accounting for 38.10% and 28.57% respectively, followed by Move Class with 14.29% and Rename Method with 9.52% . The percentage of Move Method and Move and Rename Class refactorings, by contrast, made up a mere 4.76%. Upon exploring the source code, it appears to us that the developer per- formed moving-related refactorings when adding features (e.g., update checker functionality and activate user preference option) to the system, and renaming- related operations have been performed for several enhancements related to the UI (e.g., renaming buttons, task bar, progress bar, etc). These observations may explain that adding feature is one type of development task that refactorings were interleaved with and the refactoring definition in practice seems to deviate from the rigorous academic definition of refactoring, i.e., refactoring to improve
the design of the code.
5.2.5. Case Study 5. Refactoring to fix bug

Figure 13: Commit message stating the correction of user interface related bugs.



Extract Class Rename Method Rename Variable Extract Method Move Attribute Rename Attribute Rename Parameter Move Method



Figure 14: Distribution of refactoring operations.



   This case study presents another refactoring intention, i.e., refactoring to fix bugs that differs from the academic definition of refactoring. It can be seen from the above commit message (Figure 13) that several UI-related bugs have been solved while performing refactorings. Similar to the commit in case study 4, the developer interleaved these changes with other types of refactoring.
   The pie chart above shows 7 distinct refactoring operations performed that constituted 37 refactoring instances for bug fixing-related process. The type of refactorings involved in this activity are mainly focused on extracting, moving, and renaming-related operations.
   From the graph above we can see that roughly a quarter of refactorings were Move Method. Rename Parameter, Rename Attribute, and Move Attribute constituted almost the same percentage with slight advantage to Rename Para- meter. Extract Method was comprised of 13,51%, whereas Rename Variable, Rename Method, and Extract Class combined just constituted under a fifth. The present results are significant in at least two major respects: (1) developers flossly refactor the code to reach a specific goal, i.e., fix bugs, and (2) developers did not separate refactoring techniques from bug fixing-related activities. Inter- leaving these activities may not guarantee behavior preserving transformation as reported by (Fowler et al., 1999). Developers are encouraged to frequently refactor the code to make finding and debugging bugs much easier. Fowler et al. pointed out that developers should stop refactoring if they notice a bug that needs to be fixed since mixing both tasks may lead to changing the behavior of the system. Testing the impact of these changes is a topic beyond the scope of this paper, but it is an interesting research direction that we can take into account in the future.

   Analyzing the distributions of refactoring operations in the case studies, and observing how they vary due to the context of refactoring and due to the differ- ence between production and test files, has raised our curiosity about whether we can observe similar difference if we analyze distributions of refactorings across classification categories. In the next subsection, we define the following research question to investigate the frequency of refactorings, spit by target refactored element (production vs. test) per category.

5.3. RQ1.1: Do software developers perform different types of refactoring oper- ations on test code and production code between categories?
   In Table 7, we show the volume of operations for each refactoring operation applied to the refactored test and production files grouped by the classification category associated with the file. Values in bold indicate the most common ap- plied refactoring operation - Move Class and Rename Parameter for production files, and Rename Method for test files.
   Concerning production file-related refactoring motivations, the topmost re- factoring operations performed across all refactoring motivations is Move Class refactoring, except for Bug Fix in which Rename Attribute is the highest per- formed refactoring. In the case of internal quality attribute-related motivations,












Prod.

Change Package
4626 (0.46%)
0 (0.0%)
12693 (0.56%)
0 (0.0%)
3760 (0.57%)
0 (0.0%)
5120 (0.50%)
0 (0.0%)
3541 (0.29%)
0 (0.0%)
Extract & Move Method
20822 (2.08%)
32 (2.50%)
8643 (0.38%)
30 (2.74%)
8369 (1.27%)
35 (2.22%)
27327 (2.67%)
111 (7.86%)
24576 (2.08%)
51 (2.84%)
Extract Class
13156 (1.31%)
13 (1.01%)
6785 (0.30%)
18 (1.64%)
6019 (0.91%)
68 (4.33%)
23625 (2.31%)
13 (0.92%)
19510 (1.65%)
21 (1.16%)
Extract Interface
1700 (0.17%)
0 (0.0%)
2522 (0.11%)
0 (0.0%)
1874 (0.28%)
0 (0.0%)
2104 (0.20%)
0 (0.0%)
3658 (0.30%)
2 (0.11%)
Extract Method
31357 (3.13%)
246 (19.19%)
18177 (0.80%)
52 (4.75%)
22772 (3.47%)
148 (9.42%)
40286 (3.94%)
218 (15.43%)
113416 (9.60%)
287 (15.98%)
Extract Subclass
1578 (0.16%)
0 (0.00%)
1109 (0.05%)
1 (0.09%)
1084 (0.16%)
0 (0.0%)
1171 (0.11%)
0 (0.0%)
2391 (0.20%)
9 (0.50%)
Extract Superclass
7262 (0.72%)
8 (0.62%)
7380 (0.32%)
4 (0.36%)
6917 (1.05%)
11 (0.70%)
10290 (1.00%)
13 (0.92%)
14939 (1.26%)
0 (0.0%)
Extract Variable
9922 (0.99%)
30 (2.34%)
8893 (0.39%)
40 (3.65%)
8233 (1.25%)
74 (4.71%)
15694 (1.53%)
43 (3.04%)
59323 (5.02%)
71 (3.95%)
Inline Method
6139 (0.61%)
22 (1.72%)
5100 (0.22%)
19 (1.73%)
3717 (0.56%)
14 (0.89%)
7219 (0.70%)
11 (0.77%)
12662 (1.07%)
9 (0.50%)
Inline Variable
3107 (0.31%)
4 (0.31%)
4290 (0.19%)
3 (0.27%)
1473 (0.22%)
12 (0.76%)
2443 (0.23%)
13 (0.92%)
10570 (0.89%)
10 (0.55%)
Move & Rename Attribute
171 (0.02%)
1 (0.08%)
60 (0.0%)
0 (0.0%)
318 (0.04%)
0 (0.0%)
204 (0.01%)
0 (0.0%)
120 (0.01%)
0 (0.0%)
Move & Rename Class
14426 (1.44%)
4 (0.31%)
12493 (0.55%)
14 (1.27%)
14958 (2.27%)
5 (0.31%)
16054 (1.57%)
7 (0.49%)
11192 (0.94%)
0 (0.0%)
Move Attribute
112542 (11.23%)
51 (3.98%)
43865 (1.92%)
61 (5.57%)
22716 (3.46%)
16 (1.01%)
45025 (4.41%)
33 (2.33%)
66400 (5.62%)
54 (3.00%)
Move Class
213609 (21.32%)
26 (2.03%)
1202376 (52.73%)
42 (3.83%)
129816 (19.78%)
32 (2.03%)
175675 (17.21%)
12 (0.84%)
94787 (8.02%)
41 (2.28%)
Move Method
61349 (6.12%)
120 (9.36%)
47268 (2.07%)
202 (18.46%)
21830 (3.32%)
168 (10.70%)
101096 (9.90%)
185 (13.10%)
87099 (7.37%)
89 (4.95%)
Move Source Folder
6219 (0.62%)
0 (0.0%)
4636 (0.20%)
0 (0.0%)
8087 (1.23%)
0 (0.0%)
9293 (0.91%)
0 (0.0%)
5906 (0.50%)
0 (0.0%)
Parameterize Variable
2595 (0.26%)
12 (0.94%)
1623 (0.07%)
2 (0.18%)
1572 (0.23%)
26 (1.65%)
3548 (0.34%)
6 (0.42%)
5474 (0.46%)
10 (0.55%)
Pull Up Attribute
8803 (0.88%)
52 (4.06%)
53171 (2.33%)
8 (0.73%)
8781 (1.33%)
40 (2.54%)
15023 (0.34%)
26 (1.84%)
29810 (2.52%)
69 (3.84%)
Pull Up Method
71906 (7.18%)
133 (10.37%)
26439 (1.17%)
39 (3.56%)
24539 (3.74%)
55 (3.50%)
31181 (1.47%)
39 (2.76%)
81997 (6.94%)
204 (11.36%)
Push Down Attribute
5186 (0.52%)
0 (0.0%)
5688 (0.25%)
5 (0.45%)
6476 (0.98%)
2 (0.12%)
4167 (3.05%)
0 (0.0%)
6778 (0.57%)
0 (0.0%)
Push Down Method
14215 (1.42%)
1 (0.08%)
11874 (0.52%)
8 (0.73%)
14689 (2.23%)
1 (0.06%)
9222 (0.90%)
(0.0%)
14581 (1.23%)
0 (0.0%)
Rename Attribute
68893 (6.88%)
43 (3.35%)
229670 (10.07%)
20 (1.82%)
112477 (17.14%)
67 (4.26%)
142835 (14.00%)
26 (1.84%)
109286 (9.25%)
29 (1.61%)
Rename Class
28254 (2.82%)
16 (1.25%)
56894 (2.50%)
9 (0.82%)
24241 (3.69%)
15 (0.95%)
27010 (2.64%)
18 (1.27%)
37555 (3.17%)
10 (0.55%)
Rename Method
90809 (9.06%)
314 (24.49%)
393385 (17.25%)
393 (35.92%)
97245 (14.82%)
461 (29.36%)
120188 (11.77%)
371 (26.27%)
125897 (10.65%)
532 (29.63%)
Rename Parameter
89514 (8.93%)
12 (0.94%)
41900 (1.84%)
16 (1.46%)
41483 (6.32%)
17 (1.08%)
72275 (7.08%)
14 (0.99%)
138436 (11.72%)
16 (0.89%)
Rename Variable
104621 (10.44%)
127 (9.91%)
70507 (3.09%)
100 (9.14%)
60788 (9.26%)
286 (18.21%)
108056 (10.59%)
211 (14.94%)
95289 (8.06%)
249 (13.87%)
Replace Attribute
356 (0.04%)
5 (0.39%)
207 (0.01%)
0 (0.0%)
32 (0.004%)
0 (0.0%)
212 (0.02%)
0 (0.0%)
102 (0.008%)
1 (0.05%)
Replace Variable with Attribute
8703 (0.87%)
10 (0.78%)
2546 (0.11%)
8 (0.73%)
1813 (0.27%)
17 (1.08%)
3931 (0.38%)
42 (2.97%)
5889 (0.49%)
31 (1.72%)



developers performed Move Class refactoring to move the relevant classes to the right package if there are many dependencies for the class between two packages. This could eliminate undesired dependencies between modules. Another pos- sibility for the reason to perform such refactoring is to introduce a sub-package and move a group of related classes to a new subpackage. With respect to code smell resolution motivation, developers eliminate a redundant sub-package and nesting level in the package structure when performing Move Class refactoring operations. With regards to external quality attribute-related motivation, de- velopers can target improving the understandability of the code by repackaging and moving the classes between these packages. Hence, the structure of the code becomes more understandable. Developers could also maintain code com- patibility by moving a class back to its original package to maintain backward compatibility. For feature addition or modification, Move Class refactoring is performed when adding new or modifying the implemented features. This could be done by moving the class to appropriate containers or moving a class to a package that is more functionally or conceptually relevant. Lastly, for bug fixing- related motivations, developers mainly improve parameter and method names; they rename a parameter or method to better represent its purpose and to en- force naming consistency and to conform to the project's naming conventions. Developers need to change the semantics of the code to improve the readability of the code. For test files-related refactoring motivations, the most frequently applied refactoring is Rename Method. This can be explained by the fact that test methods are the fundamental elements in a test suite. Test methods are utilized to test the production source code; hence, the high occurrence of method based refactorings in unit test files. The observed difference in the distribution of refactorings in production/test files between our study and the related work (Tsantalis et al., 2013) is also due to the size (number of projects) effect of the two groups under comparison.


5.4. RQ2: What patterns do developers use to describe their refactoring activ- ities?
   In this research question, we explore the set of 513 potential self-affirmed refactoring candidates, extracted by manual inspection from commits messages and categories top 100 features. We classify these SAR candidates into two tables: Table 8 contains generic candidate patterns that were found across cat- egories; Table 9 contains candidate patterns that are specific to each category.









Patterns

(1) Add*
(47) Chang*
(93) Cleaned out
(139) CleanUp
(185) CleaningUp
(2) Clean* up
(48) Clean-up
(94) Creat*
(140) Decompos*
(186) Encapsulat*
(3) Enhanc*
(49) Extend*
(95) Extract*
(141) Factor* Out
(187) Fix*
(4) Improv*
(50) Inlin*
(96) Introduc*
(142) Merg*
(188) Migrat*
(5) Modif*
(51) Modulariz*
(97) Mov*
(143) Organiz*
(189) Polish*
(6) Pull* Up
(52) PullUp
(98) Push Down
(144) PushDown
(190) Repackag*
(7) Re packag*
(53) Re-packag*
(99) Redesign*
(145) Re-design*
(191) Reduc*
(8) Refactor*
(54) Refin*
(100) Reformat*
(146) Remov*
(192) Renam*
(9) Reorder*
(55) Reorganiz*
(101) Re-organiz*
(147) Repackag*
(193) Replac*
(10) Restructur*
(56) Rework*
(102) Rewrit*
(148) Re-writ*
(194) Rewrot*
(11) Simplif*
(57) Split*
(103) TidyUp
(149) Tid*-up
(195) Tid* Up
(12) A bit of refactor*
(58) Basic code clean up
(104) Chang* code style
(150) Ease maintenance moving forward
(196) Replace it with
(13) Big refactor*
(59) Big cleanup
(105) Clean* up the code style
(151) Ease of code maintenance
(197) Extracted out code
(14) Better factored code
(60) Cleanliness
(106) Code style improv*
(152) Easier to maintain
(198) Reduced code dependency
(15) Code refactor*
(61) Clean* up unnecessary code
(107) Code style unification
(153) Simplify future maintenance
(199) Pushed down dependencies
(16) Code has been refactored extensively
(62) Cleanup formatting
(108) Fix code style
(154) Improve quality
(200) Simplify the code
(17) Extensive refactor*
(63) Code clean
(109) Improv* code style
(155) Improvement of code quality
(201) Less code
(18) Refactoring towards nicer name analysis
(64) Code cleanup
(110) Minor adjustments to code style
(156) Improved style and code quality
(202) Change package
(19) Heavily refactored code
(65) Code cleanliness
(111) Modifications to code style
(157) Maintain quality
(203) Cosmetic changes
(20) Heavy refactor*
(66) Code clean up
(112) Lots of modifications to code style
(158) More quality cleanup
(204) Full customization
(21) Little refactor*
(67) Massive cleanup
(113) Makes the code easier to program
(159) Better name
(205) Structure change
(22) Lot of refactor*
(68) Minor cleaning of the code
(114) Code review
(160) Chang* name
(206) Module structure change
(23) Major refactor*
(69) Housekeeping
(115) Code rewrite
(161) Chang* the name
(207) Module organization structure change
(24) Massive refactor*
(70) Major rewrite and simplification
(116) Code cosmetic
(162) Chang* the package name
(208) Polishing code
(25) Huge refactor*
(71) Improv* consistency
(117) Code revision
(163) Chang* method name
(209) Improv* code quality
(26) Minor refactor*
(72) Some fix* and optimization
(118) Code optimization
(164) Chang* method parameter names for consistency
(210) Chang* package structure
(27) More refactor*
(73) Minors fix* and tweak
(119) Code reformatting
(165) Enables condensed naming
(211) Fix quality flaws
(28) Refactor* code
(74) Fix* annoying typo
(120) Code organization
(166) Fix* naming convention
(212) Get rid of
(29) Refactor* existing schema
(75) Fix* some formatting
(156) Code rearrangement
(167) Fix nam*
(213) Chang* design
(30) Refactor out
(76) Fix* formatting
(122) Code formatting
(168) Typo in method name
(214) Improv* naming consistency
(31) Small refactor*
(77) Modifications to make it work better
(123) Code polishing
(169) Maintain convention
(215) Remov* unused classes
(32) Some refactor*
(78) Make it simpler to extend
(124) Code simplification
(170) Maintain naming consistency
(216) Minor simplification
(33) Tactical refactor*
(79) Fix* Regression
(125) Code adjustment
(171) Major renam*
(217) Fix* quality issue
(34) Moved a lot of stuff
(80) Remov* the useless
(126) Code improvement
(172) Name cleanup
(218) Naming improvement
(35) Fix this tidily
(81) Remov* unneeded variables
(127) Code style
(173) Renam* for consistency
(219) Packaging improvement
(36) Further tidying
(82) Remov* unneeded code
(128) Code restructur*
(174) Renam* according to java naming conventions
(220) Structural chang*
(37) Tidied up and tweaked
(83) Remov* redundant
(129) Code beautifying
(175) Renam* classes for consistency
(221) Hierarchy clean*
(38) Tidied up some code
(84) Remov* dependency
(130) Code tidying
(176) Renam* package
(222) Hierarchy reduction
(39) Restructur* package
(85) Remov* unused dependencies
(131) Code enhancement
(177) Resolv* naming inconsistency
(223) Enhanc* architecture
(40) Restructur* code
(86) Remov* unused
(132) Code reshuffling
(178) Simpler name
(224) Architecture enhanc*
(41) Aggregat* code
(87) Remov* unnecessary else blocks
(133) Code modification
(179) Us* appropriate variable names
(225) Trim unneeded code
(42) Beautif* code
(88) Remov* needless loop
(134) Code unification
(180) Us* more consistent variable names
(226) Remov* unneeded code
(43) Tidy code
(89) Maintain consistency
(135) Code quality
(181) Neaten up
(227) More consistent formatting
(44) Beautify*
(90) Customiz*
(136) Make code clearer
(182) Moved more code out of
(228) More easily extended
(45) Moved all integration code to separate package
(91) Improve code clarity
(137) Code clarity
(183) Fix bad merge
(229) Makes it more extensible-friendly
(46) Improve code
(92) Simplify code
(138) Clean* code
(184) Cleanup code
(230) Clean* up code






BugFix
Code Smell
External
Functional
Internal
Minor fixes
Avoid code duplication
Reusable structure
Add* feature
Decoupling
Bug* fix*
Avoid duplicate code
Improv* code reuse
Add new feature
Enhance loose coupling
Fix* bug*
Avoid redundant method
Add* flexibility
Added a bunch of features
Reduced coupling
Bug hunting
Code duplication removed
Increased flexibility
New module
Reduce coupling and scope of responsibility
Correction of bug
Delet* duplicate code
More flexibility
Fix some GUI
Prevent the tight coupling
Improv* error handling
Remove unnecessary else blocks
Provide flexibility
Added interesting feature
Reduced the code size
Fix further thread safety issues
Eliminate duplicate code
A bit more readable
Added more features
Complexity has been reduced
Fixed major bug
Fix for duplicate method
Better readability
Adding features to support
Reduce complexity
Fix numerous bug
Filter duplicate
Better readability and testability
Adding new features
Reduced greatly the complexity
Fix several bug
Joining duplicate code
Code readability optimization
Addition of a new feature
Removed unneeded complexity
Fixed a minor bug
Reduce a ton of code duplication
Easier readability
Feature added
Removes much of the complexity
Fixed a tricky bug
Reduce code duplication
Improve readability
Implement one of the batch features
Add inheritance
Fix* small bug
Reduced code repetition
Increase readability
Implementation of feature
Added support to the inheritance
Fixed nasty bug
Refactored duplicate code
Make it better readable
Implemented the experimental feature
Avoid using inheritance and using composition instead
Fix* some bug*
Clear up a small design flaw
Make it more readable
Introduced possibility to erase features
Better support for specification inheritance
Fixed some minor bugs
TemporalField has been refactored
Readability enhancement
New feature
Change* inheritance
bugfix*
Remove commented out code
Readability and supportability improvement
Remove the default feature
Extend the generated classes using inheritance
Fix* typo*
Removed a lot of code duplication
Readability improvements
Removed incomplete features
Improved support for inheritance
Fix* broken
Remov* code duplication
Reformatted for readability
Renamed many features
Perform deep inheritance
Fix* incorrect
Remove some code duplication
Simplify readability
Renamed some of the features for consistency
Remove inheritance
Fix* issue*
Removed the big code duplication
Improve* testability
Small feature addition
Inheritance management
Fix* several issue*
Removed some dead and duplicate code
Update the performance
Support of optional feature
Loosened the module dependency
Fix* concurrency issue* with
Remov* duplicate code
Add* performance
Supporting for derived features
Prevents circular inheritance
Fixes several issues
Resolved duplicate code
Scalability improvement
Added functionality
Avoid using inheritance
Solved some minor bugs
Sort out horrendous code duplication
Better performance
Added functionality for merge
Add composition
Working on a bug
Remove duplicated field
Huge performance improvement
Adding new functionality
Composition better than inheritance
Get rid of
Remov* dead code
Improv* performance
Adds two new pieces of functionality to
Us* composition
A bit of a simple solution to the issue
Remove some dead-code
More manageable
Consolidate common functionality
Separates concerns
A fix to the issue
Removed all dead code
More efficient*
Development of this functionality
Better handling of polymorphism
Fix a couple of issue
Removed apparently dead code
Make it reusable for other
Export functionality
Makes polymorphism easier
Issue management
This is a bit of dead code
Increase efficiency
Extend functionality of
Better encapsulation
Fix* minor issue
Removed more dead code
Verify correctness
Extract common functionality
Better encapsulation and less dependencies
Correct issue
Fix* code smell
Massive performance improvement
Functionality added
Pushed down dependencies
Additional fixes
Fix* some code smell
Increase performance
House common functionality
Remov* dependency
Resolv* problem
Remov* some 'code smells'
Largely resolved performance issues
Improved functionality
Split out each module into
Correct* test failure*
Update data classes
Lots of performance improvement
Move functionality

Fix* all failed test*
Remove useless class
Measuring the performance
Moved shared functionality

Fix* compile failure
Removed obviously unused/faulty code
Improv* stability
Feature/code improvements

A fix for the errors
Lots of modifications to code style
Usability improvements
Pulling-up common functionality

Better error handling
Antipattern bad for performances
Noticeable performance improvement
Push more functionality

Better error message handling
Killed really old comments
Optimizing the speed
Re-implemented missing functions

Cleanup error message
Less long methods
Performance boost
Refactored functionality

Error fix*
Removed some unnecessary fields
Performance enhancement
Refactoring existing functionality

Fixed wrong

Performance improvement
Add functionality to

Fix* error*

Performance much improved
Remov* function*

Fix* some error*

Performance optimization
Merging its functionality with

Fix small error

Performance speed-up
Remove* unnecessary function*

Fix some errors

Refactor performance test
Reworked functionality

Fix compile error

Renamed performance test
Removing obsolete functionality

Fix test error

Speed up performance
Replicating existing functionality with

Fixed more compilation errors

Backward compatible with
Split out the GUI function

Fixed some compile errors

Fix backward compatibility
Add cosmetic changes

Fixes all build errors

Fixing migration compatibility
Add* support

Fixed Failing tests

Fully compatible with
Implement* new architecture

Handle

Keep backwards compatible
Update

Handling error*

Maintain compatibility
Additional changes for

Error* fix*

Make it compatible with
UI layout has changed

Tweaking error handling

More compatible
GUI: Small changes

Various fix*

Should be backward-compatible
New UI layout

Fix* problem*

Retains backward compatibility
UI changes

Got rid of deprecated code

Stay compatible with
UI enhancements

Delet* deprecated code

Added some robustness


Remov* deprecated code

Improve robustness




Improve usability




Robustness improvement




To be more robust




Better understanding




Bit to be easier to understand




Easier to understand




Increases understandability




Adding checks on accessibility




Easier accessible and evolvable




Make class more extendable




Allow extensibility




For future extensibility




Improved modularity and extensibility




Increase testability




More testable design


	Accuracy improvement	



   Upon a closer inspection of these refactoring patterns, we have made several observations: we noticed that developers document refactoring activities at dif- ferent levels of granularity, e.g., package, class, and method level. Furthermore, we observe that developers state the motivation behind refactoring, and some of these patterns are not restricted only to fixing code smells, as in the original definition of refactoring in Fowler's book, i.e., improving the structure of the code. For instance, developers tend often to improve certain non-functional at- tributes such as the readability and testability of the source code. Additionally, developers occasionally apply the "Don't Repeat Yourself" principle by removing excessive code duplication. A few patterns indicated that developers refactor the code to improve internal quality attributes such as inheritance, polymorph- ism, and abstraction. We also noticed the application of a single responsibility principle which is meant to improve the cohesion and coupling of the class when developers explicitly mentioned a few patterns related to dependency removal. Further, we observe that developers tend to report the executed refactoring operations by explicitly using terms from Fowler's taxonomy; terms such as inline class/method, Extract Class/Superclass/Method or Push Up Field/Method
and Push Down Field/Method.
   The generic nature of some of these patterns was a critical observation that we encountered, i.e., many of these patterns are context specific and can be sub- ject to many interpretations, depending on the meaning the developer is trying to convey. For instance, the pattern fixed a problem is descriptive of any an- omaly developer encountered and it can be either functional or non-functional. Since in our study, we are interested in textual patterns related to refactoring, we decided to filter this list down by reporting patterns whose frequency in commit messages containing refactoring is significantly higher than in messages of commits without refactoring. The rationale behind this idea is to identify patterns that are repeatedly used in the context of refactoring, and less often in other contexts. Since the patterns were extracted from 111,884 messages of commits containing refactoring (we call them refactoring commits), we need to build another corpus of messages from commits that do not contain refactorings (we call them non-refactoring commits). As we plan on comparing the frequency of keywords between the two corpora, i.e., refactoring and non-refactoring com- mit messages, it is important to adequately choose the non-refactoring messages to ensure fairness. To do so, we follow the following heuristics: we randomly select a statistically significant sample of commits (confidence level of 95%), 1) chosen from the same set of 800 projects that issued the refactoring commits; 2) whose authors are from the same authors of the refactoring commits; 3) whose timestamps are in the same interval of refactoring commits timestamps; 4) and finally, the average length of commit messages is approximately close (118 for refactoring commits, and 120 for non-refactoring commits).
   Once the set of non-refactoring commit messages constructed, for each keyword, we calculate its occurrence per project for both corpora. This generates vector of 800 occurrences per corpus. Each vector dimension contains a positive number representing the keyword occurrence for a project, and zero otherwise. Figure 15 illustrates occurrences violin plots of the keyword "refactor" in both corpora.






Figure 15: Violin plots representing the occurrence of refactor keyword in (A) non-refactoring corpus vs. (B) refactoring corpus.


While it is observed in Figure 15 that the occurrence of refactor in refactoring commits is higher, we need a statistical test to prove it. So, we perform such comparison using the Mann-Whitney U test, a non-parametric test that checks continuous or ordinal data for a significant difference between two independent groups. This applies to our case, since the commits, in the first group, are inde- pendent of commits in the second group. We formulate the comparison of each keyword occurrence corpora by defining the alternative hypothesis as follows:
Hypothesis 1. The occurrence vector of refactoring commits is strictly higher than the occurrence vector of non-refactoring commits.
And so, the null hypothesis is defined as follows:
Null Hypothesis 1. The occurrence vector of non-refactoring commits is equal or smaller than the occurrence vector of refactoring commits.
   We start with generating occurrence vectors for each keyword, then we per- form the statistical test for each pair of vectors. We report our findings in Table 8 and 9 where keywords in bold are the ones rejecting the null hypothesis (i.e., p< 0.05).
With the analysis of these tables results, we observe the following:
• While previous studies have been relying on the detection of refactoring activity in software artifacts using the keyword "refactor* " (Stroggylos & Spinellis, 2007; Ratzinger et al., 2008; Ratzinger, 2007; Murphy-Hill et al., 2012; Kim et al., 2014), our findings demonstrate that developers use a variety of keywords to describe their refactoring activities. For instance, keywords such as clean up, repackage, restructure, re-design, and modu- larize has been used without the mention of the refactoring keyword, to imply the existence of refactorings in the committed code. While these keywords are not exclusive to refactoring, and could also be used for gen- eral usage, their existence in commits containing refactoring operations has been more significant (i.e., p< 0.05), which qualifies them to be close synonyms to refactoring. Table 10 enumerates the top-20 keywords, sorted by the percentage of projects they were located in.



• The keyword refactor was also used in non-refactoring commit messages. This can be explained either by its occasional misuse, like some previ- ous studies found, or by the existence of refactoring operations that were not identified by the tool we are using. Yet, the frequency of misuse of this popular pattern remains significant in the refactoring-related commit messages (i.e., p< 0.05).
• We notice that developers document refactoring activities at different levels of granularity, e.g., package, class, and method level. We also ob- serve that developers occasionally state the motivation behind refactoring, which is not restricted only to fixing code smells, as in the original defin- ition of refactoring in the Fowler's book (Fowler et al., 1999), and so, this supports the rationale behind our classification in the first research question.
• Furthermore, our classification has revealed the existence of patterns that are used in specific categories (i.e., motivations). For instance, the tra- ditional code smell category is mainly populated with keywords related to removing duplicate code. Interestingly, all patterns whose existence in refactoring commit messages is statistically significant, were related to du- plicate code deletion. Although patterns related to removing code smells exist, e.g., Clear up a small design flaw or fix code smell or Antipattern bad for performances, these patterns occurrence was not large enough to reject the null hypothesis. Nevertheless, Table 11 contains a summary of category-specific patterns that we manually identified. These keywords are found relevant based on how previous studies have been identifying refact- oring opportunities (removing code smells, improving structural metrics, optimizing external quality attributes like performance etc.). Note that, in Table 11, we did not quantify the frequency of these patterns, and we plan on the future to further analyze their popularity, along with the type of refactoring operations that are mostly used with their existence, similarly to previous empirical studies (Bavota et al., 2013, 2015).
• Developers occasionally mention the refactoring operation(s) they per- form. The Mann-Whitney U test accepted the alternative hypothesis for all patterns linked to refactoring operations i.e., Pull Up, Push Down, In- line, Extract, Rename, Encapsulate, Split, Extend, except for the famous move. Unlike code smell patterns, move does exist in 787 projects (98.37%, fourth most used keyword, after respectively Fix, Add, and Merge) and it is heavily used by both refactoring and non-refactoring commit messages.
• Similarly to move, keywords like merge, reformat, remove redundant, per- formance improvement, code style, were popular across many projects, and typically invoked by both refactoring and non-refactoring commits. So, although they do serve in documenting refactoring activities, their gen- eric nature makes them also used in several other contexts. For example, merge is typically used when developers combine classes or methods, as



Table 10: Top generic refactoring patterns.

Patterns

Refactor* (89.00%)	Renam* (83.63%)	Improv* (78.75%)	CleanUp (67.38%)
Replac* (66.88%)	Introduc (53.00%)	Extend (52.63%)	Simplif (52.50%)
Extract (49.00%)	Added support (47.38%) Split (45.50%)	Reduc* (45.00%)
Chang* name (44.88%)  Migrat (32.88%)	Enhanc (32.63%)	Organiz* (32.25%)
Rework (27.25%)	Rewrit* (27.25%)	Code clean* (25.63%)  Remov* dependency (25.00%)


well as describing the resolution of merge conflicts. Similarly, perform- ance improvement is not restricted to non-functional changes, as several performance optimization techniques and genetic improvements are not necessarily linked to refactoring.

   The extraction of these patterns raised our curiosity about the extent to which they can represent an alternative to the keyword refactor, being the de facto keyword to document refactoring activities. Figure 16 reveals examples of these patterns. In the next subsection, we challenge the hypothesis raised by (Murphy-Hill et al., 2008) about whether developers use a specific pattern, i.e., "refactor" when describing their refactoring activities. We quantify the messages with the label "refactor" and without to compare between them.

5.5. RQ2.1: Do commits containing the label Refactor indicate more refactoring activity than those without the label?
   (Murphy-Hill et al., 2008) proposed several hypotheses related to four meth- ods that gather refactoring data and outlined experiments for testing those hypotheses. One of these methods concerns mining the commit log. (Murphy- Hill et al., 2008) hypothesize that commits labeled with the keyword "refactor" do not indicate more refactoring instances than unlabeled commits. In an em- pirical context, we test this hypothesis in two steps. In the first steps, we used the keyword "refactor", exactly as dictated by the authors. Thereafter, we quantified the proportion of commits including the searched label across all the



Table 11: Summary of refactoring patterns, clustered by refactoring related categories.

Internal
External
Code Smell
Inheritance
Functionality
Duplicate Code
Abstraction
Performance
Dead Code
Complexity
Compatibility
Data Class
Composition
Readability
Long Method
Coupling
Stability
Switch Statement
Encapsulation
Usability
Lazy Class
Design Size
Flexibility
Too Many Parameters
Polymorphism
Extensibility
Primitive Obsession
Cohesion
Efficiency
Feature Envy
Messaging
Accuracy
Blob Class
Concern Separation
Accessibility
Blob Operation
Dependency
Robustness
Redundancy

Testability
Useless class

Correctness
Code style

Scalability
Antipattern

Configurability
Design Flaw

Simplicity
Code Smell

Reusability
Temporary Field

Reliability
Old Comment

Modularity


Maintainability


Traceability


Interoperability


Fault-tolerance


Repeatability


Understandability


Effectiveness


Productivity


Modifiability


Reproducibility


Adaptability


Manageability




considered projects in our benchmark. In the second step, we re-tested the hy- pothesis using the subset of 230 SAR patterns, whose occurrence in refactoring commits were found to be significant with respect to non-refactoring commits. We counted the percentage of commits containing any of our SAR labels. The result of the two rounds resides in a strict set of commits containing the label "refactor", which is included in a larger set containing all patterns, and finally a remaining set of commits which does not contain any patterns. For each of the sets, we count the number of refactoring operations identified in the commits. Then, we break down the set per operation type.
In order to compare the number of refactorings identified for each set, i.e.,






Figure 16: Sample of SAR patterns identified in our study.


labeled and unlabeled commits with the keyword "refactor", along with labeled and unlabeled commits with SAR patterns. We used the Wilcoxon test, as sug- gested by (Murphy-Hill et al., 2008) for the purpose of testing the hypothesis. We then applied the non-parametric Wilcoxon rank-sum test to estimate the significance of differences between the numbers of the sets. The choice of Wil- coxon rank-sum test is motivated by the independence of sets from each other (the occurrence of refactor is independent of the occurrence of the remaining patterns).
   Figure 17 shows the distribution of refactorings in labeled and unlabeled commits with SAR patterns (group 1 on the left) and labeled and unlabeled commits with the keyword refactor (group 2 on the right). The first observa- tion we can draw is that Replace Attribute stands as most labeled refactoring with a percentage of 35.9% for group 2, while the difference between operations percentages, in group 1, is not significant, with Move Class having the highest percentage of 48.95%. Another observation is that Pull Up Attribute turns out to be the most unlabeled refactoring with a score of 54.91% for group 1, whereas Rename Attribute tends to be the most unlabeled refactoring for group 2. This result is consistent with one of the previous studies stating that renames are rarely labeled, as they detected explicit documentation of renames in less than 1% of their dataset (Arnaoudova et al., 2014).
   By comparing the different commits that are labeled and unlabeled with SAR patterns, we observe a significant number of labeled refactoring commits for each refactoring operation supported by the tool Refactoring Miner (p-value
= 0.0005). This implies that there is a strong trend of developers in using these phrases in refactoring commits. The results for commits labeled and unlabeled "refactor" with a p-value = 0.0005 engender an opposite observation, which corroborates the expected outcome of Murphy-Hill et al.'s hypothesis. Thus, the use of "refactor" is not a great indication of refactoring activities. The difference between the two tests indicates the usefulness of the list of SAR patterns that we identified.
   It is to note that we did not perform any correspondence between the men- tioned patterns and the corresponding refactoring operation(s). In other terms,













   Change Package Extract And Move Method
Extract Class
Extract Interface Extract Method Extract Subclass Extract Superclass Extract Variable Inline Method Inline Variable
Move And Rename Attribute Move And Rename Class Move Attribute
Move Class Move Method
Move Source Folder Parameterize Variable Pull Up Attribute Pull Up Method
Push Down Attribute Push Down Method Rename Attribute Rename Class Rename Method Rename Parameter Rename Variable Replace Attribute
Replace Variable With Attribute

100	80	60	40	20	0	0	20	40	60	80	100
Refactoring Percentage (%)	Refactoring Percentage (%)


Figure 17: Distribution of refactoring operations for commits labeled and unlabeled SAR (left side) and commits labeled and unlabeled refactor (right side).



if an operation is explicitly mentioned in a commit message, we have not checked whether it was among the applied refactoring at the source code level. We op- ted for such verification to be outside of the scope of the current study, while it would be an interesting direction we can consider in our future investigations.

6. Discussions and Implications
   In this section, we want to further discuss our findings and outline their implications on future research directions in refactoring.
   Developer's Motivation behind Refactoring. One of main findings show that developers are not only driven by design improvement and code smell removal when taking decisions about refactoring. According to our RQ1 find- ings, fixing bugs, and feature implementation play a major role in triggering various refactoring activities. Traditional refactoring tools are still leading their refactoring effort based on how it is needed to cope with design antipatterns, which is acceptable to the extent where it is indeed the developer's intention, otherwise, they have not been designed or tested in different circumstances. So, an interesting future direction is to study how we can augment existing refactoring tools to better frame the developer's perception of refactoring, and then their corresponding objectives to achieve (reducing coupling, improve code readability, renaming to remove ambiguity etc.). This will automatically induce the search for more adequate refactoring operations, to achieve each objective. Refactoring Support. Classifying refactoring commits by message is an important activity because it allows us to contextualize these refactoring activ- ities with information about the development activities that led to them. This contextualization is critical and will augment our ability to study the reasoning behind decisions to apply different types of refactoring. This will lead to bet- ter support for informing developers of when to apply a refactoring and what refactoring to apply. For example, recent studies try to understand how the development context which motivated a rename refactoring affects the way the words in a name changes when the refactoring is applied (Peruma et al., 2018, 2019b) for the purpose of modeling, more formally, how names evolve given a development context. Without approaches such as the one proposed in this work, these studies will be missing critical data. In particular, our findings show that renames are dominant, for test files, across all categories (Table 7). This indicates that renames occur in many, many different development con- texts and, with our tool, studies such as these could be extended to study how names change given each individual context instead of assuming they are indis- tinguishable. This extends to other work as well; there is a critical need for



assisting developers in determining when to apply a refactoring; what refactor- ing to apply; and in some cases how to apply the refactoring (Peruma et al., 2018, 2019b; Arnaoudova et al., 2013, 2014; Liu et al., 2013).
   Additionally, there is a demonstrated need to further automate refactoring support. Prior research by (Kim et al., 2014) has investigated the way de- velopers interact with IDEs when applying refactorings. (Negara et al., 2013; Murphy-Hill & Black, 2008) have shown that refactorings are frequently applied manually instead of automatically. This indicates that current support for re- factoring is not enough; the benefit of automated application is outweighed by the cost, which other researchers have highlighted (Newman et al., 2018; Li & Thompson, 2012). Finally, we theorize that it will be beneficial to study how refactorings are applied to solve different types of problems (i.e., in this case, different maintenance tasks). This is supported by research that isolates cer- tain types of code or code changes, such as isolating test from production code (Tufano et al., 2016). Like this example, future research must understand the context surrounding refactorings by identifying the reasoning (i.e., development context) behind refactoring operations. The results from this work directly im- pact research in this area by providing a methodology to categorize refactoring commit messages and providing an exploratory discussion of the motivation be- hind different types of refactorings. We plan to explore this question in greater detail in future research.
   Refactoring Documentation. One of the main purposes of the automatic detection of refactoring is to better understand how developers cope with their software decay by extracting any refactoring strategies that can be associated with removing code smells (Tsantalis et al., 2008; Bavota et al., 2013), or im- proving the design structural measurements (Mkaouer et al., 2014; Bavota et al., 2014). However, these techniques only analyze the changes at the source code level, and provide the operations performed, without associating it with any textual description, which may infer the rationale behind the refactoring applic- ation. Our proposal, of textual patterns, is the first step towards complementing the existing effort in detecting refactorings, by augmenting it with any descrip- tion that was intended to describe the refactoring activity. As previously shown in Tables 8, 9, and 11 developers tend to add a high-level description of their refactoring activity, and occasionally mention their intention behind refactoring (remove duplicate code, improve readability, etc.), along with mentioning the refactoring operations they apply (type migration, inline methods, etc.). This paper proposes, combined with the detection of refactoring operations, a solid background for future empirical investigations. For instance, previous studies have analyzed the impact of refactoring operations on structural metrics (Bavota et al., 2015; Cedrim et al., 2016; Palomba et al., 2017). One of the main limita- tions of these studies is the absence of any context related to the application of refactorings, i.e., it is not clear whether developers did apply these refactoring with the intention of improving design metrics. Therefore, it is important to consider commits whose commit messages specifically express the refactoring for the purpose of optimizing structural metrics, such as coupling, and complexity, and so, many empirical studies can be revisited with a more adequate dataset.



   Furthermore, our study provides software practitioners with a catalog of common refactoring documentation patterns (cf., Tables 8, 9, and 11) which would represent concrete examples of common ways to document refactoring activities in commit messages. This catalog of SAR patterns can encourage developers to follow best documentation patterns and to further extend these patterns to improve refactoring changes documentation in particular and code changes in general. Indeed, reliable and accurate documentation is always of crucial importance in any software project. The presence of documentation for low level changes such as refactoring operations and commit changes helps to keep track of all aspects of software development and it improves on the quality of the end product. Its main focuses are learning and knowledge transfer to other developers.
   Another important research direction that requires further attention con- cerns the documentation of refactoring. It has been known that there is a general shortage of refactoring documentation, as developers typically focus on describing their functional updates and bug patches. Also, there is no con- sensus about how refactoring should be documented, which makes it subjective and developer specific. Moreover, the fine-grained description of refactoring can be time consuming, as typical description should contain indication about the operations performed, refactored code elements, and a hint about the intention behind the refactoring. In addition, the developer specification can be ambigu- ous as it reflects the developer's understanding of what has been improved in the source code, which can be different in reality, as the developer may not neces- sarily adequately estimate the refactoring impact on the quality improvement. Therefore, our model can help to build a corpus of refactoring descriptions, and so many studies can better analyze the typical syntax used by developers in order to develop better natural language models to improve it, and potentially automate it, just like existing studies related to other types of code changes (Buse & Weimer, 2010; Linares-Vásquez et al., 2015; Liu et al., 2018).
   Refactoring and Developer's Experience. While refactoring is being applied by various developers (AlOmar et al., 2020b), it would be interesting to evaluate their refactoring practices. We would like to capture and better understand the code refactoring best practices and learn from these developers so that we can recommend them for other developers. Previous work (AlOmar et al., 2019a) performed an exploratory study on how developers document their refactoring activities in commit messages, this activity is called Self-Affirmed Refactoring (SAR). They found that developers tend to use a variety of textual patterns to document their refactoring activities, such as "refactor", "move" and "extract". In follow-up work, (AlOmar et al., 2019b) identified which qual- ity models are more in-line with the developer's vision of quality optimization when they explicitly mention in the commit messages that they refactor to im- prove these quality attributes. Since we noticed that various developers are responsible for performing refactorings, one potential research direction is to in- vestigate which developers are responsible for the introduction of SARs in order to examine whether experience plays a role in the introduction of SARs or not. Another potential research direction is to study if developer experience is one of



the factors that might contribute to the significant improvement of the quality metrics that are aligned with developer description in the commit message. In other words, we would like to evaluate the top contributors refactoring practice against all the rest of refactoring contributors by assessing their contributions on the main internal quality attributes improvement (e.g., cohesion, coupling, and complexity).
   Refactoring Automation. There have been various studies targeting the automation of refactoring (Harman & Tratt, 2007; Simons et al., 2015; Mkaouer et al., 2015; Lin et al., 2016; Mkaouer et al., 2016). They mainly rely on the correspondence between the impact of refactoring on the source code to guide the generation of code changes that will potentially improve it. Therefore, existing studies heavily rely on structural measurements to guide the search for these code changes, and so, improving quality attributes and removing anti-patterns were the main drivers for automated refactoring. Clearly, the challenge facing such approaches is applicability. Performing large-scale code changes, impacting various components in the source code, may be catchy for its quality, but it also drastically disturbs the existing software design. Although developers are in favor for optimizing the quality of their software, they still want to recognize their own design.

7. Threats to Validity
   We identify, in this section, potential threats to the validity of our approach and our experiments.
   Internal Validity. In this paper, we analyzed only the 28 refactoring oper- ations detected by Refactoring Miner, which can be viewed as a validity threat because the tool did not consider all refactoring types mentioned by (Fowler et al., 1999). However, in a previous study, (Murphy-Hill et al., 2012) reported that these types are amongst the most common refactoring types. Moreover, we did not perform a manual validation of refactoring types detected by Refactoring Miner to assess its accuracy, so our study is mainly threatened by the accuracy of the detection tool. Yet, (Tsantalis et al., 2018) report that Refactoring Miner has a precision of 98% and a recall of 87% which significantly outperforms the previous state-of-the-art tools, which gives us confidence in using the tool.
   Further, the set of commit messages used in this study may represent a threat to validity, because not all of the messages it may indicate refactoring activities. To mitigate this risk, we manually inspected a subset of change messages and ensured that projects selected are well-commented and use meaningful commit messages. Additionally, since extracting refactoring patterns heavily depends on the content of commit messages, our results may be impacted by the quant- ity and quality of commits in a software project. To alleviate this threat, we examined multiple projects. Moreover, our manual analysis is a time consuming and an error prone task, which we tried to mitigate by focusing mainly on com- mits known to contain refactorings. Also, since our keywords largely overlap with keywords used in previous studies, this raised our confidence about the found set but does not guarantee that we did not miss any patterns.



   Another threat relates to the detection of JUnit test files. The task of associating a unit test file with its production file was an automated process (performed based on filename/string matching associations). If developers de- viate from JUnit guidelines on file naming, false positives may be triggered. However, our manual verification of random associations and the extensiveness of our dataset acts as a means of countering this risk.
   External Validity. The first threat is that the analysis was restricted to only open source, Java-based, Git-based repositories. However, we were still able to analyze 800 projects that are highly varied in size, contributors, number of commits and refactorings. Another threat concerns the generalization of the identified recurring patterns in the refactoring commits. Our choice of patterns may have an impact on our findings and may not generalize to other open source or commercial projects since the identified refactoring patterns may be different for another set of projects (e.g., outside the Java developers community or projects that have a low number of or no commit messages). Consequently, we cannot claim that the results of refactoring motivation (see Figure 3) can be generalized to other programming languages in which different refactoring tools have been used, projects with a significantly larger number of commits, and different software systems where the need for improving the design might be less important.
   Construct validity. The classification of refactorings heavily relies on com- mit messages. Even when projects are well-commented, they might not contain SAR, as developers might not document refactoring activities in the commit messages. We mitigate this risk by choosing projects that are appropriate for our analysis. Another potential threat relates to manual classification. Since the manual classification of training commit messages is a human intensive task and it is subject to personal bias, we mitigate manual classification related errors by discarding short and ambiguous commits from our dataset and repla- cing them with other commits. Another important limitation concerns the size of the dataset used for training and evaluation. The size of the used dataset was determined similarly to previous commit classification studies, but we are not certain that this number is optimal for our problem. It is better to use a systematic technique for choosing the size of the evaluation set.
   To mitigate the impact of different commit message styles and auto-generated messages, we diversified the set of projects to extract commits from. We also randomly sampled from the two commits clusters, those containing detected refactorings and those without. An additional threat to validity relates to the construction of our set of refactoring patterns. One pattern could be used as an umbrella term for lots of different types of activity (e.g., "Cleaning" might mean totally different things to different developers). However, we mitigate this threat by focusing mainly on commits known to contain refactorings. Further, recent studies (Yan et al., 2016; Kirinuki et al., 2014) indicate that commit comments could capture more than one type of classification (i.e., mixed main- tenance activity). In this work, we only consider single-labeled classification, but this is an interesting direction that we can take into account in our future work.



   Conclusion Validity. The refactoring documentation research question has been provided along with the corresponding hypotheses in order to aid in drawing a conclusion. In this context, statistical tests have been used to test the significance of the results gained. Specifically, we applied the Wilcoxon test and the Mann-Whitney U test, widely used non-parametric tests, to test whether refactoring patterns are significant or not, and to test the occurrence of refactor in refactoring commits and non-refactoring commits, respectively. These tests make no assumption that the data is normally distributed. Refactoring motiv- ation categories and the way we grouped the refactoring concepts described in previous papers and established relations between them pose a threat to the conclusion validity of our study. If some information was not described in the literature, it may affect our conclusions.

8. Conclusion
   In this paper, we performed a large-scale empirical study to explore the mo- tivation driving refactorings, the documentation of refactoring activities, and the proportion of refactoring operations performed on production and test code. In summary, the main conclusions are: (1) our study shows that code smell res- olution is not the only driver for developers to factor out their code. Refact- oring activity is also driven by changes in requirements, correction of errors, structural design optimization and nonfunctional quality attributes enhance- ment. Developers are using wide variety of refactoring operations to refactor production and test files, and (2) a wide variety of textual patterns is used to document refactoring activities in the commit messages. These patterns could demonstrate developer perception of refactoring or report a specific refactoring operation name following Fowler's names.
   As future work, we aim to investigate the effect of refactoring on both change and fault-proneness in large-scale open source systems. Specifically, we would like to investigate commit-labeled refactoring to determine if certain refactoring motivations lead to decreased change and fault-prone classes. Further, since a commit message could potentially belong to multiple categories (e.g., improve the design and fix a bug), future research could usefully explore how to automat- ically classify commits into this kind of hybrid categories. Another potentially interesting future direction will be to conduct additional studies using other re- factoring detection tools to analyze open source and industrial software projects and compare findings. Since we observed that feature requests and fix bugs are also refactoring motivators for developers, researchers are encouraged to adopt a maintenance-related refactoring beside design-related refactoring when building a refactoring tool in the future.

References
Abebe, S. L., Haiduc, S., Tonella, P., & Marcus, A. (2011). The effect of lexicon bad smells on concept location in source code. In Source Code Analysis and Manipulation (SCAM), 2011 11th IEEE International Working Conference on (pp. 125-134). Ieee.




AlDallal, J., & Abdin, A. (2017). Empirical evaluation of the impact of object-oriented code re- factoring on quality attributes: A systematic literature review. IEEE Transactions on Software Engineering, PP, 1-1. doi:10.1109/TSE.2017.2658573.
Alkadhi, R., Nonnenmacher, M., Guzman, E., & Bruegge, B. (2018). How do developers discuss rationale? In 2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER) (pp. 357-369). IEEE.
AlOmar, E. A. (2020 (last accessed October 20, 2020)). self-affirmed-refactoring repository. URL:
https://smilevo.github.io/self-affirmed-refactoring/.
AlOmar, E. A., Mkaouer, M. W., & Ouni, A. (2019a). Can refactoring be self-affirmed? an ex- ploratory study on how developers document their refactoring activities in commit messages. In Proceedings of the 3nd International Workshop on Refactoring-accepted. IEEE .
AlOmar, E. A., Mkaouer, M. W., & Ouni, A. (2020a). Toward the automatic classification of self-affirmed refactoring. Journal of Systems and Software, (p. 110821).
AlOmar, E. A., Mkaouer, M. W., Ouni, A., & Kessentini, M. (2019b). On the impact of refactoring on the relationship between quality attributes and design metrics. In 2019 ACM/IEEE Interna- tional Symposium on Empirical Software Engineering and Measurement (ESEM) (pp. 1-11). IEEE.
AlOmar, E. A., Peruma, A., Newman, C. D., Mkaouer, M. W., & Ouni, A. (2020b). On the relationship between developer experience and refactoring: An exploratory study and preliminary results. In Proceedings of the 4th International Workshop on Refactoring IWoR 2020. New York, NY, USA: Association for Computing Machinery.
AlOmar, E. A., Rodriguez, P. T., , J., Bowman, Wang, T., Adepoju, B., Lopez, K., Newman,
C. D., Ouni, A., & Mkaouer, M. W. (2020c). How do developers refactor code to improve code reusability? In International Conference on Software and Systems Reuse. Springer.
Alshayeb, M. (2009). Empirical investigation of refactoring effect on software quality. Information and software technology, 51 , 1319-1326.
Amor, J., Robles, G., Gonzalez-Barahona, J., Navarro Gsyc, A., Carlos, J., & Madrid, S. (2006).
Discriminating development activities in versioning systems: A case study.
Arnaoudova, V., Di Penta, M., Antoniol, G., & Gueheneuc, Y.-G. (2013). A new family of software anti-patterns: Linguistic anti-patterns. In Software Maintenance and Reengineering (CSMR), 2013 17th European Conference on (pp. 187-196). IEEE.
Arnaoudova, V., Eshkevari, L. M., Di Penta, M., Oliveto, R., Antoniol, G., & Gueheneuc, Y.-G. (2014). Repent: Analyzing the nature of identifier renamings. IEEE Transactions on Software Engineering, 40 , 502-532.
Barry, B. et al. (1981). Software engineering economics. New York , 197 .
Bavota, G., De Lucia, A., Di Penta, M., Oliveto, R., & Palomba, F. (2015). An experimental investigation on the innate relationship between quality and refactoring. Journal of Systems and Software, 107 , 1-14.
Bavota, G., Dit, B., Oliveto, R., Di Penta, M., Poshyvanyk, D., & De Lucia, A. (2013). An empirical study on the developers' perception of software coupling. In Proceedings of the 2013 International Conference on Software Engineering (pp. 692-701). IEEE Press.
Bavota, G., Panichella, S., Tsantalis, N., Di Penta, M., Oliveto, R., & Canfora, G. (2014). Re- commending refactorings based on team co-maintenance patterns. In Proceedings of the 29th ACM/IEEE international conference on Automated software engineering (pp. 337-342). ACM.
Bird, S. (2002). Nltk: The natural language toolkit. ArXiv , cs.CL/0205028 .
Boehm, B. W. (2002). Software pioneers. chapter Software Engineering Economics. (pp. 641-686). Berlin, Heidelberg: Springer-Verlag. URL: http://dl.acm.org/citation.cfm?id=944331.944370.
Breiman, L. (2017). Classification and Regression Trees. CRC Press.




Brownlee, J. (2018). Statistical Methods for Machine Learning: Discover how to Transform Data into Knowledge with Python. Machine Learning Mastery. URL: https://books.google.com/ books?id=386nDwAAQBAJ.
Buse, R. P., & Weimer, W. (2010). Automatically documenting program changes. In ASE (pp.
33-42). volume 10.
Cedrim, D., Sousa, L., Garcia, A., & Gheyi, R. (2016). Does refactoring improve software structural quality? a longitudinal study of 25 projects. In Proceedings of the 30th Brazilian Symposium on Software Engineering (pp. 73-82). ACM.
Chang, C.-C., & Lin, C.-J. (2011). Libsvm: A library for support vector machines, . 2 . URL:
https://doi.org/10.1145/1961189.1961199.  doi:10.1145/1961189.1961199.
Dangeti, P. (2017). Statistics for Machine Learning. Packt Publishing.
Deng, N., Tian, Y., & Zhang, C. (2012). Support Vector Machines: Optimization Based Theory, Algorithms, and Extensions. Chapman & Hall/CRC Data Mining and Knowledge Discovery Series. Taylor & Francis.
Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learn- ing algorithms. Neural computation, 10 , 1895-1923.
Dig, D., Comertoglu, C., Marinov, D., & Johnson, R. (2006). Automated detection of refactor- ings in evolving components. In D. Thomas (Ed.), ECOOP 2006 - Object-Oriented Program- ming: 20th European Conference, Nantes, France, July 3-7, 2006. Proceedings (pp. 404-428). Berlin, Heidelberg: Springer Berlin Heidelberg. URL: https://doi.org/10.1007/11785477_24. doi:10.1007/11785477_24.
Erlikh, L. (2000). Leveraging legacy system dollars for e-business. IT professional, 2 , 17-23. Fowler, M., Beck, K., Brant, J., Opdyke, W., & Roberts, d. (1999). Refactoring: Improving the
Design of Existing Code. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc. URL: http://dl.acm.org/citation.cfm?id=311424.
Harman, M., & Tratt, L. (2007). Pareto optimal search based refactoring at the design level. In Proceedings of the 9th annual conference on Genetic and evolutionary computation (pp. 1106-1113). ACM.
Hattori, L. P., & Lanza, M. (2008). On the nature of commits. In 2008 23rd IEEE/ACM International Conference on Automated Software Engineering - Workshops (pp. 63-71). doi:10.1109/ASEW.2008.4686322.
Hindle, A., Ernst, N. A., Godfrey, M. W., & Mylopoulos, J. (2011). Automated topic naming to support cross-project analysis of software maintenance activities. In Proceedings of the 8th Working Conference on Mining Software Repositories MSR '11 (pp. 163-172). New York, NY, USA: ACM. URL: http://doi.acm.org/10.1145/1985441.1985466. doi:10.1145/1985441.1985466.
Hindle, A., German, D. M., Godfrey, M. W., & Holt, R. C. (2009). Automatic classication of large changes into maintenance categories. In 2009 IEEE 17th International Conference on Program Comprehension (pp. 30-39). doi:10.1109/ICPC.2009.5090025.
Hindle, A., German, D. M., & Holt, R. (2008). What do large commits tell us?: A taxonomical study of large commits. In Proceedings of the 2008 International Working Conference on Mining Software Repositories MSR '08 (pp. 99-108). New York, NY, USA: ACM. URL: http:
//doi.acm.org/10.1145/1370750.1370773. doi:10.1145/1370750.1370773.
Hönel, S., Ericsson, M., Löwe, W., & Wingkvist, A. (2019). Importance and aptitude of source code density for commit classification into maintenance activities. In The 19th IEEE International Conference on Software Quality, Reliability, and Security.
Jurafsky, D., & Martin, J. H. (2019). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition. Prentic e Hall, .
Kim, M., Gee, M., Loh, A., & Rachatasumrit, N. (2010). Ref-finder: a refactoring reconstruction tool based on logic query templates. In Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering (pp. 371-372). ACM.




Kim, M., Zimmermann, T., & Nagappan, N. (2014). An empirical study of refactoring challenges and benefits at microsoft. IEEE Transactions on Software Engineering, 40 , 633-649. doi:10. 1109/TSE.2014.2318734.
Kirinuki, H., Higo, Y., Hotta, K., & Kusumoto, S. (2014). Hey! are you committing tangled changes? In Proceedings of the 22Nd International Conference on Program Comprehension ICPC 2014 (pp. 262-265). New York, NY, USA: ACM. URL: http://doi.acm.org/10.1145/2597008.2597798. doi:10.1145/2597008.2597798.
Kochhar, P. S., Thung, F., & Lo, D. (2014). Automatic fine-grained issue report reclassification. In Engineering of Complex Computer Systems (ICECCS), 2014 19th International Conference on (pp. 126-135). IEEE.
Kowsari, K., Jafari Meimandi, K., Heidarysafa, M., Mendu, S., Barnes, L., & Brown, D. (2019).
Text classification algorithms: A survey. Information, 10 , 150.
Lane, H., Hapke, H., & Howard, C. (2019). Natural Language Processing in Action: Understand- ing, Analyzing, and Generating Text with Python. Manning Publications Company.
Lanza, M., & Marinescu, R. (2007). Object-oriented metrics in practice: using software metrics to characterize, evaluate, and improve the design of object-oriented systems. Springer Science & Business Media.
Le, T.-D. B., Linares-Vásquez, M., Lo, D., & Poshyvanyk, D. (2015). Rclinker: Automated link- ing of issue reports and commits leveraging rich contextual information. In 2015 IEEE 23rd International Conference on Program Comprehension (pp. 36-47). IEEE.
Levin, S., & Yehudai, A. (2017). Boosting automatic commit classification into maintenance activities by utilizing source code changes. In Proceedings of the 13th International Con- ference on Predictive Models and Data Analytics in Software Engineering PROMISE (pp. 97-106). New York, NY, USA: ACM. URL: http://doi.acm.org/10.1145/3127005.3127016. doi:10.1145/3127005.3127016.
Li, H., & Thompson, S. (2012). Let's make refactoring tools user-extensible! In Proceedings of the Fifth Workshop on Refactoring Tools WRT '12 (pp. 32-39). New York, NY, USA: ACM. URL: http://doi.acm.org/10.1145/2328876.2328881. doi:10.1145/2328876.2328881.
Lin, S., Ma, Y., & Chen, J. (2013). Empirical evidence on developer's commit activity for open- source software projects. In SEKE (pp. 455-460). volume 13.
Lin, Y., Peng, X., Cai, Y., Dig, D., Zheng, D., & Zhao, W. (2016). Interactive and guided archi- tectural refactoring with search-based recommendation. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering (pp. 535-546). ACM.
Linares-Vásquez, M., Cortés-Coy, L. F., Aponte, J., & Poshyvanyk, D. (2015). Changescribe: A tool for automatically generating commit messages. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering (pp. 709-712). IEEE volume 2.
Liu, H., Guo, X., & Shao, W. (2013). Monitor-based instant software refactoring. IEEE Transac- tions on Software Engineering, 39 , 1112-1126. doi:10.1109/TSE.2013.4.
Liu, Z., Xia, X., Hassan, A. E., Lo, D., Xing, Z., & Wang, X. (2018). Neural-machine-translation- based commit message generation: how far are we? In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (pp. 373-384). ACM.
Manning, C., Raghavan, P., & Schütze, H. (2008). Introduction to Information Retrieval. Cam- bridge University Press.
Mauczka, A., Brosch, F., Schanes, C., & Grechenig, T. (2015). Dataset of developer-labeled commit messages. In 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (pp. 490-493). doi:10.1109/MSR.2015.71.
Mauczka, A., Huber, M., Schanes, C., Schramm, W., Bernhart, M., & Grechenig, T. (2012). Tracing your maintenance work - a cross-project validation of an automated classification dictionary for commit messages. In J. de Lara, & A. Zisman (Eds.), Fundamental Approaches to Software Engineering: 15th International Conference, FASE 2012, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2012, Tallinn, Estonia, March 24 - April 1, 2012. Proceedings (pp. 301-315). Berlin, Heidelberg: Springer Berlin Heidelberg. URL: https://doi.org/10.1007/978-3-642-28872-2_21. doi:10.1007/978-3-642-28872-2_21.




McBurney, P. W., Jiang, S., Kessentini, M., Kraft, N. A., Armaly, A., Mkaouer, M. W., & Mc- Millan, C. (2017). Towards prioritizing documentation effort. IEEE Transactions on Software Engineering, 44 , 897-913.
Mkaouer, M. W., Kessentini, M., Bechikh, S., Cinnéide, M. Ó., & Deb, K. (2016). On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach. Empirical Software Engineering, 21 , 2503-2545.
Mkaouer, M. W., Kessentini, M., Bechikh, S., Deb, K., & Ó Cinnéide, M. (2014). Recommendation system for software refactoring using innovization and interactive dynamic optimization. In Pro- ceedings of the 29th ACM/IEEE international conference on Automated software engineering (pp. 331-336). ACM.
Mkaouer, W., Kessentini, M., Shaout, A., Koligheu, P., Bechikh, S., Deb, K., & Ouni, A. (2015). Many-objective software remodularization using nsga-iii. ACM Transactions on Software En- gineering and Methodology (TOSEM), 24 , 17.
Moser, R., Abrahamsson, P., Pedrycz, W., Sillitti, A., & Succi, G. (2007). A case study on the impact of refactoring on quality and productivity in an agile team. In Balancing Agility and Formalism in Software Engineering (pp. 252-266). Springer.
Moser, R., Sillitti, A., Abrahamsson, P., & Succi, G. (2006). Does refactoring improve reusability?
In International Conference on Software Reuse (pp. 287-297). Springer.
Munaiah, N., Kroh, S., Cabrey, C., & Nagappan, M. (2017). Curating github for engineered software projects. Empirical Software Engineering, 22 , 3219-3253.
Murphy-Hill, E., & Black, A. P. (2008). Refactoring tools: Fitness for purpose. IEEE Software, 25 , 38-44. doi:10.1109/MS.2008.123.
Murphy-Hill, E., Black, A. P., Dig, D., & Parnin, C. (2008). Gathering refactoring data: a com- parison of four methods. In Proceedings of the 2nd Workshop on Refactoring Tools (p. 7). ACM.
Murphy-Hill, E., Parnin, C., & Black, A. P. (2012). How we refactor, and how we know it. IEEE Transactions on Software Engineering, 38 , 5-18. doi:10.1109/TSE.2011.41.
Negara, S., Chen, N., Vakilian, M., Johnson, R. E., & Dig, D. (2013). A comparative study of manual and automated refactorings. In G. Castagna (Ed.), ECOOP 2013 - Object-Oriented Programming (pp. 552-576). Berlin, Heidelberg: Springer Berlin Heidelberg.
Newman, C. D., Mkaouer, M. W., Collard, M. L., & Maletic, J. I. (2018). A study on developer perception of transformation languages for refactoring. In Proceedings of the 2nd International Workshop on Refactoring (pp. 34-41). ACM.
Paixão, M., Uchôa, A., Bibiano, A. C., Oliveira, D., Garcia, A., Krinke, J., & Arvonio, E. (2020). Behind the intents: An in-depth empirical study on software refactoring in modern code review. 17th MSR, .
Palomba, F., Zaidman, A., Oliveto, R., & De Lucia, A. (2017). An exploratory study on the rela- tionship between changes and refactoring. In 2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC) (pp. 176-185). IEEE.
Pantiuchina, J., Zampetti, F., Scalabrino, S., Piantadosi, V., Oliveto, R., Bavota, G., & Di Penta,
M. (2020). Why developers refactor source code: A mining-based study, .
Peruma, A., Almalki, K., Newman, C. D., Mkaouer, M. W., Ouni, A., & Palomba, F. (2019a). On the distribution of test smells in open source android applications: An exploratory study. In Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering CASCON '19 (p. 193-202). USA: IBM Corp.
Peruma, A., Mkaouer, M. W., Decker, M. J., & Newman, C. D. (2018). An empirical investigation of how and why developers rename identifiers. In Proceedings of the 2nd International Workshop on Refactoring (pp. 26-33). ACM.
Peruma, A., Mkaouer, M. W., Decker, M. J., & Newman, C. D. (2019b). Contextualizing rename decisions using refactorings and commit messages. In Proceedings of the 19th IEEE International Working Conference on Source Code Analysis and Manipulation, IEEE .




Potdar, A., & Shihab, E. (2014). An exploratory study on self-admitted technical debt. In Software Maintenance and Evolution (ICSME), 2014 IEEE International Conference on (pp. 91-100). IEEE.
Ratzinger, J. (2007). sPACE: Software Project Assessment in the Course of Evolution. Ph.D. thesis. URL: http://www.infosys.tuwien.ac.at/Staff/ratzinger/publications/ratzinger_ phd-thesis_space.pdf.
Ratzinger, J., Sigmund, T., & Gall, H. C. (2008). On the relation of refactorings and software defect prediction. In Proceedings of the 2008 International Working Conference on Mining Software Repositories MSR '08 (pp. 35-38). New York, NY, USA: ACM. URL: http://doi.acm.org/10. 1145/1370750.1370759. doi:10.1145/1370750.1370759.
Silva, D., Tsantalis, N., & Valente, M. T. (2016). Why we refactor? confessions of github contrib- utors. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Found- ations of Software Engineering FSE 2016 (pp. 858-870). New York, NY, USA: ACM. URL: http://doi.acm.org/10.1145/2950290.2950305. doi:10.1145/2950290.2950305.
Simons, C., Singer, J., & White, D. R. (2015). Search-based refactoring: Metrics are not enough.
In International Symposium on Search Based Software Engineering (pp. 47-61). Springer.
Singh, R., & Mangat, N. (2013). Elements of Survey Sampling. Texts in the Mathematical Sciences.
Springer Netherlands.
SKlearn (2007a). 1.12. multiclass and multilabel algorithms - scikit-learn 0.23.2 documentation.
https://scikit-learn.org/stable/modules/multiclass.html.
SKlearn (2007b). sklearn.svm.svc - scikit-learn 0.23.2 documentation. https://scikit-learn.org/ stable/modules/generated/sklearn.svm.SVC.html#sklear.svm.SVC.
Soares, G., Cavalcanti, D., Gheyi, R., Massoni, T., Serey, D., & Cornélio, M. (2009). Saferefactor- tool for checking refactoring safety. Tools Session at SBES , (pp. 49-54).
Soares, G., Gheyi, R., Murphy-Hill, E., & Johnson, B. (2013). Comparing approaches to analyze refactoring activity on software repositories. Journal of Systems and Software, 86 , 1006-1022.
Stroggylos, K., & Spinellis, D. (2007). Refactoring-does it improve software quality? In Software Quality, 2007. WoSQ'07: ICSE Workshops 2007. Fifth International Workshop on (pp. 10- 10). IEEE.
Swanson, E. B. (1976). The dimensions of maintenance. In Proceedings of the 2Nd International Conference on Software Engineering ICSE '76 (pp. 492-497). Los Alamitos, CA, USA: IEEE Computer Society Press. URL: http://dl.acm.org/citation.cfm?id=800253.807723.
Szoke, G., Antal, G., Nagy, C., Ferenc, R., & Gyimóthy, T. (2017). Empirical study on refact- oring large-scale industrial systems and its effects on maintainability. Journal of Systems and Software, 129 , 107-126.
Szoke, G., Nagy, C., Ferenc, R., & Gyimóthy, T. (2014). A case study of refactoring large-scale industrial systems to efficiently improve source code quality. In International Conference on Computational Science and Its Applications (pp. 524-540). Springer.
Tan, A.-H. et al. (1999). Text mining: The state of the art and the challenges. In Proceedings of the PAKDD 1999 Workshop on Knowledge Disocovery from Advanced Databases (pp. 65-70). sn volume 8.
Tan, C.-M., Wang, Y.-F., & Lee, C.-D. (2002). The use of bigrams to enhance text categorization.
Information processing & management, 38 , 529-546.
Tsantalis, N., Chaikalis, T., & Chatzigeorgiou, A. (2008). Jdeodorant: Identification and removal of type-checking bad smells. In 2008 12th European Conference on Software Maintenance and Reengineering (pp. 329-331). IEEE.
Tsantalis, N., & Chatzigeorgiou, A. (2011). Identification of extract method refactoring opportun- ities for the decomposition of methods. Journal of Systems and Software, 84 , 1757-1782.
Tsantalis, N., Guana, V., Stroulia, E., & Hindle, A. (2013). A multidimensional empirical study on refactoring activity. In Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research CASCON '13 (pp. 132-146). Riverton, NJ, USA: IBM Corp. URL: http://dl.acm.org/citation.cfm?id=2555523.2555539.




Tsantalis, N., Mansouri, M., Eshkevari, L. M., Mazinanian, D., & Dig, D. (2018). Accurate and efficient refactoring detection in commit history. In Proceedings of the 40th International Con- ference on Software Engineering (pp. 483-494). ACM.
Tufano, M., Palomba, F., Bavota, G., Di Penta, M., Oliveto, R., De Lucia, A., & Poshyvanyk, D. (2016). An empirical investigation into the nature of test smells. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering ASE 2016 (pp. 4- 15). New York, NY, USA: ACM. doi:10.1145/2970276.2970340.
Vassallo, C., Grano, G., Palomba, F., Gall, H. C., & Bacchelli, A. (2019). A large-scale empirical exploration on refactoring activities in open source software projects. Science of Computer Programming, 180 , 1-15.
Wang, Y. (2009). What motivate software engineers to refactor source code? evidences from pro- fessional developers. In 2009 IEEE International Conference on Software Maintenance (pp. 413-416). doi:10.1109/ICSM.2009.5306290.
Wohlin, C., Runeson, P., Höst, M., Ohlsson, M. C., Regnell, B., & Wesslén, A. (2012). Experiment- ation in software engineering. Springer Science & Business Media.
Xia, X., Lo, D., Wang, X., & Yang, X. (2016). Collective personalized change classification with multiobjective search. IEEE Transactions on Reliability, 65 , 1810-1829.
Yan, M., Fu, Y., Zhang, X., Yang, D., Xu, L., & Kymer, J. D. (2016). Automatically classi- fying software changes via discriminative topic model: Supporting multi-category and cross- project. (pp. 296 - 308). volume 113. URL: http://www.sciencedirect.com/science/article/pii/ S016412121500285X. doi:https://doi.org/10.1016/j.jss.2015.12.019.
Zhang, D., Li, B., Li, Z., & Liang, P. (2018). A preliminary investigation of self-admitted refactorings in open source software. doi:10.18293/SEKE2018-081.
Zheng, A., & Casari, A. (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. O'Reilly Media.




Consider What Humans Consider: Optimizing Commit Message Leveraging Contexts Considered By Human








Abstract

Jiawei Li
University of California, Irvine Irvine, USA
jiawl28@uci.edu
Christian Petrov Innoopract, Mediform Germany christian.petrov@mediform.io

David Faragó
Innoopract, QPR Technologies Germany drdavidfarago@gmail.com
Iftekhar Ahmed
University of California, Irvine Irvine, USA
iftekha@uci.edu
1 Introduction

Commit messages are crucial in software development, supporting maintenance tasks and communication among developers. While Large Language Models (LLMs) have advanced Commit Message Generation (CMG) using various software contexts, some contexts developers consider to write high-quality commit messages are often missed by CMG techniques and can't be easily retrieved or even retrieved at all by automated tools. To address this, we propose Commit Message Optimization (CMO), which enhances human-written messages by leveraging LLMs and search-based optimization. CMO starts with human-written messages and iter- atively improves them by integrating key contexts and feedback from external evaluators. Our extensive evaluation shows CMO generates commit messages that are significantly more Rational, Comprehensive, and Expressive while outperforming state-of-the- art CMG methods and human messages 40.3%-78.4% of the time. Moreover, CMO can support existing CMG techniques to further improve message quality and generate high-quality messages when the human-written ones are left blank.

CCS Concepts
• Software and its engineering ? Software maintenance tools.
Keywords
commit message optimization, large language model
ACM Reference Format:
Jiawei Li, David Faragó, Christian Petrov, and Iftekhar Ahmed. 2025. Con- sider What Humans Consider: Optimizing Commit Message Leveraging Contexts Considered By Human. In . ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn


Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Conference'17, July 2017, Washington, DC, USA
(c) 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn

Commit messages are essential in documenting code changes by explaining what was modified and why, as well as providing neces- sary context for software maintenance. High-quality messages suc- cinctly capture the change details ("What"), the rationale ("Why"), and any additional relevant information, which is vital for assess- ing the commit's impact on the codebase [48, 68]. Well-written messages also help reduce software defect proneness [47]. How- ever, developers often neglect to write high-quality messages due to time constraints or lack of motivation. This leads to vague or incomplete messages-even in projects under the Apache Software Foundation [47, 68]. This issue is widespread; for instance, Dyer et al. [31] found that approximately 14% of commit messages in over 23,000 SourceForge projects were entirely blank.
  To assist developers in writing commit messages and elevate the commit message quality issue, numerous automated Commit Message Generation (CMG) techniques have been proposed by the Software Engineering (SE) research community. These methods, given a set of code changes, aim to generate descriptive commit mes- sages. These techniques can be broadly categorized into Template- based approaches where predefined patterns are used for generating messages [51, 64]. Retrieval-based methods which find similar past commit messages and adapt them [41, 65]. Translation-based tech- niques treat commit message generation as a language translation problem, translating code changes into natural language [53, 59, 76]. Hybrid approaches combine elements from the above techniques to enhance performance [39, 53, 65]. However, researchers have noted that these methods often produce messages that lack key information, such as the "What" or "Why," provide insufficient con- text, follow simple patterns, or are semantically unrelated to the human-written messages and difficult to read [29, 42, 55, 70]. This underscores the need for further improvements in CMG techniques to generate higher-quality messages.
   Recent advancements in Large Language Models (LLMs) have sig- nificantly enhanced various software engineering tasks, including CMG. Li et al. [48] introduced a state-of-the-art approach leverag- ing GPT-4 with the ReAct prompting technique [79]. Their method, Omniscient Message Generator (OMG), outperformed previous CMG techniques by incorporating a broader range of contextual information. While traditional CMG methods rely solely on code changes [30, 53, 65], OMG integrates additional software contexts, such as pull request titles and method/class summaries. However,


since commit messages serve as one of the primary communication channels for developers, facilitating discussions on diverse soft- ware maintenance activities that often depend on various software contexts [47, 56, 68], it is plausible that OMG misses certain nu- anced contextual information considered by developers to compose high-quality messages. This raises the question:
  RQ1:What software context information do developers consider when writing high-quality commit messages that existing CMG techniques fail to capture?
   The variety of contextual information developers consider while writing commit messages makes it difficult for automated tools to capture all the necessary human-considered contexts for gen- erating high-quality commit messages. To address this, integrat- ing human guidance is essential for producing commit messages that reflect these diverse contexts. Human guidance has already proven effective in tasks like code repair and assertion generation [23, 80], and similar methods could be beneficial in CMG. Eliseeva et al. [33] introduced Commit Message Completion (CMC) by us- ing human-typed prefixes to help LLMs generate more accurate commit messages. However, their study did not evaluate whether the generated messages included essential information like the "What"/"Why" information. Furthermore, they only considered the preceding commit to assist in message personalization, overlooking other critical software contexts that can impact commit message quality [47, 48, 68] including class/method containing the changes and related issue report/pull request.
  Inspired by CMC, we propose a new method called Commit Message Optimization (CMO) to enhance the quality of commit mes- sages by leveraging a search-based optimization approach. Starting with a human-written message, CMO iteratively refines and im- proves the message to optimize its quality. This method addresses the fact that human-written commit messages can often contain quality issues, such as missing essential information or poor read- ability [47, 68]. CMO brings two key advantages; firstly, by start- ing from a human-written message, the model gains access to the nuanced context a developer would naturally consider for the as- sociated code changes. This includes information that might be impractical for automated systems to retrieve but is readily avail- able in a developer's mind. Second, using the power of LLMs, CMO can "fix" or optimize the commit message by incorporating widely considered contexts and addressing quality issues that the human developer may miss. The iterative process allows the LLM to en- hance the message with more relevant information, improving its clarity and completeness.
   Since developers often leave commit messages empty due to time and motivation constraints [31], CMO is designed to address this issue by optimizing an initial message generated through few-shot prompting [24]. Additionally, we investigate whether CMO can enhance existing CMG techniques, such as OMG, to improve the quality of generated messages. While CMG techniques have been extensively studied in research, their adoption among software practitioners remains limited [29, 33]. Our goal is to bridge this gap by making CMG more practical and effective for real-world soft- ware development: (1) developers can optimize their own commit messages, mitigating the concern of CMG generating semantically irrelevant messages from scratch. (2) CMG-generated messages can be readily optimized with guidance and frequently considered


contexts, which would improve CMG's effectiveness in practice. Thus, we propose the following research questions:
  RQ2: How does CMO perform compared to the state-of-the- art CMG technique, CMC technique, and human developers? RQ3: How can CMO support existing CMG techniques and generate messages when the human-written messages were
left blank?
  Finally, we conduct an ablation study to analyze the effective- ness of the major components of CMO in generating high-quality commit messages:
  RQ4: How do software context collection tools and search- based optimization contribute to the overall effectiveness?
To sum up, our contributions are as follows:
1. We systematically identify software contexts that are widely considered by developers to write high-quality commit mes- sages but missed by existing CMG approaches.
2. We propose a novel framework, CMO, that optimizes exist- ing human-written commit messages using LLMs, software contexts, and automated evaluators.
3. Our human evaluation shows that our approach with appropriate hyper-parameters can significantly improve human-written commit message quality and outperform existing CMG and CMC techniques.
4. Our approach can enhance the performance of the state-of- the-art CMG technique and generate high-quality commit messages when the human-written ones were left blank.
2 Background & Related Work
2.1 Commit Message Quality
Researchers have traditionally assessed commit message quality using syntactic features like message length, word frequency, punc- tuation, and imperative verb mood [25, 26], but these methods overlook the semantic content. To address this, Tian et al.[68] pro- posed that high-quality commit messages should summarize code changes ("What") and explain their motivations ("Why"). However, they considered issue report/pull request links as providing the "Why" without evaluating the content of these links. Li et al. [47] addressed this by considering both commit messages and their link contents. In a separate study, Li et al. [48] expanded the criteria to include additional quality expectations from developers. Our study aims to improve both CMG-generated and human-written commit messages by optimizing them based on these quality factors.
2.2 Commit Message Generation
Researchers have developed various CMG techniques that automate message generation from code changes. These techniques have a variety of underlying mechanisms. Template-based approaches [51, 64] use predefined templates for specific code changes but struggle with generalization and often omit the crucial "Why" information. Retrieval-based methods [41, 65] retrieve similar code changes and reuse their commit messages. However, this approach relies on human-written messages, which are frequently flawed-about 44% of the commit messages in Open-Source Software (OSS) projects miss critical "What" or "Why" details [68]. Translation-based tech- niques [53, 59] use Neural Machine Translation (NMT) to "translate" code changes into commit messages but a majority (90%) of the


generated messages are short, simple, and inadequate [29]. Hy- brid approaches [39, 65] combine retrieval and translation methods, yet their effectiveness can be hampered by the limitations of both techniques and issues with the fusion process [39].
   Despite advancements in CMG, most approaches still rely solely on the git diff as input, overlooking associated software contexts that could enhance message quality [30, 53, 65]. Only a few stud- ies have incorporated additional contexts, such as issue states or modified ASTs, alongside code changes [53, 72]. Recently, Eliseeva et al.[33] introduced CMC that uses the prefix of human-written messages and commit history to improve performance and tries to address the issue of generating commit messages that are semanti- cally irrelevant to human-written ones [42, 55, 70]. However, the authors did not assess the semantic quality of completed messages ("What"/"Why") [47, 48, 68] or consider essential software contexts crucial for producing high-quality commit messages such as pull request/issue report and the changed class/method [48].
  With the advent of powerful LLMs such as ChatGPT [12] and GPT-4 [16], researchers have integrated these models into CMG techniques, achieving results comparable or superior to existing approaches [48, 74, 75]. To meet software developers' expectations for high-quality commit messages while considering broader soft- ware contexts, Li et al. [48] introduced OMG, which uses ReAct prompting [79] with GPT-4. This approach retrieves relevant soft- ware contexts, such as issue reports, pull requests, and method/class changes, to ground commit messages in factual software repository data. Human evaluations showed that OMG had set a new state- of-the-art (SOTA) CMG, with some messages deemed better than those that humans wrote.
  However, OMG still missed several software context details in- cluding certain related code changes in commit history and project requirement, which humans commonly consider when writing high- quality commit messages. To overcome the limitations of the SOTA technique, our study introduces additional retrieval tools, leverages human-written commit messages as a "prefix" when available, and integrates guidance from external evaluators. These enhancements ensure higher message quality by preserving critical information and preventing omissions.

2.3 Large Language Models & Search-based Optimization
In recent years, LLMs have been increasingly applied to optimiza- tion, search-based planning, and problem-solving across various domains [52, 77, 78]. Optimization typically begins with an initial solution, which is iteratively refined or replaced to maximize an objective function. Traditionally, this process requires carefully for- mulating the optimization problem and using an external solver to compute updates. However, recent studies have shown that embed- ding the optimization problem in a prompt and instructing LLMs to refine solutions based on previous outputs and problem descrip- tions iteratively can yield strong performance [77, 78]. Despite this progress, existing CMG techniques have not explored this ap- proach's applicability. This paper addresses this gap by leveraging a state-of-the-art LLM as an optimizer to enhance commit message generation. Our approach iteratively refines commit messages by incorporating new context at each step, producing progressively


improved candidates. By integrating feedback from multiple com- mit message quality evaluators, our method enables the LLM to generate higher-quality commit messages through a continuous optimization process.

3 RQ1: Categorization of Missing Software Context Information
Li et al. [48] used GPT-4 in their OMG approach to generate commit messages, achieving state-of-the-art results in CMG. However, since commit messages are a key communication channel for discussing diverse maintenance activities across various software contexts [47, 68], we argue that developers have considerable freedom to consider any information sources that originate from various places in the software repository or even interpersonal discussions and hence OMG may miss certain crucial information in its generated messages.
   For example, as shown in Figure 1a, understanding the defini- tion and usage of the invoked method getComputer is essential to generate a commit message that semantically aligns with the human-written one. OMG generated a message as "Fix: Exclude "master" from testGetComputerView() test. In the testGetComputer- View() method in the ComputerClientLiveTest.java file, a condition has been added to exclude the "master" from the test. This change ensures that the test only validates the display name of each com- puter in the view, excluding the "master".", which reflects its lack of knowledge or consideration of this invoked method. Instead, it only captures the textual content of the source code at a surface level ("...exclude the "master" from the test.", "only validates the display name of each computer in the view, excluding the "master"."), missing the essential reason behind the change that "master is not accessible via getComputer." In this study, we investigate the software contexts humans consider when writing high-quality messages that OMG failed to capture. Incorporating them in the CMG techniques may further improve the generated message quality. We present the methodology in (Section 3.1) and the results (Section 3.2).
3.1 Methodology to Answer RQ1
Li et al. [48] constructed the OMG dataset (Section 4.3.3), consisting of 381 commits from 32 Apache projects. Each commit in this dataset includes both a human-written commit message and an OMG- generated message. To comprehensively evaluate commit message quality, Li et al. proposed four key metrics (Section 4.3.2), scoring all messages on a 5-point Likert scale (0: poor to 4: excellent). The full scoring process is detailed in [48]. To ensure the use of high- quality human-written messages, we filtered out commits where the human-written message scored below 3 in any of the four metrics. This selection process resulted in 262 commits, retaining both their human-written and OMG-generated messages.
   We qualitatively analyzed the code changes, high-quality human- written commit messages, and OMG-generated messages for the 262 selected commits to identify the software contexts likely considered when composing these messages. This analysis followed the open coding protocol [38]. Two researchers with more than five years of Java development independently examined the code changes and associated commit messages, coding the software contexts refer- enced in the messages. These coded contexts were then verified


through close examination to ensure their relevance. For example, as illustrated in Figure 1a, if the human-written message referenced details about the invoked method getComputer, we coded this as Excluded Callee Knowledge (Section 3.2). To verify its importance, we manually inspected the method's body to determine whether awareness of its functionality would support message composition. Similarly, we identified and verified contexts that contributed to the OMG-generated messages. Each newly identified context was com- pared against existing codes during coding to determine whether it represented a distinct category or a subset of an existing one. This iterative comparison was conducted throughout the coding process and refined through negotiated agreement [34], where researchers discussed the rationale for applying specific codes and reached a consensus. Ultimately, we identified the contexts considered by human authors but overlooked by OMG. Section 3.2 details the finalized set of context codes.


3.2 Answer to RQ1: Software Context Missed by CMG
3.2.1 Context Themes and Frequency. We identified seven cate- gories of software context that humans considered while writ- ing high-quality messages, but OMG missed. Specifically, 67.6% (177/262) of OMG-generated messages failed to include some soft- ware contexts recognized by humans. Notably, a single commit can exhibit multiple contexts missed by OMG but considered by humans. The categories are described below:
1. Unreferenced Software Maintenance Goals (55.9%, 99/177): This category involves developers describing code changes with reference to a specific maintenance goal, such as functional correc- tion, new feature addition, or non-functional improvement, without citing an issue report or pull request. Instead, the commit message itself conveys the goal. For example, a message like "change required after plexus update" [2] indicates that the changes ensure compatibil- ity with an updated third-party dependency, with the maintenance goal serving as context for explaining the code changes.
  Moreover, some code changes are made to resolve some personal mistakes developers have made in the past. These mistakes can include copy-paste errors, accidents due to negligence, and typos. For example, "fix typo: wrong if guard variable" [9] shows the reason for the wrong if guard variable is the fact that some developer made a typo. In addition, some developers may simply write commit messages as "Remove author tag. Thanks Sylvain for pointing at this, this happens when you copy paste and don't think about what you're doing." [1] or "Remove getFilter method inadvertantly left in" [6].
2. Excluded Callee Knowledge (24.9%, 44/177): This category occurs when developers modify method calls or make changes related to method calls, leading them to describe the "What"/"Why" based on their knowledge of those invoked methods. However, such methods are not defined in the code change (git diff). For example, as shown in Figure 1a [8], developers used the invoked method to explain the motivation behind the changes. In this case, the addition of the if statement is due to the fact that "master" is inaccessible via getComputer. Understanding the method getComputer was essential for crafting this commit message.


3. Implicit Project Requirements/Practices (24.3%, 43/177): De- velopers often implicitly rely on project requirements or recom- mended practices to explain code changes. Phrases like "should," "do not," or "allow" suggest that a project requirement or convention influenced the decision, though developers did not reference spe- cific requirements directly. Instead, they use certain words to imply the necessity of the change. For example, "Don't try to config mdb destination if we aren't auto creating resources." [4] suggests that the change was made to correct a violation of a project requirement, even though the specific requirement is not explicitly mentioned.
4. Excluded Variable Data Types (15.3%, 27/177): Certain vari- ables are often referenced but not defined in the code change (i.e., git diff), but either in another file or in the same file outside the code change. However, understanding the class or data types of such variables helps explain the code changes in the commit message. For example, in commit [10], developers added several methods, each returning a variable. The message "Add getters for private ivars" highlights that the variables' data types and access modifiers were considered.
5. Miscellaneous Related Code Changes in History (9.6%, 17/177): Code changes from the project's history are often referenced when writing commit messages for a given commit. These historical changes may relate to the current commit in various ways, such as fixing defect-introducing changes, enhancing earlier features, or repurposing previous modifications. For example, "Re-adding Reflection2.constructor. Removed in 671749d but used downstream in jclouds-labs" [3]. Here, the previous changes serve as context for composing the message.
6. Complete Enclosing Code Blocks (7.3%, 13/177): The sur- rounding source code is often considered when writing commit messages. However, the limited length of the git diff-which rep- resents the code changes - may not provide sufficient context for developers or CMG approaches to fully explain or summarize the changes. Additional context, such as the entire enclosing statement block, is often needed but may not be included in the git diff. OMG missed this information because its summaries of the enclosing method or class captured only high-level functionality, leaving out important detailed source code information. Hence this theme fo- cuses on the source code near the changed lines, as opposed to the broader summaries of the method or class considered by OMG. For example, accurately describing the changes in Figure 1b [5] (i.e., onEndRequest) requires access to the full enclosing try-catch block to identify the executed method in the try section. Relying solely on the git diff would miss this crucial context, as reflected in the associated human-written commit message.
7. Requisite Compile/run-time Information (5.1%, 9/177): When developers commit code changes to fix compilation or run-time errors, they often explicitly specify this in the commit message. For example, "Resolve trivial compilation error after previous merge" [7] indicating the changes address a compile-time error, reflecting the developers' understanding of the software's compile-time behavior.


	
(a) Example of Necessary Callee Knowledge	(b) Example of Complete Enclosing Code Blocks

Figure 1: Example Commits


4 RQ2: Effectiveness of Commit Message Optimization (CMO)
4.1 Automated Software Context Retrieval
To support the LLMs' reasoning and generation of commit messages that are more factual and grounded on the code base, we implement tools to automatically retrieve the identified software contexts in Section 3.2 that were missed by OMG. These tools aim to capture the human-considered contexts, further enhancing the quality of LLM-generated commit messages.
  To extract Excluded Callee Knowledge, Excluded Variable Data Types, and Complete Enclosing Code Blocks (Section 3.2), we lever- aged JavaParser [58], a widely used tool for parsing Java program elements [48, 73]. This allowed us to retrieve relevant information about invoked methods, variables, and statement blocks related to the modified code. Specifically, we identified the data types and class information of variables referenced in the code change (Ex- cluded Variable Data Types). Additionally, we extracted the smallest enclosing statement blocks, which often provide sufficient con- text for composing human-written commit messages (Complete Enclosing Code Blocks). These blocks, delimited by curly brackets, included all statement types supported by JavaParser. For Excluded Callee Knowledge, we extracted the method bodies of functions invoked within the modified code and summarized them using a state-of-the-art method-level code summarization technique [37], also employed by OMG. This approach helps manage the limited in- put context length of LLMs [69]. Our extraction process focused on retrieving invoked methods and variable data types defined within the project's source files or as part of the Java language. Identifying methods and data types from third-party libraries remains an open area for future work.
  However, we did not implement automated tools to retrieve information for all software context themes in Section 3.2 due to the impracticality of automatically retrieving them and the significant noise brought by this process. To support our decision and assess the feasibility of implementing the retrieval tools for these themes, we also consulted with two senior Java software engineers with more than ten years of Java software development and commit message writing experience.
   For Unreferenced Software Maintenance Goals, code changes often address specific maintenance objectives documented in artifacts like issue reports or pull requests. However, identifying these goals without explicit references is challenging. While issue-commit link recovery techniques could help, their limited accuracy (0.1-0.5 [81]) makes their integration risky, potentially reducing commit message

reliability. As improving these techniques is beyond our scope, we excluded them. Moreover, some goals, like personal mistakes recognized during coding, exist only in the developer's mind and cannot be automatically retrieved.
  Another approach is to compile and run the code before and after changes to detect performance improvements or bug fixes. However, this requires analyzers, tests, and environment setup. After resetting repositories to pre- and post-change versions and resolving dependencies, only 33% compiled and ran successfully, likely due to bugs, dependency issues, or other factors. Hence, to avoid introducing significant noise, we did not develop tools for retrieving compile/run-time information to help LLMs identify maintenance goals. Similarly, we also didn't implement retrieval tools for Requisite Compile/run-time Information.
  For Implicit Project Requirements/Practices, detailed information about the specific requirements guiding code changes is often unavailable. Creating traceability links between source code and requirements requires substantial manual effort and is typically incomplete in most projects [40]. Moreover, existing traceability recovery techniques perform poorly, with F-1 scores below 0.5 [27, 40, 57, 83], risking significant noise if applied. Since improving these methods is beyond our scope, we did not develop tools to extract linked requirements from code changes.
  For Miscellaneous Related Code Changes in History, automatically retrieving relevant historical commits is challenging. Our analysis of 17 commits where developers referenced past changes revealed four types of relevant history: (1) the most recent commit changing the same lines (23.5%), (2) the most recent commit changing the same functions (17.6%), (3) a specific commit affecting the same lines or functions (23.5%), and (4) a commit impacting related code units or motivating the current change (35.4%). Since which type of his- torical change is relevant depends heavily on context, any retrieval tool based on a single pattern would likely introduce significant noise. Therefore, we did not implement such a tool.

4.2 Commit Message Optimization (CMO)
4.2.1 Motivation. Since many of the contexts frequently consid- ered by developers for writing high-quality commit messages are missed by CMG techniques and cannot be reliably extracted us- ing automated tools (as discussed in Section 4.1), human-written commit messages offer a valuable alternative. These messages in- herently capture software contexts that are difficult or impractical to extract automatically. Incorporating human-written messages as a reference in CMG could mitigate the need for extensive con- text retrieval while improving message quality. However, despite


the proven success of human guidance in automating other SE tasks-such as program repair and assertion generation [23, 80]-its potential in CMG has been largely overlooked. In this study, we introduce Commit Message Optimization (CMO), a technique that enhances commit message quality by optimizing human-written messages with additional software contexts frequently considered by developers. The following sections outline the design and im- plementation of CMO in detail.
4.2.2 Objective Function. In this study, we aim to enhance the qual- ity of generated commit messages using four key metrics detailed in Section 4.3.2. To achieve this, we employed two complemen- tary evaluators. The first, called the LLM-based Quality Evaluator, leverages LLMs' reasoning capabilities to assess commit messages based on corresponding git diffs, focusing on how well the message reflects the code changes. The second, termed the Retrieval-based Quality Evaluator, focuses on identifying essential software con- texts-beyond just the diff-that are typically present in high-quality human-written messages. Together, these evaluators offer a holis- tic view of message quality. Below, we describe each evaluator in detail and explain how their outputs are combined into a unified evaluation score.
  We detail the evaluators and the combined evaluation score calculation mechanism below.
1) LLM-based Quality Evaluator: We fine-tuned GPT-3.5-Turbo
[13] (GPT-4 was unavailable for fine-tuning at the time) to auto- matically assess commit messages based on the four metrics in Section 4.3.2 since LLMs can serve as evaluators [35, 44, 78]. Using the training split of the OMG dataset, we trained the model and evaluated on the validation split (Section 4.3.3). Each commit in the dataset is associated with three different messages (human- written, FIRA-generated (FIRA [30], a CMG outperformed by OMG [48]), and OMG-generated), resulting in 915 training and 228 val- idation messages, each labeled with four human-assigned scores. We framed this as a multi-class classification task, where Likert scores (0-4) served as class labels and fine-tuned GPT-3.5-Turbo to predict scores based on git diffs and commit messages. We fine- tuned separate models to improve performance, one for each metric. To mitigate class imbalance (e.g., only 8.6% of messages received a score of 3 for Rationality), we applied random oversampling [45] using imblearn [17]. Table 1 presents the classifier performance on the validation split for each metric.
Table 1: The Performance of LLM-based Quality Evaluators


Accuracy
Precision
Recall
F1
Rationality
0.719
0.653
0.719
0.684
Comprehensiveness
0.719
0.695
0.719
0.660
Conciseness
0.895
0.801
0.895
0.845
Expressiveness
0.912
0.849
0.912
0.875

2) Retrieval-based Quality Evaluator: To provide additional human guidance regarding the contexts to be considered in the objective function of optimizing the messages, especially when the initial human-written message is of low quality without valuable contexts, we introduce the Retrieval-based Quality Evaluator. This evaluator gauges the quality of generated commit messages by


measuring their semantic similarity to high-quality human-written messages that cover both the "What" and "Why" aspects, providing quality estimation by comparing with what a skilled human de- veloper would write. As depicted in Figure 2, the process starts by retrieving git diffs from a data corpus that are semantically similar to the target diff being optimized. The evaluator then compares the generated commit message with a high-quality human-written message, using semantic similarity as the evaluation score. In addi- tion, the retrieved high-quality human-written messages and their git diffs are provided to the optimization process as extra guidance.


Figure 2: Overview of Retrieval-based Quality Evaluator

  To construct a high-quality data corpus for retrieving similar git diffs, we collected commits from 32 Apache projects previously analyzed by OMG [48] and other studies [47, 56]. To ensure the in- clusion of only well-formed commit messages, we filtered commits based on whether they contained both "What" and "Why" infor- mation, using the criteria for "Good" commit messages defined by Tian et al.[68]. This classification was performed using the state-of- the-art model from Li et al. [47], which automatically detects the presence of "What" and "Why" components in commit messages.
  The evaluator starts by identifying diffs that are most similar to the target diff. For this purpose, we employed CCT5 [49], a pre- trained language model designed to capture the semantic essence of code changes. We represented each git diff using the vectorized embedding of the special token [CLS] from CCT5's final encoder layer and computed cosine similarity to identify the most similar diffs to the target diff.
  Next, to assess the semantic similarity between commit mes- sages, we utilized all-mpnet-base-v2 [20], a high-performing natu- ral language model from the Sentence Transformer Leaderboard [19]. We vectorized commit messages and measured cosine sim- ilarity between the generated message and human-written mes- sages from the most semantically similar git diffs identified using CCT5 [36, 37, 55]. This cosine similarity score served as an evalua- tion metric, reflecting how closely the generated message aligned with high-quality human-written messages.
   To mitigate potential bias from relying on a single most similar git diff, we experimented with different retrieval sizes. Specifically, we retrieved the top 1, 5, 10, and 20 most similar git diffs and computed the final evaluation score as the average cosine similarity between the generated message and human-written messages.
3) Combined Evaluation Score: Finally, we combine the results from both evaluators to produce a single score for each of the four metrics, which serves as the objective function. To normalize

the evaluation score of the Retrieval-based Quality Evaluator ("Sim Score" ), which ranges from 0 to 1, we scaled it to a range of 0 to 4 by multiplying by 4. This allowed us to directly compare it with the LLM-based Quality Evaluator score ("LLM Score" ).
   Additionally, we propose that the combined evaluation score can better approximate human judgment by incorporating the corre- lation between "Sim Score" and human-labeled scores, as well as "LLM Score" and human-labeled scores, into the weighting strategy. Following prior research that uses Pearson correlation to assess the effectiveness of automated metrics compared to human judg- ment [54, 71], we conducted a Pearson correlation analysis on the scores from the validation split of the OMG dataset. The result- ing correlation coefficients were used as weights for "Sim Score" and "LLM Score" in the combined score (Equation 1). Here, Sim Co- eff represents the correlation coefficient between "Sim Score" and human-labeled scores, while LLM Coeff represents the correlation for "LLM Score" and human-labeled scores.
  For the Retrieval-based Quality Evaluator, we configured it to retrieve the top 10 most similar git diffs, as this setting yielded the highest correlation coefficient. Notably, since "Sim Score" did not exhibit a significant correlation (p > 0.05) with human-labeled scores for Conciseness, we relied solely on "LLM Score" for that metric.


candidates (msg_candidate). At each subsequent step, the candidate with the highest optimization score is dequeued from the priority queue and updated with the contexts that haven't been considered yet, generating further candidates for improvement (lines 11-17).
   We implemented the UPDATE function by prompting GPT-4, as it has demonstrated SOTA performance in CMG through prompt- ing [48]. The prompt included the target git diff, a definition of git diff, the expected commit message format[48], and explanations of the four evaluation metrics and their scoring criteria [48]. To help GPT-4 optimize for higher scores from the Retrieval-based Qual- ity Evaluator (which contributes to the overall optimization score), we also provided the top 10 git diffs most similar to the target diff, along with their corresponding commit messages. GPT-4 was explic- itly instructed to improve the existing commit message-whether human-written or a previous candidate-rather than generating a new message from scratch.
  We incorporated multiple stopping criteria alongside a fixed step_limit to control the optimization process. First, we intro- duced a dynamic score improvement threshold (improve_threshold) that decreases as the number of optimization steps grows (line 7), based on the assumption that improvement naturally dimin- ishes as candidate messages approach higher quality. Initially, the threshold is set as a percentage (p) of the optimization score of the human-written message (line 3), with a minimum threshold

	?????? ?????? ?? ??	

	?????? ?????? ?? ??	

(min_threshold) to prevent it from nearing zero (lines 8-9). The

???????????? ?????????? = ?????? ?????????? × 4 × (	) + ?????? ?????????? × (	)  (1)
4.2.3 Search-based Optimization Algorithm Design. Algorithm 1 outlines the search-based optimization process.We used the tools described in Section 4.1 to retrieve relevant software contexts. Ad- ditionally, we incorporated all tools from OMG. A comprehensive list of all automatically retrieved contexts is provided in Table 2.
   Since our optimization starts from a human-written commit mes- sage, we enhanced OMG's commit type classifier by including both the git diff and the human-written message in the classifier prompt. Following OMG, we evaluated the classifier on the dataset from Levin et al. [46], using accuracy as the evaluation metric, consistent with prior reports. With the added human-written messages, our classifier achieved 81.0% accuracy, outperforming OMG's classi- fier (53.6%) and Levin et al.'s results (76.7%). Since commit type information is an essential part of the message format expected by developers [48], we provide it directly rather than using it as an optimization context. As a result, we consider seven software contexts in total for commit message optimization.
Table 2: Automatically Retrieved Software Contexts

Important File Information
Commit Type Information
Pull Request/Issue Report
Method Body Summary
Class Body Summary
Complete Enclosing Code Blocks
Excluded Callee Knowledge
Excluded Variable Data Types

   To simplify our search-based algorithm for optimizing a single objective function, we summed the four metrics from the Combined Evaluation Score (Section 4.2.2, referred to as the optimization score, which is returned by the EVALUATE function.) The algorithm first updates the human-written commit message using each available context individually (Table 2), generating different commit message

optimization halts if the score improvement between the latest highest_score and the one updated two steps earlier is less than the threshold (lines 22-23), enabling longer optimization for low- quality messages and early stopping for high-quality ones. Addition- ally, we set GPT-4's temperature to zero to generate deterministic outputs [48, 61]. However, if the improvement across two steps falls below the threshold, we increase the temperature to allow GPT-4 to produce more diverse candidates, potentially surpassing the threshold and continuing the optimization process [52].

4.3 Methodology to Answer RQ2
4.3.1 Baselines. We selected OMG[48], CMC [33] (chosen because both CMC and our approach are guided by human input), and lastly, human-written commit messages as baselines.

4.3.2 Commit Message Evaluation Metrics. Li et al. [48] proposed four metrics to comprehensively evaluate the quality of commit messages. In this work, we also used these metrics:
Rationality: Assesses whether the commit message provides a logical explanation for the code changes (the "Why" information) and clearly states the commit type. Comprehensiveness: Evalu- ates whether the commit message includes a summary of the code changes (the "What" information) and covers all affected files. Con- ciseness: Measures whether the commit message conveys essential information succinctly, ensuring readability and ease of under- standing. Expressiveness: Reflects whether the commit message is grammatically correct and fluent. We also adopted traditional met- rics that are widely used in previous CMG works including BLEU [60], METEOR [22], and ROUGE-L [50] where the human-written messages are used as references.



Algorithm 1 Search-based Optimization

Require: git diff (diff), available contexts, human msg
Ensure: optimized msg, evaluation score
1: ???????? ? 0, ????????_?????????? ? ??
2: h????h?????? _?????????? ? EVALUATE(diff, human msg)
3: ??????????????_??h??????h?????? ? h????h?????? _?????????? × ??, ??????_??h??????h?????? ?
??????????????_??h??????h??????


all techniques in this study (Section 4.4, 5, 6) and the significant manual effort required for human evaluation (Section 4.3.4, 5.1, 6.1).
4.3.4 Human Evaluation. We also conducted a human evaluation in which two researchers, each with over five years of Java devel- opment experience, independently evaluated 119 commits from the CMO dataset (Section 4.3.3). For each commit, they ranked

????????_??????????
4: ????????????????_??????????.enqueue(human msg)

four commit messages-comprising the human-written message optimized by CMO and baseline messages (human-written, OMG,

5: while ???????? < ????????_?????????? do
6:	???????? ? ???????? + 1
7:	??????????????_??h??????h??????  ?  (??????????????_??h??????h?????? ×(????????_?????????? -???????? ) )
8:	if ??????????????_??h??????h?????? < ??????_??h??????h?????? then
9:	??????????????_??h??????h?????? ? ??????_??h??????h??????
10:	end if
11:	?????? _?????? ? ????????????????_??????????.dequeue()
12:	for each context in available contexts do
13:		??????_?????????????????? ? UPDATE(??????_??????, diff, context, considered contexts, EVALUATE(diff, ?????? _??????))
14:	????????????????_??????????.enqueue(??????_??????????????????)
15:	end for
16:	????????????????_??????????.sort(key = evaluation score)
17:	?????? _?????????? ? EVALUATE(diff, ????????????????_??????????[0].msg)
18:	if ?????? _?????????? > h????h?????? _?????????? then
19:	h????h?????? _?????????? ? ?????? _??????????
20:	optimized msg ? ????????????????_??????????[0].msg
21:	evaluation score ? h????h?????? _??????????
22:	if h????h?????? _??????????-h????h?????? _????????????????????????_?? ????_????????????????????_??????
< ??????????????_??h??????h?????? then
23:	break
24:	end if
25:	end if
 26: end while	


4.3.3 Dataset. Our dataset contains 500 commits from the 32 Apache Java projects studied by OMG [48] consisting two sub-datasets: OMG dataset: Li et al. [48] collected 381 commits where each com- mit's code diff is paired with three commit messages: one human- written, one generated by OMG, and one by FIRA [30], a CMG technique that OMG outperformed. We randomly sampled 80%
(305) of the commits from the OMG dataset to obtain the training split, which was used to train the LLM-based Quality Evaluator (Sec- tion 4.2.2). The remaining 20% (76) were used in the validation split to evaluate the performance of the LLM-based Quality Evaluator and calculate the correlation in the weighting strategy of the Com- bined Evaluation Score. Thus, all the components and parameters of CMO are finalized on these two splits of this sub-dataset.
CMO dataset: To prevent data leakage and evaluate the effective- ness of CMO, we collected commits from the Apache projects whose commit dates were after the knowledge cutoff date of GPT-4 and GPT-3.5-Turbo (December 2023) [21]. Note that we only collected commits of similar size that could be processed by OMG to ensure a fair comparison. From this pool, we randomly sampled 119 commits out of 171 eligible commits (95% confidence level and a 5% margin of error) due to the financial cost of using the OpenAI API to run

and CMC)-based on the four evaluation metrics (Section 4.3.2). To ensure fairness, the evaluators were blinded to the sources of all messages, including the human-written ones, but had access to all relevant software contexts associated with the code changes.
  As part of the human evaluation, we also surveyed 22 Apache OSS developers to assess commit messages, following university- approved Institutional Review Boards (IRB) protocols. Each devel- oper received a survey containing 10 randomly sampled git diffs (from the 119 in the CMO dataset), along with four correspond- ing messages-human-written, OMG-generated, CMC-completed, and CMO-optimized-for each diff. Developers ranked the mes- sages based on the four evaluation metrics (Section 4.3.2). To avoid overwhelming participants and ensure higher completion rates, we limited the survey to 10 diffs instead of using the full set, as large workloads can reduce participation [66]. As in the researcher evaluation, developers were blinded to message origins but could access relevant software contexts.

4.3.5 Hyper-parameter Tuning. As shown in Algorithm 1, several hyper-parameters that may alter the behavior of CMO exist.To opti- mize CMO's performance, we explored a range of hyper-parameters: percentage value p (5, 10, 15, 20), temperature (0.5, 1), and step_limit
(10, 30, 50, 60). Following a Grid Search approach [43], we eval- uated all possible combinations of these hyper-parameters. Due to budget constraints associated with using the OpenAI API [18], we randomly sampled 10 commits from the validation split of the OMG dataset and optimized their human-written messages. Four authors independently evaluated the optimized messages using the evaluation metrics described in Section 4.3.2. Based on these evaluations, we selected p = 5, temperature = 1, and step_limit = 50 as the final CMO settings used in subsequent experiments.

4.4 Answer to RQ2: Effectiveness of CMO
Table 3 (white rows) presents the researchers' evaluations, showing the relative rankings where both evaluators agreed. The table indi- cates whether CMO-optimized messages were ranked Higher, Equal, or Lower compared to the three baselines. The highest percentages for each metric are highlighted. For comparisons between CMO and human, CMO-optimized messages were considered better by both researchers for 42.9% of the commits in terms of Rationality, 63.0% in Comprehensiveness, and 69.8% Expressiveness. Compared with OMG, CMO-optimized messages were considered better for 42.9% of the commits in terms of Rationality, 40.3% in Comprehensiveness, and 44.0% Expressiveness. Table 3 (grey rows) shows developers' eval- uations (all 22 developers completed the survey), where each cell shows how often CMO-optimized messages were relatively ranked. These results align with the researchers' evaluation, reinforcing

that CMO outperforms OMG/CMC and enhances human-written messages, though lags behind in Conciseness.

Table 3: Human Evaluation (by researchers (First Three Rows) and by developers (Last Three Rows)) Results on Commit Messages (RQ2)


	Rationality	Comprehensiveness	Conciseness	Expressiveness	
Higher Equal Lower Higher Equal Lower Higher Equal Lower Higher Equal Lower
CMO VS Human
42.9%
5.9%
0.8%
63.0%
6.7%
0.8%
8.4%
5.9%
30.3%
69.8%
7.8%
22.4%













CMO VS CMC
58.8%
5.9%
1.7%
67.2%
4.2%
0.8%
4.2%
4.2%
34.5%
78.4%
0.9%
20.7%
CMO VS Human
74.5%
13.7%
11.8%
78.2%
15.0%
6.8%
10.9%
12.7%
76.4%
61.4%
25.5%
13.1%













CMO VS CMC
76.4%
15.4%
8.2%
74.1%
17.3%
8.6%
9.1%
13.2%
77.7%
53.6%
31.8%
14.6%

   We also evaluated the performance using automated evaluators by calculating the average scores for the Combined Evaluation Score with Equation 1 (Section 4.2.2), averaged across all commits in the CMO dataset. We conducted Welch's t-test [63] and Cohen's D [28] to assess the statistical significance of score differences between approaches. As shown in Table 4, CMO statistically significantly outperforms OMG with medium effect size (Cohen's D larger than 0.5) for all metrics. Additionally, CMO effectively optimizes human- written messages with large effect size (Cohen's D larger than 0.8) for all metrics.
  Aligning with the findings of Li et al. [48], our analysis using traditional automatic evaluation metrics (Section 4.3.2) fails to cap- ture the performance differences between CMO and CMC. Notably, CMC scores significantly higher in BLEU, METEOR, and ROUGE- L (e.g., BLEU for CMC is 24.67, while for CMO, it is only 7.51). Since these metrics rely on human-written messages as references, they cannot assess whether CMO has successfully optimized those messages. This highlights the limitations of traditional metrics in evaluating CMG tasks, reinforcing the conclusions of Li et al. [48].

   While CMO demonstrates superior performance over OMG/CMC and effectively optimizes human-written messages, its Conciseness remains lacking. Prior research indicates that LLMs tend to generate more detailed and longer commit messages compared to humans [33, 48, 82], which may explain this outcome. Figure 3 [11] illus- trates an example where CMO optimizes a human-written commit message and outperforms both OMG and CMC, as confirmed by researchers and automated evaluators.
Table 4: Automatic Evaluation Results (by evaluators) on Commit Messages
5 
RQ3: CMO for CMG
5.1 Methodology to Answer RQ3
Since human-written commit messages are sometimes left blank [31], we first prompted GPT-4 to generate an initial message and then applied CMO to optimize that message instead of the human- written one. The prompt included the target git diff, a definition of git diff, the expected message format[48], and ten similar git diffs with their high-quality human-written commit messages. We refer to this CMO variant as CMO-blank. Furthermore, beyond optimizing human-written messages as in RQ2 (Section 4.4), we investigated whether CMO can enhance existing CMG techniques by generating higher-quality messages. To that end, we selected the state-of-the-art CMG technique OMG [48] and used its generated messages as initial inputs for CMO to further optimize, denoted as CMO-OMG. We generated messages for the 119 commits in the CMO dataset (Section 4.3.3) and used the similar human evaluation used in RQ2 (Section 4.3.4).
5.2 Answer to RQ3: Effectiveness of CMO Supporting CMG and Blank Initial Message
Table 5 presents researchers' ranking results for CMO-OMG, fo- cusing on cases where both researchers agreed. Compared to base- lines (top of Table 5), CMO-OMG-optimized messages were judged superior for 21.0%-52.9% of commits in Rationality, 26.1%-63.9% in Comprehensiveness, and 27.7%-74.8% in Expressiveness. These findings are consistent with Table 4, where CMO-OMG achieves statistically significantly higher automated scores on Rationality, Comprehensiveness, and Expressiveness compared to baselines.
   Similarly, the middle section of Table 5 shows results for CMO- blank, which performs comparably: outperforming baselines for 21.0%-55.5% of commits in Rationality, 25.2%-62.2% in Comprehen- siveness, and 36.1%-77.3% in Expressiveness. Table 4 further con- firms these improvements with statistical significance. These results demonstrate that CMO can enhance messages produced by CMG techniques like OMG and generate high-quality messages when human-written messages are missing. However, like CMO, these variants lag behind in Conciseness.
  When comparing CMO-OMG and CMO-blank, both achieve sim- ilar quality in Rationality and Comprehensiveness for 25.2%-26.1% of commits, though CMO-OMG has a slight edge. This suggests that CMG techniques like OMG can still benefit CMO by providing stronger initial messages, helping achieve higher final quality than starting from scratch. Thus, depending on organizational needs and resource availability, developers can choose whether to start CMO from an OMG-generated message or from empty.

Table 5: Human Evaluation (by researchers) Results on Com- mit Messages (RQ3)





CMO-OMG VS CMO-blank 15.2% 26.1% 8.4%  16.0% 25.2% 8.4%  16.0%  8.4% 10.9%  36.1% 26.9% 37.0%










6 RQ4: Ablation Study
6.1 Methodology to Answer RQ4

Figure 3: Example of Code Refactoring

6.2 Answer to RQ4: Effect of Components
As shown in Table 4, removing any of the frequently used tools lowers CMO's scores.However, these reductions are not statisti- cally significant (p-value > 0.05) when comparing CMO to CMO- MethodBody and CMO-Type. In contrast, CMO-Search and CMO-File exhibit statistically significant decreases in average scores, with medium effect sizes, across all three metrics.
Table 6 presents the relative rankings where both researchers

The results of RQ2 and RQ3 demonstrate that CMO optimizing human-written messages outperforms other techniques. Therefore, we selected this version of CMO to conduct an ablation study, as- sessing the effectiveness of its two key components: the automated software context collection tools and the search-based optimiza- tion, which includes the automated evaluators. Due to financial constraints of OpenAI API usage [18], we could not ablate all seven context collection tools individually to produce seven distinct vari- ants. Instead, we focused on the three most frequently used tools identified in Section 4.4: Important File Information, Method Body Summary, and Excluded Variable Data Types. This led to creating three CMO variants-CMO-File, CMO-MethodBody, and CMO-Type- each omitting one tool while retaining the remaining six.
  To assess the role of search-based optimization, we developed another variant, CMO-Search, which removes the search algorithm and simply feeds all contexts retrieved by the seven tools directly into GPT-4 to optimize human-written messages. Two researchers independently evaluated 119 commits from the CMO dataset, rank- ing five messages for each commit: the human-written message optimized by the complete CMO and four messages optimized by its corresponding component-ablated variants. The rankings were based on the four established metrics (Section 4.3.2).

Table 6: Human Evaluation (by researchers) Results on Com- mit Messages (RQ4)


	Rationality	Comprehensiveness	Conciseness	Expressiveness	
Higher Equal Lower Higher Equal Lower Higher Equal Lower Higher Equal Lower
CMO VS CMO-Search
17.7%
26.1%
5.9%
13.4%
20.2%
4.2%
5.0%
10.9%
17.6%
25.2%
28.6%
46.2%


























CMO VS CMO-MB
9.2%
39.5%
0.8%
13.4%
32.8%
0.8%
10.1%
15.1%
14.3%
31.9%
36.1%
31.9%

agreed, indicating whether CMO-optimized messages were ranked Higher, Equal, or Lower compared to each ablated variant. When compared to CMO-Search and CMO-File, CMO-optimized messages were considered superior in Rationality for 16.8%-17.7% of the com- mits and in Comprehensiveness for 13.4%-21.0%. In contrast, when compared with CMO-MethodBody and CMO-Type, a smaller pro- portion of CMO-optimized messages were ranked higher-only 7.6%-9.2% in Rationality and 9.2%-13.4% in Comprehensiveness. These results highlight two key insights: (1) the search-based op- timization component significantly improves message quality, as evidenced by CMO's advantage over CMO-Search, and (2) the Im- portant File Information plays a crucial role in guiding message optimization to focus on the files that matter most to developers, as previously noted by Li et al. [48].


7 Threats to Validity
In this section we list potential threats impacting the validity of our experiments.
  Construct Validity: To ensure the quality of our prompts, we followed best practices [14, 15] and a trial-and-error process where multiple authors manually evaluated the quality of the optimized messages on a sample of commits. Potential subjectivity in human evaluation was addressed by providing definitions of metrics and examples to guide the process where multiple people participated. Internal Validity: The OMG dataset contains 381 commits, which may affect both the performance of the LLM-based Quality Evaluator and the overall effectiveness of CMO. Nevertheless, this dataset is the only available set manually labeled with the evalu- ation metrics described in Section 4.3.2, enabling us to train the


evaluator and fine-tune CMO's parameters. To avoid data leakage, we collected 119 commits. Due to the substantial manual effort involved in human evaluation and the financial cost of using the OpenAI API, we did not analyze thousands of commits.
  External Validity: We experimented with only the commits whose git diffs can be processed and generated messages by OMG. Our findings may not be generalizable to all OSS projects, all types of commits and to all LLMs.

8 Conclusion and Future Work
In this study, we identify software contexts considered by humans but missed by the state-of-the-art CMG technique, OMG. We pro- pose Commit Message Optimization (CMO), which leverages GPT-4 to optimize human-written messages by incorporating these missed contexts. Several automated quality evaluators serve as the objec- tive function, aiming to maximize message quality. Our results demonstrate that CMO outperforms both OMG and CMC. More- over, CMO can be applied to existing CMG techniques to further improve message quality, and generate high-quality messages even when initial human-written messages are empty.
  Future work entails exploring how enhancing the quality of the commit messages, while preserving human-considered information, impacts other SE tasks that rely on commit message such as security patch identification [84], patch correctness prediction [67], code refactoring recommendation [62], and defect prediction [32].

References
[1] 2004.	Commit Example 1.	github.com/apache/cocoon/commit/505071 78cbe40b27b9052b487cfb1da475223da1.
[2] 2006.	Commit Example 2.	github.com/apache/archiva/commit/ce2921 9ee6409b111ce8f8191b1792c2b65a04a8.
[3] 2006.	Commit Example 4.	github.com/apache/jclouds/commit/2a5928 b55dff2e7e26a29c1c887400d4c645852e.
[4] 2007.	Commit Example 3.	github.com/apache/tomee/commit/f1e867 7a6cf85b899ba4727728b10cccdc79b39f.
[5] 2011.	Commit Example 10.	github.com/apache/wicket/commit/cc1956 38e57df89bcdb8f72b344107c673143d96.
[6] 2011. Commit Example 12. github.com/apache/logging-log4j2/commit/27c0a1 556a47a2c594aece3c355e891ff4f19836.
[7] 2012.	Commit Example 7.	github.com/apache/hadoop/commit/6f2aae 046d9ebeeae1cd82a687ed6fb165405988.
[8] 2012.	Commit Example 9.	github.com/apache/jclouds/commit/559a37 ceff1f7d5dc83cd32e377610e410769eeb.
[9] 2016. Commit Example 6. github.com/apache/cassandra/commit/eaced9 a541d09d55973b6f88d720e16ac948a559.
[10] 2016. Commit Example 8. github.com/apache/logging-log4j2/commit/856607 faf268136fc6f2e27cf3716a5e16a0f48d.
[11] 2020. Successful Case Commit. github.com/apache/jena/commit/ecfe9e da7adf6f9fcc0d88e8538e436103e823b0.
[12] 2022.	ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt.
[13] 2022. GPT-3.5 Turbo. https://platform.openai.com/docs/models/gpt-3-5-turbo.
[14] 2022.	IBM	Global	AI	Adoption	Index	2022. https://www.ibm.com/watson/resources/ai-adoption.
[15] 2023. awesome-chatgpt-prompts. https://github.com/f/awesome-chatgpt- prompts.
[16] 2023. GPT-4. https://openai.com/research/gpt-4.
[17] 2024. imbalanced-learn documentation. https://imbalanced-learn.org/stable/.
[18] 2024. OpenAI API Pricing. https://openai.com/api/pricing/.
[19] 2024. Sentence Transformers. https://sbert.net/docs/sentence_transformer/pre trained_models.html#original-models.
[20] 2024. sentence-transformers/all-mpnet-base-v2. https://huggingface.co/sentence- transformers/all-mpnet-base-v2.
[21] 2025. OpenAI Models. https://platform.openai.com/docs/models/.
[22] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings


of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65-72.
[23] Marcel Böhme, Charaka Geethal, and Van-Thuan Pham. 2020. Human-in-the- loop automatic program repair. In 2020 IEEE 13th international conference on software testing, validation and verification (ICST). IEEE, 274-285.
[24] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.
[25] Kuljit Kaur Chahal and Munish Saini. 2018. Developer dynamics and syntactic quality of commit messages in oss projects. In Open Source Systems: Enterprise Software and Solutions: 14th IFIP WG 2.13 International Conference, OSS 2018, Athens, Greece, June 8-10, 2018, Proceedings 14. Springer, 61-76.
[26] Dan Chen and Sally E Goldin. 2020. A project-level investigation of software commit comments and code quality. In 2020 3rd International Conference on Information and Communications Technology (ICOIACT). IEEE, 240-245.
[27] Lei Chen, Dandan Wang, Junjie Wang, and Qing Wang. 2019. Enhancing un- supervised requirements traceability with sequential semantics. In 2019 26th Asia-Pacific Software Engineering Conference (APSEC). IEEE, 23-30.
[28] Marc J Diener. 2010. Cohen's d. The Corsini encyclopedia of psychology (2010), 1-1.
[29] Jinhao Dong, Yiling Lou, Dan Hao, and Lin Tan. 2023. Revisiting learning-based commit message generation. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 794-805.
[30] Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, Wenjie Zhang, and Dan Hao. 2022. FIRA: fine-grained graph-based code change representation for automated commit message generation. In Proceedings of the 44th International Conference on Software Engineering. 970-981.
[31] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. 2013. Boa: A language and infrastructure for analyzing ultra-large-scale software repositories. In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 422- 431.
[32] Beyza Eken, RiFat Atar, Sahra Sertalp, and Ayse Tosun. 2019. Predicting defects with latent and semantic features from commit logs in an industrial setting. In 2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW). IEEE, 98-105.
[33] Aleksandra Eliseeva, Yaroslav Sokolov, Egor Bogomolov, Yaroslav Golubev, Danny Dig, and Timofey Bryksin. 2023. From commit message generation to history-aware commit message completion. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 723-735.
[34] Jane Forman and Laura Damschroder. 2007. Qualitative content analysis. In Empirical methods for bioethics: A primer. Emerald Group Publishing Limited, 39-62.
[35] Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan. 2024. Llm-based nlg evaluation: Current status and challenges. arXiv preprint arXiv:2402.01383 (2024).
[36] Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, Hongyu Zhang, and Michael R Lyu. 2023. What makes good in-context demonstrations for code intelligence tasks with llms?. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 761-773.
[37] Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large language models are few- shot summarizers: Multi-intent comment generation via in-context learning. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. 1-13.
[38] Barney G Glaser. 2016. Open Coding Descriptions. Grounded theory review 15, 2
(2016).
[39] Yichen He, Liran Wang, Kaiyi Wang, Yupeng Zhang, Hang Zhang, and Zhoujun Li. 2023. COME: Commit Message Generation with Modification Embedding. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis. 792-803.
[40] Tobias Hey, Fei Chen, Sebastian Weigelt, and Walter F Tichy. 2021. Improving traceability link recovery using fine-grained requirements-to-code relations. In 2021 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 12-22.
[41] Yuan Huang, Nan Jia, Hao-Jie Zhou, Xiang-Ping Chen, Zi-Bin Zheng, and Ming- Dong Tang. 2020. Learning human-written commit messages to document code changes. Journal of Computer Science and Technology 35 (2020), 1258-1277.
[42] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat- ing commit messages from diffs using neural machine translation. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 135-146.
[43] Álvaro Barbero Jiménez, Jorge López Lázaro, and José R Dorronsoro. 2008. Finding optimal model parameters by discrete grid search. In Innovations in hybrid intelligent systems. Springer, 120-127.
[44] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. In



The Twelfth International Conference on Learning Representations.
[45] Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas, et al. 2006. Han- dling imbalanced datasets: A review. GESTS international transactions on computer science and engineering 30, 1 (2006), 25-36.
[46] Stanislav Levin and Amiram Yehudai. 2017. Boosting automatic commit classifica- tion into maintenance activities by utilizing source code changes. In Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering. 97-106.
[47] Jiawei Li and Iftekhar Ahmed. 2023. Commit message matters: Investigating impact and evolution of commit message quality. In 2023 IEEE/ACM 45th Interna- tional Conference on Software Engineering (ICSE). IEEE, 806-817.
[48] Jiawei Li, David Faragó, Christian Petrov, and Iftekhar Ahmed. 2024. Only diff Is Not Enough: Generating Commit Messages Leveraging Reasoning and Action of Large Language Model. Proceedings of the ACM on Software Engineering 1, FSE (2024), 745-766.
[49] Bo Lin, Shangwen Wang, Zhongxin Liu, Yepang Liu, Xin Xia, and Xiaoguang Mao. 2023. Cct5: A code-change-oriented pre-trained model. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1509-1521.
[50] Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL-04). 605-612.
[51] Mario Linares-Vásquez, Luis Fernando Cortés-Coy, Jairo Aponte, and Denys Poshyvanyk. 2015. Changescribe: A tool for automatically generating commit messages. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 2. IEEE, 709-712.
[52] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. 2023. Large language models as evolutionary optimizers. arXiv preprint arXiv:2310.19046 (2023).
[53] Shangqing Liu, Cuiyun Gao, Sen Chen, Lun Yiu Nie, and Yang Liu. 2020. Atom: Commit message generation based on abstract syntax tree and hybrid ranking. IEEE Transactions on Software Engineering 48, 5 (2020), 1800-1817.
[54] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 (2023).
[55] Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu Wang. 2018. Neural-machine-translation-based commit message generation: how far are we?. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. 373-384.
[56] Umme Ayda Mannan, Iftekhar Ahmed, Carlos Jensen, and Anita Sarma. 2020. On the relationship between design discussions and design quality: a case study of Apache projects. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 543-555.
[57] Kevin Moran, David N Palacio, Carlos Bernal-Cárdenas, Daniel McCrystal, Denys Poshyvanyk, Chris Shenefiel, and Jeff Johnson. 2020. Improving the effectiveness of traceability link recovery using hierarchical bayesian networks. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 873-885.
[58] Nicholas Smith, Danny van Bruggen, and Federico Tomassetti. 2023. Tools for your Java code. https://javaparser.org/.
[59] Lun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam, Yang Liu, and Zenglin Xu. 2021. Coregen: Contextualized code representation learning for commit message generation. Neurocomputing 459 (2021), 97-107.
[60] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311-318.
[61] Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. 2023. Towards making the most of chatgpt for machine translation. arXiv preprint arXiv:2303.13780 (2023).
[62] Soumaya Rebai, Marouane Kessentini, Vahid Alizadeh, Oussama Ben Sghaier, and Rick Kazman. 2020. Recommending refactorings via commit message analysis. Information and Software Technology 126 (2020), 106332.
[63] Graeme D Ruxton. 2006. The unequal variance t-test is an underused alternative to Student's t-test and the Mann-Whitney U test. Behavioral Ecology 17, 4 (2006), 688-690.
[64] Jinfeng Shen, Xiaobing Sun, Bin Li, Hui Yang, and Jiajun Hu. 2016. On automatic summarization of what and why information in source code changes. In 2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC), Vol. 1. IEEE, 103-112.
[65] Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2022. RACE: Retrieval-augmented Commit Message Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 5520-5530.
[66] Edward Smith, Robert Loftin, Emerson Murphy-Hill, Christian Bird, and Thomas Zimmermann. 2013. Improving developer participation rates in surveys. In 2013 6th International workshop on cooperative and human aspects of software engineering (CHASE). IEEE, 89-92.
[67] 
Haoye Tian, Xunzhu Tang, Andrew Habib, Shangwen Wang, Kui Liu, Xin Xia, Jacques Klein, and Tegawendé F Bissyandé. 2022. Is this change the answer to that problem? correlating descriptions of bug and code changes for evaluating patch correctness. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. 1-13.
[68] Yingchen Tian, Yuxia Zhang, Klaas-Jan Stol, Lin Jiang, and Hui Liu. 2022. What makes a good commit message?. In Proceedings of the 44th International Conference on Software Engineering. 2389-2401.
[69] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).
[70] Bei Wang, Meng Yan, Zhongxin Liu, Ling Xu, Xin Xia, Xiaohong Zhang, and Dan Yang. 2021. Quality assurance for automated commit message generation. In 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 260-271.
[71] Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048 (2023).
[72] Liran Wang, Xunzhu Tang, Yichen He, Changyu Ren, Shuhua Shi, Chaoran Yan, and Zhoujun Li. 2023. Delving into commit-issue correlation to enhance commit message generation models. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 710-722.
[73] Ying Wang, Bihuan Chen, Kaifeng Huang, Bowen Shi, Congying Xu, Xin Peng, Yijian Wu, and Yang Liu. 2020. An empirical study of usages, updates and risks of third-party libraries in java projects. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 35-45.
[74] Yifan Wu, Ying Li, and Siyu Yu. 2024. Commit Message Generation via Chat- GPT: How Far Are We?. In Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering. 124-129.
[75] Yifan Wu, Yunpeng Wang, Ying Li, Wei Tao, Siyu Yu, Haowen Yang, Wei Jiang, and Jianguo Li. 2025. An Empirical Study on Commit Message Generation using LLMs via In-Context Learning. arXiv preprint arXiv:2502.18904 (2025).
[76] Shengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, Hanghang Tong, and Jian Lu. 2019. Commit message generation for source code changes. In IJCAI.
[77] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2024. Large Language Models as Optimizers. In The Twelfth International Conference on Learning Representations. https://openreview.net/ forum?id=Bb4VGOWELI
[78] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems 36 (2024).
[79] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022).
[80] Lucas Zamprogno, Braxton Hall, Reid Holmes, and Joanne M Atlee. 2022. Dy- namic human-in-the-loop assertion generation. IEEE Transactions on Software Engineering 49, 4 (2022), 2337-2351.
[81] Chenyuan Zhang, Yanlin Wang, Zhao Wei, Yong Xu, Juhong Wang, Hui Li, and Rongrong Ji. 2023. EALink: An efficient and accurate pre-trained framework for issue-commit link recovery. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 217-229.
[82] Yuxia Zhang, Zhiqing Qiu, Klaas-Jan Stol, Wenhui Zhu, Jiaxin Zhu, Yingchen Tian, and Hui Liu. 2024. Automatic commit message generation: A critical review and directions for future work. IEEE Transactions on Software Engineering (2024).
[83] Teng Zhao, Qinghua Cao, and Qing Sun. 2017. An improved approach to trace- ability recovery based on word embeddings. In 2017 24th Asia-Pacific Software Engineering Conference (APSEC). IEEE, 81-89.
[84] Yaqin Zhou, Jing Kai Siow, Chenyu Wang, Shangqing Liu, and Yang Liu. 2021. Spi: Automated identification of security patches via commits. ACM Transactions on Software Engineering and Methodology (TOSEM) 31, 1 (2021), 1-27.




